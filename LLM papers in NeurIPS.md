# Papers about Language Modeling in NeurIPS 2023
## Token-Scaled Logit Distillation for Ternary Weight Generative Language Models<sup>poster<sup>

Authors: Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, Jungwook Choi

Link: [https://neurips.cc/virtual/2023/poster/72260](https://neurips.cc/virtual/2023/poster/72260)

Abstract:

 Generative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and achieves enhanced accuracy in tasks like common-sense QA and arithmetic reasoning  as well as natural language understanding. Our code is available at https://github.com/aiha-lab/TSLD.

æ‘˜è¦:

ç”Ÿæˆè¯­è¨€æ¨¡å‹ï¼ˆGLMï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆã€ç†è§£å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¾ƒå¤§çš„æ¨¡å‹å°ºå¯¸ç»™å®é™…éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚ç„¶è€Œï¼Œå½“å‰ç”Ÿæˆæ¨¡å‹çš„ QAT æ–¹æ³•å¯¼è‡´äº†å‡†ç¡®æ€§çš„æ˜æ˜¾æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨ä¸º GLM è®¾è®¡çš„æ–°é¢–çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºä»¤ç‰Œè§„æ¨¡ Logit è’¸é¦ï¼Œå¯ä»¥é˜²æ­¢è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶æä¾›ä»æ•™å¸ˆæ¨¡å‹å’ŒåŸºæœ¬äº‹å®ä¸­è¿›è¡Œçš„å“è¶Šå­¦ä¹ ã€‚è¿™é¡¹ç ”ç©¶æ ‡å¿—ç€é¦–æ¬¡å¯¹å¤§è§„æ¨¡ GLM çš„ä¸‰å…ƒæƒé‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒè¿›è¡Œäº†è¯„ä¼°ï¼Œå…¶å›°æƒ‘åº¦ä¸‹é™å°äº 1.0ï¼Œå¹¶åœ¨å¸¸è¯† QAã€ç®—æœ¯æ¨ç†ä»¥åŠè‡ªç„¶è¯­è¨€ç†è§£ç­‰ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ https://github.com/aiha-lab/TSLD è·å–ã€‚

## Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision<sup>poster<sup>

Authors: Jiaxin Zhang, Zhuohang Li, Kamalika Das, Sricharan Kumar

Link: [https://neurips.cc/virtual/2023/poster/71465](https://neurips.cc/virtual/2023/poster/71465)

Abstract:

 Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the $\bf 3\times$ human annotation baselines in all four tasks and achieves very close performance as $\bf 5\times$ human annotation on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºå…¶éƒ¨ç½²è§„æ¨¡å·¨å¤§ã€å®¹æ˜“å—åˆ°é”™è¯¯ä¿¡æ¯çš„å½±å“ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œæ•°æ®æ³¨é‡Šæˆæœ¬é«˜æ˜‚ï¼Œå®ƒä»¬å¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡çš„é€‚ç”¨æ€§å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„äº¤äº’å¼å¤šä¿çœŸå­¦ä¹ ï¼ˆIMFLï¼‰æ¡†æ¶ï¼Œç”¨äºåœ¨æœ‰é™çš„æ³¨é‡Šé¢„ç®—ä¸‹ç»æµæœ‰æ•ˆåœ°å¼€å‘å°å‹ç‰¹å®šé¢†åŸŸçš„ LMã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒè¿‡ç¨‹åˆ¶å®šä¸ºå¤šä¿çœŸåº¦å­¦ä¹ é—®é¢˜ï¼Œé‡ç‚¹æ˜¯ç¡®å®šåœ¨ä½ä¿çœŸåº¦è‡ªåŠ¨ LLM æ³¨é‡Šå’Œé«˜ä¿çœŸåº¦äººå·¥æ³¨é‡Šä¹‹é—´å–å¾—å¹³è¡¡çš„æœ€ä½³è·å–ç­–ç•¥ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å¢å¼ºæ³¨é‡Šå¤šæ ·æ€§å’Œä¿¡æ¯é‡çš„æ¢ç´¢-åˆ©ç”¨æŸ¥è¯¢ç­–ç•¥ï¼Œç»“åˆäº†ä¸¤ç§åˆ›æ–°è®¾è®¡ï¼š1ï¼‰æç¤ºæ£€ç´¢ï¼Œä»äººå·¥æ³¨é‡Šçš„æ ·æœ¬ä¸­é€‰æ‹©ä¸Šä¸‹æ–‡ä¸­çš„ç¤ºä¾‹ä»¥æ”¹è¿›LLMæ³¨é‡Šï¼Œ2ï¼‰å¯å˜æ‰¹é‡å¤§å°æ§åˆ¶é€‰æ‹©æ¯ä¸ªä¿çœŸåº¦çš„é¡ºåºä»¥ä¿ƒè¿›çŸ¥è¯†è’¸é¦ï¼Œæœ€ç»ˆæé«˜æ³¨é‡Šè´¨é‡ã€‚é’ˆå¯¹é‡‘èå’ŒåŒ»ç–—ä»»åŠ¡çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å•ä¸€ä¿çœŸåº¦æ³¨é‡Šç›¸æ¯”ï¼ŒIMFL å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚é‰´äºäººå·¥æ³¨é‡Šçš„é¢„ç®—æœ‰é™ï¼ŒIMFL åœ¨æ‰€æœ‰å››é¡¹ä»»åŠ¡ä¸­éƒ½æ˜¾ç€ä¼˜äº $\bf 3\times$ äººå·¥æ³¨é‡ŠåŸºçº¿ï¼Œå¹¶ä¸”åœ¨å…¶ä¸­ä¸¤é¡¹ä»»åŠ¡ä¸Šå®ç°äº†ä¸ $\bf 5\times$ äººå·¥æ³¨é‡Šéå¸¸æ¥è¿‘çš„æ€§èƒ½ã€‚è¿™äº›æœ‰å¸Œæœ›çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡é‡‡ç”¨ IMFL å¯ä»¥æ˜¾ç€é™ä½ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„é«˜äººå·¥æ³¨é‡Šæˆæœ¬ï¼ŒIMFL ä½¿ç”¨è¾ƒå°‘çš„äººå·¥æ³¨é‡Šï¼Œå¹¶è¾…ä»¥æ›´ä¾¿å®œå’Œæ›´å¿«çš„ LLMï¼ˆä¾‹å¦‚ GPT-3.5ï¼‰æ³¨é‡Šæ¥å®ç°å¯æ¯”è¾ƒçš„æ€§èƒ½ã€‚

## LayoutGPT: Compositional Visual Planning and Generation with Large Language Models<sup>poster<sup>

Authors: Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Xuehai He, S Basu, Xin Eric Wang, William Yang Wang

Link: [https://neurips.cc/virtual/2023/poster/71328](https://neurips.cc/virtual/2023/poster/71328)

Abstract:

 Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance visual planning skills of LLMs. We show that LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40\% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness. Lastly, LayoutGPT achieves comparable performance to supervised methods in 3D indoor scene synthesis, demonstrating its effectiveness and potential in multiple visual domains.

æ‘˜è¦:

åœ¨è§†è§‰ç”Ÿæˆä¸­è·å¾—é«˜åº¦çš„ç”¨æˆ·å¯æ§æ€§é€šå¸¸éœ€è¦å¤æ‚ã€ç»†ç²’åº¦çš„è¾“å…¥ï¼Œä¾‹å¦‚å¸ƒå±€ã€‚ç„¶è€Œï¼Œä¸ç®€å•çš„æ–‡æœ¬è¾“å…¥ç›¸æ¯”ï¼Œè¿™æ ·çš„è¾“å…¥ç»™ç”¨æˆ·å¸¦æ¥äº†å¾ˆå¤§çš„è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•é€šè¿‡æ ¹æ®æ–‡æœ¬æ¡ä»¶ç”Ÿæˆå¸ƒå±€æ¥å……å½“è§†è§‰è§„åˆ’å™¨ï¼Œä»è€Œä¸è§†è§‰ç”Ÿæˆæ¨¡å‹åä½œã€‚æˆ‘ä»¬æå‡ºäº† LayoutGPTï¼Œä¸€ç§ç”¨æ ·å¼è¡¨è¯­è¨€ç¼–å†™ä¸Šä¸‹æ–‡è§†è§‰æ¼”ç¤ºçš„æ–¹æ³•ï¼Œä»¥å¢å¼ºæ³•å­¦ç¡•å£«çš„è§†è§‰è§„åˆ’æŠ€èƒ½ã€‚æˆ‘ä»¬è¯æ˜ LayoutGPT å¯ä»¥åœ¨å¤šä¸ªé¢†åŸŸç”Ÿæˆåˆç†çš„å¸ƒå±€ï¼ŒèŒƒå›´ä» 2D å›¾åƒåˆ° 3D å®¤å†…åœºæ™¯ã€‚ LayoutGPT åœ¨å°†æ•°å­—å’Œç©ºé—´å…³ç³»ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­è¨€æ¦‚å¿µè½¬æ¢ä¸ºå¸ƒå±€å®‰æ’ä»¥å®ç°å¿ å®çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢ä¹Ÿè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å½“ä¸ä¸‹æ¸¸å›¾åƒç”Ÿæˆæ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ï¼ŒLayoutGPT çš„æ€§èƒ½æ¯”æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹/ç³»ç»Ÿé«˜å‡º 20-40%ï¼Œå¹¶ä¸”åœ¨è®¾è®¡è§†è§‰å¸ƒå±€çš„æ•°å€¼å’Œç©ºé—´æ­£ç¡®æ€§æ–¹é¢å®ç°äº†ä¸äººç±»ç”¨æˆ·ç›¸å½“çš„æ€§èƒ½ã€‚æœ€åï¼ŒLayoutGPT åœ¨ 3D å®¤å†…åœºæ™¯åˆæˆä¸­å®ç°äº†ä¸ç›‘ç£æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šä¸ªè§†è§‰é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚

## Parts of Speechâ€“Grounded Subspaces in Vision-Language Models<sup>poster<sup>

Authors: James Oldfield, Christos Tzelepis, Yannis Panagakis, Mihalis Nicolaou, Ioannis Patras

Link: [https://neurips.cc/virtual/2023/poster/70789](https://neurips.cc/virtual/2023/poster/70789)

Abstract:

 Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased toward specific visual properties (such as objects or actions) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIPâ€™s joint vision-language space by leveraging the association between parts of speech and specific visual modes of variation (e.g. nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. Whatâ€™s more, we show the proposed model additionally facilitates learning subspaces corresponding to specific visual appearances (e.g. artistsâ€™ painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artistsâ€™ styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification.

æ‘˜è¦:

äº‹å®è¯æ˜ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹äº§ç”Ÿçš„æ½œåœ¨å›¾åƒè¡¨ç¤ºå¯¹äºå„ç§ä¸‹æ¸¸ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å®ç”¨æ€§å› å…¶ä¸ä¸åŒè§†è§‰å±æ€§çš„çº ç¼ è€Œå—åˆ°é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘çš„å·¥ä½œè¡¨æ˜ CLIP å›¾åƒè¡¨ç¤ºé€šå¸¸ä»¥ä¸å¯é¢„æµ‹çš„æ–¹å¼åå‘äºç‰¹å®šçš„è§†è§‰å±æ€§ï¼ˆä¾‹å¦‚å¯¹è±¡æˆ–åŠ¨ä½œï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºè®®é€šè¿‡åˆ©ç”¨è¯æ€§å’Œç‰¹å®šè§†è§‰å˜åŒ–æ¨¡å¼ä¹‹é—´çš„å…³è”ï¼ˆä¾‹å¦‚ï¼Œåè¯ä¸ç‰©ä½“ç›¸å…³ï¼Œå½¢å®¹è¯æè¿°å¤–è§‚ï¼‰ï¼Œæ¥åˆ†ç¦»CLIPè”åˆè§†è§‰è¯­è¨€ç©ºé—´ä¸­ä¸åŒè§†è§‰æ¨¡æ€çš„è¡¨ç¤ºã€‚è¿™æ˜¯é€šè¿‡åˆ¶å®šé€‚å½“çš„æˆåˆ†åˆ†ææ¨¡å‹æ¥å®ç°çš„ï¼Œè¯¥æ¨¡å‹å­¦ä¹ æ•è·ä¸ç‰¹å®šè¯æ€§ç›¸å¯¹åº”çš„å˜å¼‚æ€§çš„å­ç©ºé—´ï¼ŒåŒæ—¶å…±åŒæœ€å°åŒ–å…¶ä½™éƒ¨åˆ†çš„å˜å¼‚æ€§ã€‚è¿™æ ·çš„å­ç©ºé—´ä»¥å°é—­å½¢å¼äº§ç”Ÿå›¾åƒæˆ–æ–‡æœ¬çš„ä¸åŒè§†è§‰å±æ€§çš„è§£å¼€è¡¨ç¤ºï¼ŒåŒæ—¶å°Šé‡è¡¨ç¤ºæ‰€åœ¨çš„æµå½¢çš„åŸºç¡€å‡ ä½•å½¢çŠ¶ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜æ‰€æå‡ºçš„æ¨¡å‹è¿˜æœ‰åŠ©äºå­¦ä¹ ä¸ç‰¹å®šè§†è§‰å¤–è§‚ï¼ˆä¾‹å¦‚è‰ºæœ¯å®¶çš„ç»˜ç”»é£æ ¼ï¼‰ç›¸å¯¹åº”çš„å­ç©ºé—´ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿä»åŸºäº CLIP çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­é€‰æ‹©æ€§åœ°åˆ é™¤æ•´ä¸ªè§†è§‰ä¸»é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯è§†åŒ–å­ç©ºé—´æŠ•å½±å¹¶é˜²æ­¢æ¨¡ä»¿è‰ºæœ¯å®¶é£æ ¼æ¥å®šæ€§åœ°éªŒè¯æ¨¡å‹ï¼Œå¹¶é€šè¿‡ç±»ä¸å˜æ€§æŒ‡æ ‡å’Œå¯¹åŸºçº¿é›¶æ ·æœ¬åˆ†ç±»çš„æ”¹è¿›æ¥å®šé‡åœ°éªŒè¯æ¨¡å‹ã€‚

## Augmenting Language Models with Long-Term Memory<sup>poster<sup>

Authors: Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei

Link: [https://neurips.cc/virtual/2023/poster/72461](https://neurips.cc/virtual/2023/poster/72461)

Abstract:

 Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMem can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LongMem can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents.

æ‘˜è¦:

ç”±äºè¾“å…¥é•¿åº¦é™åˆ¶ï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åªèƒ½æä¾›å›ºå®šå¤§å°çš„è¾“å…¥ï¼Œä»è€Œé˜»æ­¢å®ƒä»¬åˆ©ç”¨è¿‡å»è¾“å…¥ä¸­ä¸°å¯Œçš„é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œå³å¢å¼ºé•¿æœŸè®°å¿†çš„è¯­è¨€æ¨¡å‹ï¼ˆLongMemï¼‰ï¼Œå®ƒä½¿æ³•å­¦ç¡•å£«èƒ½å¤Ÿè®°ä½æ‚ ä¹…çš„å†å²ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è§£è€¦ç½‘ç»œæ¶æ„ï¼Œå°†åŸå§‹éª¨å¹²LLMå†»ç»“ä¸ºå†…å­˜ç¼–ç å™¨ï¼Œå°†è‡ªé€‚åº”æ®‹å·®ä¾§ç½‘ç»œä½œä¸ºå†…å­˜æ£€ç´¢å™¨å’Œè¯»å–å™¨ã€‚è¿™ç§è§£è€¦çš„å†…å­˜è®¾è®¡å¯ä»¥è½»æ¾ç¼“å­˜å’Œæ›´æ–°é•¿æœŸè¿‡å»çš„ä¸Šä¸‹æ–‡ä»¥è¿›è¡Œå†…å­˜æ£€ç´¢ï¼Œè€Œä¸ä¼šé­å—å†…å­˜é™ˆæ—§çš„å½±å“ã€‚é€šè¿‡è®°å¿†å¢å¼ºé€‚åº”è®­ç»ƒçš„å¢å¼ºï¼ŒLongMem å¯ä»¥è®°ä½å¾ˆä¹…ä»¥å‰çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ä½¿ç”¨é•¿æœŸè®°å¿†è¿›è¡Œè¯­è¨€å»ºæ¨¡ã€‚æ‰€æå‡ºçš„å†…å­˜æ£€ç´¢æ¨¡å—å¯ä»¥å¤„ç†å…¶å†…å­˜åº“ä¸­æ— é™é•¿åº¦çš„ä¸Šä¸‹æ–‡ï¼Œä»¥æœ‰åˆ©äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šå¸¸ï¼ŒLongMem å¯ä»¥å°†é•¿æ ¼å¼å†…å­˜æ‰©å¤§åˆ° 65k ä¸ªæ ‡è®°ï¼Œä»è€Œå°†å¤šé•œå¤´é¢å¤–æ¼”ç¤ºç¤ºä¾‹ç¼“å­˜ä¸ºé•¿æ ¼å¼å†…å­˜ï¼Œç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ ChapterBreakï¼ˆä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡åŸºå‡†ï¼‰ä¸Šä¼˜äºå¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®°å¿†å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢æ¯”æ³•å­¦ç¡•å£«å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆå¸®åŠ©è¯­è¨€æ¨¡å‹è®°å¿†å’Œåˆ©ç”¨é•¿æ ¼å¼å†…å®¹ã€‚

## Statistical Knowledge Assessment for Large Language Models<sup>poster<sup>

Authors: Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Zhifang Sui, Lei Li

Link: [https://neurips.cc/virtual/2023/poster/70415](https://neurips.cc/virtual/2023/poster/70415)

Abstract:

 Given varying prompts regarding a factoid question, can a large language model (LLM) reliably generate factually correct answers? Existing LLMs may generate distinct responses for different prompts. In this paper, we study the problem of quantifying knowledge contained in an LLM regarding a given set of facts. We propose KaRR, a statistical approach to assess factual knowledge for LLMs. The main idea is to estimate the ratio of LLM generating text corresponding to the answer entity given diverse prompts of the subject and the querying relation, versus it generating by random chances. Our assessment suite contains a comprehensive set of 994,123 entities and 600 relations, with 1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes, including LLaMA, Alpaca, OPT, etc. Experiments show that our results have a strong correlation (0.43 Kendall's $\tau$) with the results of human assessment on LLMs. Our results reveal that the knowledge in LLMs with the same backbone architecture adheres to the scaling law, while tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably.

æ‘˜è¦:

ç»™å®šæœ‰å…³äº‹å®é—®é¢˜çš„ä¸åŒæç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) èƒ½å¦å¯é åœ°ç”Ÿæˆäº‹å®ä¸Šæ­£ç¡®çš„ç­”æ¡ˆï¼Ÿç°æœ‰çš„æ³•å­¦ç¡•å£«å¯èƒ½ä¼šå¯¹ä¸åŒçš„æç¤ºäº§ç”Ÿä¸åŒçš„ååº”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é‡åŒ–æ³•å­¦ç¡•å£«ä¸­åŒ…å«çš„æœ‰å…³ç»™å®šäº‹å®é›†çš„çŸ¥è¯†çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡º KaRRï¼Œä¸€ç§è¯„ä¼°æ³•å­¦ç¡•å£«äº‹å®çŸ¥è¯†çš„ç»Ÿè®¡æ–¹æ³•ã€‚ä¸»è¦æ€æƒ³æ˜¯ä¼°è®¡LLMåœ¨ç»™å®šä¸»é¢˜å’ŒæŸ¥è¯¢å…³ç³»çš„ä¸åŒæç¤ºçš„æƒ…å†µä¸‹ç”Ÿæˆä¸ç­”æ¡ˆå®ä½“ç›¸å¯¹åº”çš„æ–‡æœ¬ä¸éšæœºç”Ÿæˆçš„æ–‡æœ¬çš„æ¯”ç‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°å¥—ä»¶åŒ…å«ä¸€æ•´å¥— 994,123 ä¸ªå®ä½“å’Œ 600 ä¸ªå…³ç³»ï¼Œä»¥åŠ 1,395,905 ä¸ªæ–‡æœ¬åˆ«åã€‚æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•è¯„ä¼°äº† 20 ä¸ªä¸åŒè§„æ¨¡çš„ LLMï¼ŒåŒ…æ‹¬ LLaMAã€Alpacaã€OPT ç­‰ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»“æœä¸äººç±»å¯¹ LLM çš„è¯„ä¼°ç»“æœå…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼ˆ0.43 Kendall's $\tau$ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå…·æœ‰ç›¸åŒä¸»å¹²æ¶æ„çš„æ³•å­¦ç¡•å£«ä¸­çš„çŸ¥è¯†éµå¾ªç¼©æ”¾æ³•åˆ™ï¼Œè€Œå¯¹æŒ‡ä»¤è·Ÿè¸ªæ•°æ®çš„è°ƒæ•´æœ‰æ—¶ä¼šæŸå®³æ¨¡å‹å¯é åœ°ç”Ÿæˆäº‹å®ä¸Šæ­£ç¡®çš„æ–‡æœ¬çš„èƒ½åŠ›ã€‚

## Meta-in-context learning in large language models<sup>poster<sup>

Authors: Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, Eric Schulz

Link: [https://neurips.cc/virtual/2023/poster/70239](https://neurips.cc/virtual/2023/poster/70239)

Abstract:

 Large language models have shown tremendous performance in a variety of tasks.  In-context learning  -- the ability to improve at a task after being provided with a number of demonstrations -- is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we broaden the scope of our investigation to encompass two diverse benchmarks: one focusing on real-world regression problems and the other encompassing multiple NLP tasks. In both cases, we observe competitive performance comparable to that of traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ€§èƒ½ã€‚æƒ…å¢ƒå­¦ä¹ â€”â€”åœ¨æä¾›å¤šæ¬¡æ¼”ç¤ºåæ”¹è¿›ä»»åŠ¡çš„èƒ½åŠ›â€”â€”è¢«è§†ä¸ºä»–ä»¬æˆåŠŸçš„ä¸»è¦å› ç´ ä¹‹ä¸€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æœ¬èº«æ¥é€’å½’åœ°æé«˜ã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºå…ƒæƒ…å¢ƒå­¦ä¹ ã€‚ç€çœ¼äºä¸¤ä¸ªç†æƒ³åŒ–çš„é¢†åŸŸï¼Œä¸€ä¸ªä¸€ç»´å›å½’ä»»åŠ¡å’Œä¸€ä¸ªåŒè‡‚è€è™æœºä»»åŠ¡ï¼Œæˆ‘ä»¬è¡¨æ˜å…ƒä¸Šä¸‹æ–‡å­¦ä¹ è‡ªé€‚åº”åœ°é‡å¡‘äº†å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å¯¹äºé¢„æœŸä»»åŠ¡çš„å…ˆéªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å…ƒä¸Šä¸‹æ–‡å­¦ä¹ ä¿®æ”¹äº†æ­¤ç±»æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰©å¤§äº†è°ƒæŸ¥èŒƒå›´ï¼Œæ¶µç›–ä¸¤ä¸ªä¸åŒçš„åŸºå‡†ï¼šä¸€ä¸ªä¸“æ³¨äºç°å®ä¸–ç•Œçš„å›å½’é—®é¢˜ï¼Œå¦ä¸€ä¸ªæ¶µç›–å¤šä¸ª NLP ä»»åŠ¡ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½è§‚å¯Ÿåˆ°ä¸ä¼ ç»Ÿå­¦ä¹ ç®—æ³•ç›¸å½“çš„ç«äº‰æ€§èƒ½ã€‚æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„å·¥ä½œæé«˜äº†æˆ‘ä»¬å¯¹ä¸Šä¸‹æ–‡å­¦ä¹ çš„ç†è§£ï¼Œå¹¶ä¸ºçº¯ç²¹é€šè¿‡å…ƒä¸Šä¸‹æ–‡å­¦ä¹ è€Œä¸æ˜¯ä¼ ç»Ÿçš„å¾®è°ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”å®ƒä»¬æ‰€åº”ç”¨çš„ç¯å¢ƒé“ºå¹³äº†é“è·¯ã€‚

## Language Models can Solve Computer Tasks<sup>poster<sup>

Authors: Geunwoo Kim, Pierre Baldi, Stephen McAleer

Link: [https://neurips.cc/virtual/2023/poster/71929](https://neurips.cc/virtual/2023/poster/71929)

Abstract:

 Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent \textbf{R}ecursively \textbf{C}riticizes and \textbf{I}mproves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.

æ‘˜è¦:

èƒ½å¤Ÿåœ¨è®¡ç®—æœºä¸Šæ‰§è¡Œä¸€èˆ¬ä»»åŠ¡çš„ä»£ç†å¯ä»¥é€šè¿‡è‡ªåŠ¨æ‰§è¡Œé‡å¤ä»»åŠ¡å¹¶ååŠ©è§£å†³å¤æ‚é—®é¢˜æ¥æé«˜æ•ˆç‡å’Œç”Ÿäº§åŠ›ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæ­¤ç±»ä»£ç†åº”è¯¥èƒ½å¤Ÿè§£å†³é€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤å‘ˆç°ç»™ä»–ä»¬çš„æ–°è®¡ç®—æœºä»»åŠ¡ã€‚ç„¶è€Œï¼Œä»¥å‰è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•éœ€è¦å¤§é‡çš„ä¸“å®¶æ¼”ç¤ºå’Œç‰¹å®šäºä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ï¼Œè¿™å¯¹äºæ–°ä»»åŠ¡æ¥è¯´éƒ½æ˜¯ä¸åˆ‡å®é™…çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å¯ä»¥ä½¿ç”¨ç®€å•çš„æç¤ºæ–¹æ¡ˆæ‰§è¡Œç”±è‡ªç„¶è¯­è¨€å¼•å¯¼çš„è®¡ç®—æœºä»»åŠ¡ï¼Œå…¶ä¸­ä»£ç† \textbf{R} é€’å½’åœ° \textbf{C} æ‰¹è¯„å’Œ \textbf{ I}æé«˜äº†å…¶è¾“å‡ºï¼ˆRCIï¼‰ã€‚ RCI æ–¹æ³•åœ¨è‡ªåŠ¨åŒ–è®¡ç®—æœºä»»åŠ¡æ–¹é¢æ˜¾ç€ä¼˜äºç°æœ‰çš„ LLM æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ MiniWoB++ åŸºå‡†ä¸Šè¶…è¶Šäº†ç›‘ç£å­¦ä¹  (SL) å’Œå¼ºåŒ–å­¦ä¹  (RL) æ–¹æ³•ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å¤šä¸ª LLMï¼Œå‘ç°ä½¿ç”¨ InstructGPT-3+RLHF LLM çš„ RCI åœ¨ MiniWoB++ ä¸Šæ˜¯æœ€å…ˆè¿›çš„ï¼Œæ¯ä¸ªä»»åŠ¡ä»…ä½¿ç”¨å°‘é‡æ¼”ç¤ºï¼Œè€Œä¸æ˜¯æ•°ä¸‡æ¬¡ï¼Œå¹¶ä¸”æ²¡æœ‰ç‰¹å®šäºä»»åŠ¡çš„å¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº† RCI æç¤ºåœ¨å¢å¼ºæ³•å­¦ç¡•å£«åœ¨ä¸€ç³»åˆ—è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…¶è¡¨ç°ä¼˜äºå¸¦æœ‰å¤–éƒ¨åé¦ˆçš„æ€æƒ³é“¾ (CoT) æç¤ºã€‚æˆ‘ä»¬å‘ç° RCI ä¸ CoT ç»“åˆä½¿ç”¨æ¯”å•ç‹¬ä½¿ç”¨ä¸¤è€…æ•ˆæœæ›´å¥½ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼šhttps://github.com/posgnu/rci-agentã€‚

## VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models<sup>poster<sup>

Authors: Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schmidt

Link: [https://neurips.cc/virtual/2023/poster/73556](https://neurips.cc/virtual/2023/poster/73556)

Abstract:

 We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluating instruction-following vision-language models for real-world use. Our starting point is curating 70 "instruction families" that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.

æ‘˜è¦:

æˆ‘ä»¬ä»‹ç» VisIT-Benchï¼ˆè§†è§‰æŒ‡ä»¤åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å®é™…ä½¿ç”¨çš„æŒ‡ä»¤è·Ÿè¸ªè§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„å‡ºå‘ç‚¹æ˜¯ç­–åˆ’ 70 ä¸ªâ€œæŒ‡ä»¤æ—â€ï¼Œæˆ‘ä»¬è®¾æƒ³æŒ‡ä»¤è°ƒæ•´çš„è§†è§‰è¯­è¨€æ¨¡å‹åº”è¯¥èƒ½å¤Ÿè§£å†³è¿™äº›é—®é¢˜ã€‚é™¤äº† VQAv2 å’Œ COCO ç­‰è¯„ä¼°ä¹‹å¤–ï¼Œä»»åŠ¡èŒƒå›´è¿˜åŒ…æ‹¬ä»åŸºæœ¬è¯†åˆ«åˆ°æ¸¸æˆå’Œåˆ›æ„ç”Ÿæˆã€‚ç»è¿‡æ•´ç†åï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åŒ…å« 592 ä¸ªæµ‹è¯•æŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢éƒ½æœ‰ä¸€ä¸ªäººå·¥ç¼–å†™çš„æŒ‡ä»¤æ¡ä»¶æ ‡é¢˜ã€‚è¿™äº›æè¿°è¡¨é¢äº†ç‰¹å®šäºæŒ‡ä»¤çš„å› ç´ ï¼Œä¾‹å¦‚ï¼Œå¯¹äºè¯¢é—®è½®æ¤…ä½¿ç”¨è€…åº—é¢çš„å¯è®¿é—®æ€§çš„æŒ‡ä»¤ï¼Œä»¥æŒ‡ä»¤ä¸ºæ¡ä»¶çš„æ ‡é¢˜æè¿°äº†å¡é“/æ½œåœ¨çš„éšœç¢ç‰©ã€‚è¿™äº›æè¿°ä½¿å¾— 1) æ”¶é›†æ¯ä¸ªå®ä¾‹çš„ç»äººå·¥éªŒè¯çš„å‚è€ƒè¾“å‡ºï¼› 2ï¼‰ä½¿ç”¨çº¯æ–‡æœ¬æ³•å­¦ç¡•å£«è‡ªåŠ¨è¯„ä¼°å€™é€‰å¤šæ¨¡å¼ç”Ÿæˆï¼Œä¸äººç±»åˆ¤æ–­ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬ä½¿ç”¨äººå·¥å’Œè‡ªåŠ¨è¯„ä¼°æ¥é‡åŒ–æ¨¡å‹å’Œå‚è€ƒä¹‹é—´çš„è´¨é‡å·®è·ï¼›ä¾‹å¦‚ï¼Œæ€§èƒ½æœ€ä½³çš„æŒ‡ä»¤è·Ÿè¸ªæ¨¡å‹ä»…åœ¨ 27% çš„æ¯”è¾ƒä¸­èƒœè¿‡ GPT-4 å‚è€ƒæ¨¡å‹ã€‚ VisIT-Benchæ˜¯åŠ¨æ€å‚ä¸çš„ï¼Œä»ä¸šè€…åªéœ€åœ¨é¡¹ç›®ç½‘ç«™ä¸Šæäº¤ä»–ä»¬çš„æ¨¡å‹å“åº”å³å¯ï¼›æ•°æ®ã€ä»£ç å’Œæ’è¡Œæ¦œå¯åœ¨ https://visit-bench.github.io/ è·å–ã€‚

## Test-Time Distribution Normalization for Contrastively Learned Visual-language Models<sup>poster<sup>

Authors: Yifei Zhou, Juntao Ren, Fengyu Li, Ramin Zabih, Ser Nam Lim

Link: [https://neurips.cc/virtual/2023/poster/71464](https://neurips.cc/virtual/2023/poster/71464)

Abstract:

 Advances in the field of visual-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations.  One of the most representative approaches proposed recently known as CLIP has quickly garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper however reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should ideally also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference in a computationally efficient way. We propose Distribution Normalization (DN), where we approximate the mean representation of a batch of test samples and use such a mean to represent what would be analogous to negative samples in the InfoNCE loss. DN requires no retraining or fine-tuning and can be effortlessly applied during inference. Extensive experiments on a wide variety of downstream tasks exhibit a clear advantage of DN over the dot product on top of other existing test-time augmentation methods.

æ‘˜è¦:

è§†è§‰è¯­è¨€å¯¹æ¯”å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ä½¿å¾—è®¸å¤šä¸‹æ¸¸åº”ç”¨ç¨‹åºå¯ä»¥é€šè¿‡ç®€å•åœ°è·å–å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„ç‚¹ç§¯æ¥é«˜æ•ˆã€å‡†ç¡®åœ°è¿›è¡Œã€‚æœ€è¿‘æå‡ºçš„æœ€å…·ä»£è¡¨æ€§çš„æ–¹æ³•ä¹‹ä¸€ç§°ä¸º CLIPï¼Œç”±äºå…¶æœ‰æ•ˆæ€§è€Œè¿…é€Ÿè·å¾—äº†å¹¿æ³›é‡‡ç”¨ã€‚ CLIP ä½¿ç”¨ InfoNCE æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œè¯¥æŸå¤±åŒæ—¶è€ƒè™‘æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼Œä»¥å¸®åŠ©å­¦ä¹ æ›´ç¨³å¥çš„è¡¨ç¤ºç©ºé—´ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ­ç¤ºäº†é‡‡ç”¨ç‚¹ç§¯çš„å¸¸è§ä¸‹æ¸¸å®è·µåªæ˜¯ä¼˜åŒ–ç›®æ ‡çš„é›¶é˜¶è¿‘ä¼¼ï¼Œå¯¼è‡´æµ‹è¯•æœŸé—´ä¿¡æ¯ä¸¢å¤±ã€‚ç›´è§‚ä¸Šï¼Œç”±äºæ¨¡å‹æ˜¯æ ¹æ® InfoNCE æŸå¤±è¿›è¡Œä¼˜åŒ–çš„ï¼Œå› æ­¤ç†æƒ³æƒ…å†µä¸‹æµ‹è¯•æ—¶é—´ç¨‹åºä¹Ÿåº”è¯¥ä¿æŒä¸€è‡´ã€‚é—®é¢˜åœ¨äºå¦‚ä½•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»¥ä¸€ç§è®¡ç®—æœ‰æ•ˆçš„æ–¹å¼æ£€ç´¢ä»»ä½•ç±»ä¼¼çš„è´Ÿæ ·æœ¬ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºåˆ†å¸ƒå½’ä¸€åŒ–ï¼ˆDNï¼‰ï¼Œæˆ‘ä»¬è¿‘ä¼¼ä¸€æ‰¹æµ‹è¯•æ ·æœ¬çš„å‡å€¼è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨è¿™æ ·çš„å‡å€¼æ¥è¡¨ç¤ºä¸ InfoNCE æŸå¤±ä¸­çš„è´Ÿæ ·æœ¬ç±»ä¼¼çš„å†…å®¹ã€‚ DN ä¸éœ€è¦é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­è½»æ¾åº”ç”¨ã€‚å¯¹å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å…¶ä»–ç°æœ‰çš„æµ‹è¯•æ—¶é—´å¢å¼ºæ–¹æ³•ä¹‹ä¸Šï¼ŒDN ç›¸å¯¹äºç‚¹ç§¯å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

## CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models<sup>poster<sup>

Authors: Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng LYU, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, Bernhard SchÃ¶lkopf

Link: [https://neurips.cc/virtual/2023/poster/70983](https://neurips.cc/virtual/2023/poster/70983)

Abstract:

 The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the â€œcausal inference engineâ€ postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs

æ‘˜è¦:

æ‰§è¡Œå› æœæ¨ç†çš„èƒ½åŠ›è¢«å¹¿æ³›è®¤ä¸ºæ˜¯æ™ºåŠ›çš„æ ¸å¿ƒç‰¹å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ˜¯å¦èƒ½å¤Ÿè¿è´¯åœ°æ¨ç†å› æœå…³ç³»ã€‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„å¤§éƒ¨åˆ†ç°æœ‰å·¥ä½œéƒ½ä¾§é‡äºè¯„ä¼°æ³•å­¦ç¡•å£«ä¸­çš„å¸¸è¯†å› æœæ¨ç†ï¼Œå› æ­¤æ— æ³•è¯„ä¼°æ¨¡å‹æ˜¯å¦å¯ä»¥æ ¹æ®ä¸€ç»„æ˜ç¡®å®šä¹‰çš„å½¢å¼è§„åˆ™æ‰§è¡Œå› æœæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ NLP ä»»åŠ¡ï¼Œå³è‡ªç„¶è¯­è¨€ä¸­çš„å› æœæ¨ç†ï¼Œå—åˆ° Judea Pearl ç­‰äººæå‡ºçš„â€œå› æœæ¨ç†å¼•æ“â€çš„å¯å‘ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å« 10K æ ·æœ¬çš„å¤§å‹æ•°æ®é›† CLadderï¼šåŸºäºå› æœå›¾å’ŒæŸ¥è¯¢ï¼ˆå…³è”ã€å¹²é¢„å’Œåäº‹å®ï¼‰çš„é›†åˆï¼Œæˆ‘ä»¬é€šè¿‡é¢„è¨€æœºå› æœæ¨ç†å¼•æ“è·å¾—ç¬¦å·é—®é¢˜å’ŒçœŸå®ç­”æ¡ˆã€‚ç„¶åå°†å®ƒä»¬ç¿»è¯‘æˆè‡ªç„¶è¯­è¨€ã€‚æˆ‘ä»¬åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šä¸ªæ³•å­¦ç¡•å£«ï¼Œå¹¶å¼•å…¥å¹¶è¯„ä¼°äº†å®šåˆ¶çš„æ€ç»´é“¾æç¤ºç­–ç•¥ CausalCoTã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡å¯¹äºæ³•å­¦ç¡•å£«æ¥è¯´éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥çš„åˆ†æï¼Œä»¥æ›´æ·±å…¥åœ°äº†è§£æ³•å­¦ç¡•å£«çš„å› æœæ¨ç†èƒ½åŠ›

## DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining<sup>poster<sup>

Authors: Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, Adams Wei Yu

Link: [https://neurips.cc/virtual/2023/poster/70588](https://neurips.cc/virtual/2023/poster/70588)

Abstract:

 The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.

æ‘˜è¦:

é¢„è®­ç»ƒæ•°æ®åŸŸï¼ˆä¾‹å¦‚ç»´åŸºç™¾ç§‘ã€ä¹¦ç±ã€ç½‘ç»œæ–‡æœ¬ï¼‰çš„æ··åˆæ¯”ä¾‹æå¤§åœ°å½±å“è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é‡‡ç”¨æœ€å°æœ€å¤§ä¼˜åŒ–çš„åŸŸé‡æ–°åŠ æƒï¼ˆDoReMiï¼‰ï¼Œå®ƒé¦–å…ˆä½¿ç”¨åŸŸä¸Šçš„ç»„åˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆGroup DROï¼‰æ¥è®­ç»ƒä¸€ä¸ªå°å‹ä»£ç†æ¨¡å‹ï¼Œä»¥åœ¨ä¸äº†è§£ä¸‹æ¸¸ä»»åŠ¡çš„æƒ…å†µä¸‹äº§ç”ŸåŸŸæƒé‡ï¼ˆæ··åˆæ¯”ä¾‹ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›åŸŸæƒé‡å¯¹æ•°æ®é›†è¿›è¡Œé‡æ–°é‡‡æ ·ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªæ›´å¤§çš„å…¨å°ºå¯¸æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨ 280M å‚æ•°ä»£ç†æ¨¡å‹ä¸Šä½¿ç”¨ DoReMi æ¥æŸ¥æ‰¾åŸŸæƒé‡ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è®­ç»ƒ 8B å‚æ•°æ¨¡å‹ï¼ˆå¤§ 30 å€ï¼‰ã€‚åœ¨ The Pile ä¸Šï¼ŒDoReMi æ”¹å–„äº†æ‰€æœ‰é¢†åŸŸçš„å¤æ‚æ€§ï¼Œå³ä½¿å®ƒé™ä½äº†æŸä¸ªé¢†åŸŸçš„æƒé‡ã€‚ä¸ä½¿ç”¨ The Pile é»˜è®¤åŸŸæƒé‡è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒDoReMi å°†å¹³å‡å°‘æ ·æœ¬ä¸‹æ¸¸å‡†ç¡®åº¦æé«˜äº† 6.5%ï¼Œå¹¶ä»¥å‡å°‘ 2.6 å€çš„è®­ç»ƒæ­¥éª¤è¾¾åˆ°åŸºçº¿å‡†ç¡®åº¦ã€‚åœ¨ GLaM æ•°æ®é›†ä¸Šï¼Œä¸äº†è§£ä¸‹æ¸¸ä»»åŠ¡çš„ DoReMi ç”šè‡³å¯ä»¥ä¸ä½¿ç”¨é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è°ƒæ•´çš„åŸŸæƒé‡çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚

## Counterfactual Memorization in Neural Language Models<sup>poster<sup>

Authors: Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramer, Nicholas Carlini

Link: [https://neurips.cc/virtual/2023/poster/72772](https://neurips.cc/virtual/2023/poster/72772)

Abstract:

 Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive information from their training data.Understanding this memorization is important in real world applications and also from a learning-theoretical perspective. An open question in previous studies of language model memorization is how to filter out ``common'' memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data.We formulate a notion of counterfactual memorization which characterizes how a model's predictions change if a particular document is omitted during training.We identify and study counterfactually-memorized training examples in standard text datasets.We estimate the influence of each memorized training example on the validation set and on generated texts, showing how this can provide direct evidence of the source of memorization at test time.

æ‘˜è¦:

å¹¿æ³›åº”ç”¨äºå„ç§ NLP ä»»åŠ¡çš„ç°ä»£ç¥ç»è¯­è¨€æ¨¡å‹å­˜åœ¨è®°å¿†è®­ç»ƒæ•°æ®ä¸­æ•æ„Ÿä¿¡æ¯çš„é£é™©ã€‚ç†è§£è¿™ç§è®°å¿†å¯¹äºç°å®ä¸–ç•Œçš„åº”ç”¨ä»¥åŠä»å­¦ä¹ ç†è®ºçš„è§’åº¦æ¥çœ‹éƒ½å¾ˆé‡è¦ã€‚å…ˆå‰è¯­è¨€æ¨¡å‹è®°å¿†ç ”ç©¶ä¸­çš„ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜æ˜¯å¦‚ä½•è¿‡æ»¤æ‰â€œå¸¸è§â€è®°å¿†ã€‚äº‹å®ä¸Šï¼Œå¤§å¤šæ•°è®°å¿†æ ‡å‡†ä¸è®­ç»ƒé›†ä¸­å‡ºç°çš„æ¬¡æ•°å¯†åˆ‡ç›¸å…³ï¼Œæ•è·è®°å¿†çš„ç†Ÿæ‚‰çŸ­è¯­ã€å…¬å…±çŸ¥è¯†ã€æ¨¡æ¿æ–‡æœ¬æˆ–å…¶ä»–é‡å¤æ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†åäº‹å®è®°å¿†çš„æ¦‚å¿µï¼Œå®ƒæè¿°äº†æ¨¡å‹çš„é¢„æµ‹å¦‚ä½•æ”¹å˜ï¼Œå¦‚æœåœ¨è®­ç»ƒæœŸé—´çœç•¥ç‰¹å®šæ–‡æ¡£ã€‚æˆ‘ä»¬è¯†åˆ«å¹¶ç ”ç©¶æ ‡å‡†æ–‡æœ¬æ•°æ®é›†ä¸­åäº‹å®è®°å¿†çš„è®­ç»ƒç¤ºä¾‹ã€‚æˆ‘ä»¬ä¼°è®¡æ¯ä¸ªè®°å¿†çš„è®­ç»ƒç¤ºä¾‹å¯¹éªŒè¯é›†å’Œç”Ÿæˆæ–‡æœ¬çš„å½±å“ï¼Œå±•ç¤ºè¿™å¦‚ä½•æä¾›ç›´æ¥è¯æ®æ¥æºè€ƒè¯•æ—¶çš„è®°å¿†ã€‚

## Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models<sup>poster<sup>

Authors: Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong Ji

Link: [https://neurips.cc/virtual/2023/poster/70226](https://neurips.cc/virtual/2023/poster/70226)

Abstract:

 Recently,  growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and  affordable  solution for the effective VL adaption of LLMs, called  Mixture-of-Modality Adaptation (MMA).  Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to  bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an  automatic  shift between single- and multi-modal instructions without compromising their ability of natural language understanding.  To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN.  To validate MMA and LaVIN, we conduct extensive experiments  under two setups, namely  multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency  of LaVIN  than existing multimodal LLMs, but also confirm  its  great potential   as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA.   Our  code is anonymously released at:  https://anonymous.4open.science/r/LaVIN--1067.

æ‘˜è¦:

æœ€è¿‘ï¼Œäººä»¬å¯¹æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ¨¡æ€èƒ½åŠ›è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œä¾‹å¦‚è§†è§‰è¯­è¨€ï¼ˆVLï¼‰å­¦ä¹ ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯é€šç”¨äººå·¥æ™ºèƒ½çš„ä¸‹ä¸€ä¸ªé‡Œç¨‹ç¢‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆæˆæœ¬é«˜æ˜‚ï¼Œä¸ä»…éœ€è¦ä¼˜åŒ–è¿‡å¤šçš„å‚æ•°ï¼Œè€Œä¸”åœ¨VLæŒ‡ä»¤è°ƒä¼˜ä¹‹å‰è¿˜éœ€è¦å†æ¬¡è¿›è¡Œå¤§è§„æ¨¡çš„é¢„è®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”ç»æµå®æƒ çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äº LLM çš„æœ‰æ•ˆ VL é€‚åº”ï¼Œç§°ä¸ºæ··åˆæ¨¡æ€é€‚åº”ï¼ˆMMAï¼‰ã€‚ MMAæ²¡æœ‰ä½¿ç”¨å¤§å‹ç¥ç»ç½‘ç»œæ¥è¿æ¥å›¾åƒç¼–ç å™¨å’ŒLLMï¼Œè€Œæ˜¯é‡‡ç”¨è½»é‡çº§æ¨¡å—ï¼ˆå³é€‚é…å™¨ï¼‰æ¥å¼¥åˆLLMå’ŒVLä»»åŠ¡ä¹‹é—´çš„å·®è·ï¼Œè¿™ä¹Ÿä½¿å¾—å›¾åƒå’Œè¯­è¨€æ¨¡å‹çš„è”åˆä¼˜åŒ–æˆä¸ºå¯èƒ½ã€‚åŒæ—¶ï¼ŒMMAè¿˜é…å¤‡äº†è·¯ç”±ç®—æ³•ï¼Œå¸®åŠ©æ³•å­¦ç¡•å£«å®ç°å•æ¨¡æ€å’Œå¤šæ¨¡æ€æŒ‡ä»¤ä¹‹é—´çš„è‡ªåŠ¨åˆ‡æ¢ï¼Œè€Œä¸å½±å“å…¶è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†éªŒè¯ MMAï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºæœ€è¿‘çš„ä¸€ä¸ªåä¸º LLaMA çš„æ³•å­¦ç¡•å£«ï¼Œå¹¶å°†è¿™ç§å½¢æˆçš„å¤§å‹è§†è§‰è¯­è¨€æŒ‡å¯¼æ¨¡å‹ç§°ä¸º LaVINã€‚ä¸ºäº†éªŒè¯ MMA å’Œ LaVINï¼Œæˆ‘ä»¬åœ¨ä¸¤ç§è®¾ç½®ä¸‹è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå³å¤šæ¨¡æ€ç§‘å­¦é—®ç­”å’Œå¤šæ¨¡æ€å¯¹è¯ã€‚å®éªŒç»“æœä¸ä»…è¯æ˜äº† LaVIN æ¯”ç°æœ‰å¤šæ¨¡æ€ LLM çš„ç«äº‰æ€§èƒ½å’Œä¼˜è¶Šçš„è®­ç»ƒæ•ˆç‡ï¼Œè€Œä¸”è¯å®äº†å…¶ä½œä¸ºé€šç”¨èŠå¤©æœºå™¨äººçš„å·¨å¤§æ½œåŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒLaVINçš„å®é™…èŠ±è´¹æå…¶ä½å»‰ï¼Œä»…éœ€è¦1.4ä¸ªå°æ—¶çš„è®­ç»ƒæ—¶é—´å’Œ380ä¸‡ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œæå¤§åœ°è¯å®äº†MMAçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç åŒ¿åå‘å¸ƒäºï¼šhttps://anonymous.4open.science/r/LaVIN--1067ã€‚

## Fine-Tuning Language Models with Just Forward Passes<sup>oral<sup>

Authors: Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason Lee, Danqi Chen, Sanjeev Arora

Link: [https://neurips.cc/virtual/2023/poster/71437](https://neurips.cc/virtual/2023/poster/71437)

Abstract:

 Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12Ã— memory reduction and up to 2Ã— GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.

æ‘˜è¦:

å¾®è°ƒè¯­è¨€æ¨¡å‹ (LM) å·²åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†éšç€ LM è§„æ¨¡çš„å¢å¤§ï¼Œåå‘ä¼ æ’­éœ€è¦å¤§é‡çš„å†…å­˜ã€‚é›¶é˜¶ (ZO) æ–¹æ³•åŸåˆ™ä¸Šå¯ä»¥ä»…ä½¿ç”¨ä¸¤æ¬¡å‰å‘ä¼ é€’æ¥ä¼°è®¡æ¢¯åº¦ï¼Œä½†ç†è®ºä¸Šå¯¹äºä¼˜åŒ–å¤§å‹æ¨¡å‹æ¥è¯´é€Ÿåº¦éå¸¸æ…¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…å­˜é«˜æ•ˆçš„é›¶é˜¶ä¼˜åŒ–å™¨ï¼ˆMeZOï¼‰ï¼Œé‡‡ç”¨ç»å…¸çš„ ZO-SGD æ–¹æ³•è¿›è¡Œå°±åœ°æ“ä½œï¼Œä»è€Œä»¥ä¸æ¨ç†ç›¸åŒçš„å†…å­˜å ç”¨æ¥å¾®è°ƒ LMã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨å•ä¸ª A100 80GB GPUï¼ŒMeZO å¯ä»¥è®­ç»ƒ 300 äº¿ä¸ªå‚æ•°çš„æ¨¡å‹ï¼Œè€Œåå‘ä¼ æ’­å¾®è°ƒåœ¨ç›¸åŒé¢„ç®—ä¸‹åªèƒ½è®­ç»ƒ 2.7B LMã€‚æˆ‘ä»¬å¯¹æ¨¡å‹ç±»å‹ï¼ˆå±è”½å’Œè‡ªå›å½’ LMï¼‰ã€æ¨¡å‹è§„æ¨¡ï¼ˆé«˜è¾¾ 66Bï¼‰å’Œä¸‹æ¸¸ä»»åŠ¡ï¼ˆåˆ†ç±»ã€å¤šé¡¹é€‰æ‹©å’Œç”Ÿæˆï¼‰è¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ˆ1ï¼‰MeZO æ˜¾ç€ä¼˜äºä¸Šä¸‹æ–‡å­¦ä¹ å’Œçº¿æ€§æ¢æµ‹ï¼› (2) MeZO é€šè¿‡è·¨å¤šä¸ªä»»åŠ¡çš„åå‘ä¼ æ’­å®ç°äº†ä¸å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼Œå†…å­˜å‡å°‘äº†é«˜è¾¾ 12 å€ï¼ŒGPU å°æ—¶å‡å°‘äº†é«˜è¾¾ 2 å€ï¼› ï¼ˆ3ï¼‰MeZOå…¼å®¹å…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆçš„è°ƒè°æŠ€æœ¯ï¼Œä¾‹å¦‚LoRAå’Œå‰ç¼€è°ƒè°ï¼› (4) MeZOå¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–ä¸å¯å¾®ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œæœ€å¤§åŒ–å‡†ç¡®ç‡æˆ–F1ï¼‰ã€‚æˆ‘ä»¬ç”¨ç†è®ºè§è§£æ”¯æŒæˆ‘ä»¬çš„å®è¯ç ”ç©¶ç»“æœï¼Œå¼ºè°ƒå……åˆ†çš„é¢„è®­ç»ƒå’Œä»»åŠ¡æç¤ºå¦‚ä½•ä½¿ MeZO èƒ½å¤Ÿå¾®è°ƒå·¨å¤§çš„æ¨¡å‹ï¼Œå°½ç®¡ç»å…¸çš„ ZO åˆ†æè¡¨æ˜æƒ…å†µå¹¶éå¦‚æ­¤ã€‚

## ParselğŸ: Algorithmic Reasoning with Language Models by Composing Decompositions<sup>poster<sup>

Authors: Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, Nick Haber

Link: [https://neurips.cc/virtual/2023/poster/70349](https://neurips.cc/virtual/2023/poster/70349)

Abstract:

 Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\% to 85\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel.

æ‘˜è¦:

å°½ç®¡æœ€è¿‘åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½† LLM ä»éš¾ä»¥å¤„ç†åˆ†å±‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚ç”Ÿæˆå¤æ‚çš„ç¨‹åºã€‚å¯¹äºè¿™äº›ä»»åŠ¡ï¼Œäººç±»é€šå¸¸ä»é«˜çº§ç®—æ³•è®¾è®¡å¼€å§‹ï¼Œé€æ­¥å®ç°æ¯ä¸ªéƒ¨åˆ†ã€‚æˆ‘ä»¬å¼•å…¥äº† Parselï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä½¿ç”¨ä»£ç æ³•å­¦ç¡•å£«è‡ªåŠ¨å®ç°å’ŒéªŒè¯å¤æ‚ç®—æ³•çš„æ¡†æ¶ã€‚ä½¿ç”¨ Parselï¼Œæˆ‘ä»¬è‡ªåŠ¨å°†ç®—æ³•ä»»åŠ¡åˆ†è§£ä¸ºåˆ†å±‚è‡ªç„¶è¯­è¨€å‡½æ•°æè¿°ï¼Œç„¶åä½¿ç”¨æµ‹è¯•æœç´¢å¯èƒ½çš„å‡½æ•°å®ç°çš„ç»„åˆã€‚æˆ‘ä»¬è¯æ˜ Parsel å¯ä»¥è·¨éœ€è¦åˆ†å±‚æ¨ç†çš„é¢†åŸŸä½¿ç”¨ï¼ŒåŒ…æ‹¬ç¨‹åºç»¼åˆå’Œæœºå™¨äººè§„åˆ’ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLLM ä½¿ç”¨ Parsel è§£å†³äº† APPS æ•°æ®é›†ä¸­æ›´å¤šçš„ç«äº‰çº§åˆ«é—®é¢˜ï¼Œå…¶é€šè¿‡ç‡æ¯”ä¹‹å‰ç›´æ¥é‡‡æ · AlphaCode å’Œ Codex çš„ç»“æœé«˜å‡º 75% ä»¥ä¸Šï¼ŒåŒæ—¶é€šå¸¸ä½¿ç”¨è¾ƒå°çš„æ ·æœ¬é¢„ç®—ã€‚æ­¤å¤–ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç° Parsel å¯ä»¥å°† HumanEval ä¸Šæœ€å…ˆè¿›çš„ pass@1 æ€§èƒ½ä» 67% æé«˜åˆ° 85%ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä½¿ç”¨ Parsel çš„ LLM ç”Ÿæˆçš„æœºå™¨äººè®¡åˆ’è¢«è®¤ä¸ºå‡†ç¡®çš„å¯èƒ½æ€§æ˜¯ç›´æ¥ç”Ÿæˆçš„è®¡åˆ’çš„ä¸¤å€å¤šã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨ Parsel å¦‚ä½•è§£å†³ LLM é™åˆ¶ï¼Œå¹¶è®¨è®º Parsel å¦‚ä½•å¯¹äººç±»ç¨‹åºå‘˜æœ‰ç”¨ã€‚æˆ‘ä»¬åœ¨ https://github.com/ezelikman/parsel å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚

## MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks<sup>poster<sup>

Authors: Allen Nie, Yuhui Zhang, Atharva Shailesh Amdekar, Chris Piech, Tatsunori Hashimoto, Tobias Gerstenberg

Link: [https://neurips.cc/virtual/2023/poster/71498](https://neurips.cc/virtual/2023/poster/71498)

Abstract:

 Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.

æ‘˜è¦:

äººç±»å¯¹ç‰©ç†å’Œç¤¾ä¼šä¸–ç•Œçš„å¸¸è¯†æ€§ç†è§£æ˜¯å›´ç»•ç›´è§‰ç†è®ºç»„ç»‡çš„ã€‚è¿™äº›ç†è®ºæ”¯æŒåšå‡ºå› æœå’Œé“å¾·åˆ¤æ–­ã€‚å½“ä¸å¥½çš„äº‹æƒ…å‘ç”Ÿæ—¶ï¼Œæˆ‘ä»¬è‡ªç„¶ä¼šé—®ï¼šè°åšäº†ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆï¼Ÿè®¤çŸ¥ç§‘å­¦é¢†åŸŸçš„ä¸°å¯Œæ–‡çŒ®ç ”ç©¶äº†äººä»¬çš„å› æœç›´è§‰å’Œé“å¾·ç›´è§‰ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†è®¸å¤šç³»ç»Ÿæ€§åœ°å½±å“äººä»¬åˆ¤æ–­çš„å› ç´ ï¼Œä¾‹å¦‚è¿åè§„èŒƒä»¥åŠä¼¤å®³æ˜¯å¦å¯ä»¥é¿å…æˆ–ä¸å¯é¿å…ã€‚æˆ‘ä»¬ä» 24 ç¯‡è®¤çŸ¥ç§‘å­¦è®ºæ–‡ä¸­æ”¶é›†äº†æ•…äº‹æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿï¼Œç”¨ä»–ä»¬è°ƒæŸ¥çš„å› ç´ æ¥æ³¨é‡Šæ¯ä¸ªæ•…äº‹ã€‚ä½¿ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ˜¯å¦å¯¹åŸºäºæ–‡æœ¬çš„åœºæ™¯åšå‡ºä¸äººç±»å‚ä¸è€…ä¸€è‡´çš„å› æœå’Œé“å¾·åˆ¤æ–­ã€‚æ€»ä½“è€Œè¨€ï¼Œä¸æœ€è¿‘çš„æ³•å­¦ç¡•å£«çš„ä¸€è‡´æ€§æœ‰æ‰€æ”¹å–„ã€‚ç„¶è€Œï¼Œé€šè¿‡ç»Ÿè®¡åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ³•å­¦ç¡•å£«å¯¹ä¸åŒå› ç´ çš„æƒè¡¡ä¸äººç±»å‚ä¸è€…æˆªç„¶ä¸åŒã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒç­–åˆ’çš„æŒ‘æˆ˜æ•°æ®é›†ä¸è®¤çŸ¥ç§‘å­¦çš„è§è§£ç›¸ç»“åˆå¦‚ä½•å¸®åŠ©æˆ‘ä»¬è¶…è¶Šä»…ä»…åŸºäºæ€»ä½“æŒ‡æ ‡çš„æ¯”è¾ƒï¼šæˆ‘ä»¬å‘ç°æ³•å­¦ç¡•å£«çš„éšå«å€¾å‘ï¼Œå¹¶æ˜¾ç¤ºè¿™äº›å€¾å‘åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¸äººç±»ç›´è§‰ç›¸ç¬¦ã€‚

## Pengi: An Audio Language Model for Audio Tasks<sup>poster<sup>

Authors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang

Link: [https://neurips.cc/virtual/2023/poster/70860](https://neurips.cc/virtual/2023/poster/70860)

Abstract:

 In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 21 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding.

æ‘˜è¦:

åœ¨éŸ³é¢‘å¤„ç†é¢†åŸŸï¼Œè¿ç§»å­¦ä¹ ä¿ƒè¿›äº†è‡ªç›‘ç£å­¦ä¹ å’Œé›¶æ ·æœ¬å­¦ä¹ æŠ€æœ¯çš„å…´èµ·ã€‚è¿™äº›æ–¹æ³•å¯¼è‡´äº†å¤šåŠŸèƒ½æ¨¡å‹çš„å¼€å‘ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿå¤„ç†å„ç§ä»»åŠ¡ï¼ŒåŒæ—¶æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¨¡å‹æœ¬è´¨ä¸Šç¼ºä¹ä¸ºå¼€æ”¾å¼ä»»åŠ¡ï¼ˆä¾‹å¦‚éŸ³é¢‘å­—å¹•æˆ–éŸ³é¢‘é—®ç­”ï¼‰ç”Ÿæˆæ‰€éœ€è¯­è¨€çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç» Pengiï¼Œä¸€ç§æ–°é¢–çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡å°†æ‰€æœ‰éŸ³é¢‘ä»»åŠ¡æ„å»ºä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ¥åˆ©ç”¨è¿ç§»å­¦ä¹ ã€‚å®ƒä»¥å½•éŸ³å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆè‡ªç”±æ ¼å¼çš„æ–‡æœ¬ä½œä¸ºè¾“å‡ºã€‚è¾“å…¥éŸ³é¢‘ç”±éŸ³é¢‘ç¼–ç å™¨è¡¨ç¤ºä¸ºä¸€ç³»åˆ—è¿ç»­åµŒå…¥ã€‚æ–‡æœ¬ç¼–ç å™¨å¯¹ç›¸åº”çš„æ–‡æœ¬è¾“å…¥æ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚ä¸¤ä¸ªåºåˆ—ç»„åˆèµ·æ¥ä½œä¸ºå‰ç¼€ï¼Œä»¥æç¤ºé¢„è®­ç»ƒçš„å†»ç»“è¯­è¨€æ¨¡å‹ã€‚ Pengiçš„ç»Ÿä¸€æ¶æ„æ”¯æŒå¼€æ”¾å¼ä»»åŠ¡å’Œå°é—­å¼ä»»åŠ¡ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„å¾®è°ƒæˆ–ç‰¹å®šäºä»»åŠ¡çš„æ‰©å±•ã€‚å½“å¯¹ 21 ä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä¸­å‡ ä¸ªä»»åŠ¡ä¸­äº§ç”Ÿäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†è¯­è¨€æ¨¡å‹ä¸éŸ³é¢‘æ¨¡å‹è¿æ¥èµ·æ¥æ˜¯è¿ˆå‘é€šç”¨éŸ³é¢‘ç†è§£çš„é‡è¦ä¸€æ­¥ã€‚

## Language Model Alignment with Elastic Reset<sup>poster<sup>

Authors: Michael Noukhovitch, Samuel Lavoie, Florian Strub, Aaron Courville

Link: [https://neurips.cc/virtual/2023/poster/72738](https://neurips.cc/virtual/2023/poster/72738)

Abstract:

 Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available https://github.com/mnoukhov/elastic-reset

æ‘˜è¦:

é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼ˆä¾‹å¦‚æ ¹æ®äººç±»åé¦ˆï¼ˆHFï¼‰ï¼‰å¾®è°ƒè¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§é‡è¦çš„å¯¹é½æ–¹æ³•ã€‚ä½†é’ˆå¯¹å¥–åŠ±æ¨¡å‹è¿›è¡Œä¼˜åŒ–å¯èƒ½ä¼šæé«˜å¥–åŠ±ï¼ŒåŒæ—¶é™ä½å…¶ä»–é¢†åŸŸçš„è¡¨ç°ï¼Œè¿™ç§ç°è±¡ç§°ä¸ºå¥–åŠ±é»‘å®¢ã€å¯¹é½ç¨æˆ–è¯­è¨€æ¼‚ç§»ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¤ä¸ºå¸¸ç”¨çš„æµ‹è¯•æŒ‡æ ‡æ˜¯ä¸å¤Ÿçš„ï¼Œè€Œæ˜¯è¡¡é‡ä¸åŒç®—æ³•å¦‚ä½•åœ¨å¥–åŠ±å’Œæ¼‚ç§»ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æ ‡å‡†æ–¹æ³•é€šè¿‡åœ¨çº¿æ¨¡å‹å’Œåˆå§‹æ¨¡å‹ä¹‹é—´çš„ Kullback-Lieber (KL) æƒ©ç½šæ¥ä¿®æ”¹å¥–åŠ±ã€‚æˆ‘ä»¬æå‡ºäº† Elastic Resetï¼Œè¿™æ˜¯ä¸€ç§æ–°ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¸æ˜¾å¼ä¿®æ”¹è®­ç»ƒç›®æ ‡çš„æƒ…å†µä¸‹ä»¥æ›´å°‘çš„æ¼‚ç§»å®ç°æ›´é«˜çš„å¥–åŠ±ã€‚æˆ‘ä»¬å®šæœŸå°†åœ¨çº¿æ¨¡å‹é‡ç½®ä¸ºå…¶è‡ªèº«çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿ (EMA)ï¼Œç„¶åå°† EMA æ¨¡å‹é‡ç½®ä¸ºåˆå§‹æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨ EMAï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é‡ç½®åå¯ä»¥å¿«é€Ÿæ¢å¤ï¼Œå¹¶åœ¨ç›¸åŒæ­¥æ•°ä¸‹ä»¥æ›´å°‘çš„æ¼‚ç§»è·å¾—æ›´é«˜çš„å¥–åŠ±ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨ Elastic Reset å¾®è°ƒè¯­è¨€æ¨¡å‹å¯ä»¥åœ¨å°è§„æ¨¡æ¢è½´ç¿»è¯‘åŸºå‡†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä¸­ç­‰è§„æ¨¡çš„ç±»ä¼¼ RLHF çš„ IMDB æ¨¡æ‹Ÿæƒ…æ„Ÿä»»åŠ¡ä¸­ä¼˜äºæ‰€æœ‰åŸºå‡†ï¼Œå¹¶å¯¼è‡´æ›´ä½¿ç”¨ LLaMA-7B å®ç°é«˜æ€§èƒ½ä¸”æ›´åŠ ä¸€è‡´çš„æŠ€æœ¯ QA èŠå¤©æœºå™¨äººã€‚å¯ç”¨ä»£ç  https://github.com/mnoukhov/elastic-reset

## HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models<sup>poster<sup>

Authors: CHEN CHEN, Yuchen Hu, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Pin-Yu Chen, Eng-Siong Chng

Link: [https://neurips.cc/virtual/2023/poster/73533](https://neurips.cc/virtual/2023/poster/73533)

Abstract:

 Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as output transcription. The proposed benchmark contains a novel dataset, "HyPoradise" (HP), encompassing more than 316,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. More surprisingly, LLM with reasonable prompt design can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new paradigm for ASR error correction with LLMs.

æ‘˜è¦:

æ·±åº¦ç¥ç»ç½‘ç»œçš„è¿›æ­¥ä½¿å¾—è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿèƒ½å¤Ÿåœ¨å‡ ä¸ªå…¬å¼€å¯ç”¨çš„å¹²å‡€è¯­éŸ³æ•°æ®é›†ä¸Šè¾¾åˆ°äººç±»çš„åŒç­‰æ°´å¹³ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ ASR ç³»ç»Ÿåœ¨é‡åˆ°ä¸åˆ©æ¡ä»¶æ—¶ä¹Ÿä¼šç»å†æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºè®­ç»ƒæœ‰ç´ çš„å£°å­¦æ¨¡å‹å¯¹è¯­éŸ³åŸŸä¸­çš„å˜åŒ–ï¼ˆä¾‹å¦‚èƒŒæ™¯å™ªå£°ï¼‰å¾ˆæ•æ„Ÿã€‚ç›´è§‚ä¸Šï¼Œäººç±»ä¾é ä»–ä»¬çš„è¯­è¨€çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šæ¨¡ç³Šçš„å£å¤´æœ¯è¯­çš„å«ä¹‰é€šå¸¸æ˜¯ä»ä¸Šä¸‹æ–‡çº¿ç´¢æ¨æ–­å‡ºæ¥çš„ï¼Œä»è€Œå‡å°‘äº†å¯¹å¬è§‰ç³»ç»Ÿçš„ä¾èµ–ã€‚å—è¿™ä¸€è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªå¼€æºåŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨å¤–éƒ¨å¤§è¯­è¨€æ¨¡å‹ (LLM) è¿›è¡Œ ASR çº é”™ï¼Œå…¶ä¸­ N æœ€ä½³è§£ç å‡è®¾ä¸ºçœŸå®çš„è½¬å½•é¢„æµ‹æä¾›äº†ä¿¡æ¯å…ƒç´ ã€‚è¿™ç§æ–¹æ³•æ˜¯ä¼ ç»Ÿè¯­è¨€æ¨¡å‹é‡æ–°è¯„åˆ†ç­–ç•¥çš„èŒƒå¼è½¬å˜ï¼Œä¼ ç»Ÿè¯­è¨€æ¨¡å‹é‡æ–°è¯„åˆ†ç­–ç•¥åªèƒ½é€‰æ‹©ä¸€ä¸ªå€™é€‰å‡è®¾ä½œä¸ºè¾“å‡ºè½¬å½•ã€‚æ‹Ÿè®®çš„åŸºå‡†åŒ…å«ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†â€œHyPoradiseâ€ï¼ˆHPï¼‰ï¼ŒåŒ…å«è¶…è¿‡ 316,000 å¯¹ N æœ€ä½³å‡è®¾ä»¥åŠè·¨æµè¡Œè¯­éŸ³é¢†åŸŸçš„ç›¸åº”å‡†ç¡®è½¬å½•ã€‚ç»™å®šè¿™ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†ä¸‰ç§åŸºäºæ³•å­¦ç¡•å£«çš„çº é”™æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å…·æœ‰ä¸åŒæ•°é‡çš„æ ‡è®°å‡è®¾-è½¬å½•å¯¹ï¼Œä»è€Œæ˜¾ç€é™ä½äº†å•è¯é”™è¯¯ç‡ (WER)ã€‚å®éªŒè¯æ®è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æŠ€æœ¯é€šè¿‡è¶…è¶Šä¼ ç»ŸåŸºäºé‡æ’åºçš„æ–¹æ³•çš„ä¸Šé™å®ç°äº†çªç ´ã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå…·æœ‰åˆç†æç¤ºè®¾è®¡çš„LLMç”šè‡³å¯ä»¥çº æ­£N-beståˆ—è¡¨ä¸­ç¼ºå¤±çš„é‚£äº›æ ‡è®°ã€‚æˆ‘ä»¬é€šè¿‡å‘å¸ƒçš„é¢„è®­ç»ƒæ¨¡å‹å°†æˆ‘ä»¬çš„ç»“æœå…¬å¼€ç”¨äºå¯é‡å¤çš„ç®¡é“ï¼Œä»è€Œä¸ºæ³•å­¦ç¡•å£«çš„ ASR çº é”™æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚

## Guiding Large Language Models via Directional Stimulus Prompting<sup>poster<sup>

Authors: Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan

Link: [https://neurips.cc/virtual/2023/poster/71484](https://neurips.cc/virtual/2023/poster/71484)

Abstract:

 We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) towards specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We evaluate our method across various tasks, including summarization, dialogue response generation, and chain-of-thought reasoning. Our experiments indicate a consistent improvement in the performance of LLMs such as ChatGPT, Codex, and InstructGPT on these supervised tasks with minimal labeled data. Remarkably, by utilizing merely 80 dialogues from the MultiWOZ dataset, our approach boosts ChatGPT's performance by a relative 41.4%, achieving or exceeding the performance of some fully supervised state-of-the-art models. Moreover, the instance-specific chain-of-thought prompt generated through our method enhances InstructGPT's reasoning accuracy, outperforming both generalized human-crafted prompts and those generated through automatic prompt engineering. The code and data are publicly available at https://github.com/Leezekun/Directional-Stimulus-Prompting.

æ‘˜è¦:

æˆ‘ä»¬å¼•å…¥äº†å®šå‘åˆºæ¿€æç¤ºï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºæŒ‡å¯¼é»‘ç›’å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°ç‰¹å®šçš„æ‰€éœ€è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯ç›´æ¥è°ƒæ•´ LLMï¼Œè€Œæ˜¯é‡‡ç”¨ä¸€ä¸ªå°å‹å¯è°ƒç­–ç•¥æ¨¡å‹ï¼ˆä¾‹å¦‚ T5ï¼‰æ¥ä¸ºæ¯ä¸ªè¾“å…¥å®ä¾‹ç”Ÿæˆè¾…åŠ©å®šå‘åˆºæ¿€æç¤ºã€‚è¿™äº›å®šå‘åˆºæ¿€æç¤ºå……å½“ç»†è‡´å…¥å¾®çš„ã€ç‰¹å®šäºå®ä¾‹çš„æç¤ºå’Œçº¿ç´¢ï¼ŒæŒ‡å¯¼æ³•å­¦ç¡•å£«ç”Ÿæˆæ‰€éœ€çš„ç»“æœï¼Œä¾‹å¦‚åœ¨ç”Ÿæˆçš„æ‘˜è¦ä¸­åŒ…å«ç‰¹å®šçš„å…³é”®å­—ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ”¿ç­–æ¨¡å‹æ¥æ¢ç´¢ä½¿æ³•å­¦ç¡•å£«ä¸æœŸæœ›è¡Œä¸ºä¿æŒä¸€è‡´çš„å®šå‘åˆºæ¿€æç¤ºï¼Œä»è€Œå›é¿äº†ç›´æ¥æ³•å­¦ç¡•å£«è°ƒæ•´çš„æŒ‘æˆ˜ã€‚æ”¿ç­–æ¨¡å‹å¯ä»¥é€šè¿‡ 1ï¼‰ä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒå’Œ 2ï¼‰åŸºäº LLM è¾“å‡ºçš„ç¦»çº¿æˆ–åœ¨çº¿å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å„ç§ä»»åŠ¡ä¸­è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ‘˜è¦ã€å¯¹è¯å“åº”ç”Ÿæˆå’Œæ€æƒ³é“¾æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒChatGPTã€Codex å’Œ InstructGPT ç­‰ LLM åœ¨ä½¿ç”¨æœ€å°‘æ ‡è®°æ•°æ®çš„ç›‘ç£ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æŒç»­æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡ä»…åˆ©ç”¨ MultiWOZ æ•°æ®é›†ä¸­çš„ 80 ä¸ªå¯¹è¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°† ChatGPT çš„æ€§èƒ½æé«˜äº†ç›¸å¯¹ 41.4%ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡äº†ä¸€äº›å®Œå…¨ç›‘ç£çš„æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„ç‰¹å®šäºå®ä¾‹çš„æ€ç»´é“¾æç¤ºæé«˜äº† InstructGPT çš„æ¨ç†å‡†ç¡®æ€§ï¼Œä¼˜äºå¹¿ä¹‰çš„äººå·¥åˆ¶ä½œçš„æç¤ºå’Œé€šè¿‡è‡ªåŠ¨æç¤ºå·¥ç¨‹ç”Ÿæˆçš„æç¤ºã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ https://github.com/Leezekun/Directional-Stimulus-Prompting ä¸Šå…¬å¼€è·å–ã€‚

## Evaluating and Inducing Personality in Pre-trained Language Models<sup>poster<sup>

Authors: Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu

Link: [https://neurips.cc/virtual/2023/poster/72136](https://neurips.cc/virtual/2023/poster/72136)

Abstract:

 Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. In this study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by leveraging human psychometric tests in a **principled** and **quantitative** manner? If so, can we induce a specific personality in LLMs? To answer these questions, we introduce the Machine Personality Inventory (MPI) tool for studying machine behaviors; MPI follows standardizedpersonality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise a Personality Prompting (P$^2$) method to induce LLMs with specific personalities in a **controllable** way, capable of producing diverse and verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the essential indicator for various downstream tasks, and could further motivate research into equally intriguing human-like machine behaviors.

æ‘˜è¦:

æœºå™¨è¡Œä¸ºçš„æ ‡å‡†åŒ–å’Œé‡åŒ–è¯„ä¼°æ˜¯ç†è§£æ³•å­¦ç¡•å£«çš„å…³é”®ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨äººç±»äººæ ¼ç†è®ºä½œä¸ºç ”ç©¶æœºå™¨è¡Œä¸ºçš„å·¥å…·ï¼Œä»å¿ƒç†æµ‹é‡ç ”ç©¶ä¸­æ±²å–çµæ„Ÿã€‚äººæ ¼ç ”ç©¶èµ·æºäºå¯¹äººç±»è¡Œä¸ºçš„å“²å­¦æ¢ç´¢ï¼Œæ·±å…¥ç ”ç©¶ä¸ªä½“åœ¨æ€ç»´ã€æƒ…æ„Ÿå’Œè¡Œä¸ºæ–¹é¢çš„å·®å¼‚ã€‚ä¸ºäº†æ„å»ºå’Œç†è§£ç±»äººçš„ç¤¾äº¤æœºå™¨ï¼Œæˆ‘ä»¬ä¸ç¦è¦é—®ï¼šæˆ‘ä»¬èƒ½å¦ä»¥**æœ‰åŸåˆ™çš„**å’Œ**å®šé‡çš„**æ–¹å¼åˆ©ç”¨äººç±»å¿ƒç†æµ‹è¯•æ¥è¯„ä¼°æœºå™¨è¡Œä¸ºï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬èƒ½å¦åœ¨æ³•å­¦ç¡•å£«ä¸­å¼•å…¥ç‰¹å®šçš„ä¸ªæ€§ï¼Ÿä¸ºäº†å›ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æœºå™¨ä¸ªæ€§æ¸…å•ï¼ˆMPIï¼‰å·¥å…·æ¥ç ”ç©¶æœºå™¨è¡Œä¸ºï¼› MPI éµå¾ªåŸºäºå¤§äº”äººæ ¼å› ç´ ï¼ˆå¤§äº”ï¼‰ç†è®ºå’Œäººæ ¼è¯„ä¼°é‡è¡¨çš„æ ‡å‡†åŒ–äººæ ¼æµ‹è¯•ã€‚é€šè¿‡ä½¿ç”¨ MPI ç³»ç»Ÿåœ°è¯„ä¼°æ³•å­¦ç¡•å£«ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€ä¸ªè¯æ®ï¼Œè¯æ˜ MPI åœ¨ç ”ç©¶æ³•å­¦ç¡•å£«è¡Œä¸ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§äººæ ¼æç¤ºï¼ˆP$^2$ï¼‰æ–¹æ³•ï¼Œä»¥**å¯æ§**çš„æ–¹å¼è¯±å¯¼å…·æœ‰ç‰¹å®šäººæ ¼çš„LLMï¼Œèƒ½å¤Ÿäº§ç”Ÿå¤šæ ·åŒ–ä¸”å¯éªŒè¯çš„è¡Œä¸ºã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œé€šè¿‡é‡‡ç”¨ä¸ªæ€§ä½œä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„åŸºæœ¬æŒ‡æ ‡æ¥ä¸ºæœªæ¥çš„ç ”ç©¶å¸¦æ¥å¯ç¤ºï¼Œå¹¶å¯ä»¥è¿›ä¸€æ­¥æ¨åŠ¨å¯¹åŒæ ·æœ‰è¶£çš„ç±»äººæœºå™¨è¡Œä¸ºçš„ç ”ç©¶ã€‚

## On-the-Fly Adapting Code Summarization on Trainable Cost-Effective Language Models<sup>poster<sup>

Authors: Yufan Cai, Yun Lin, Chenyan Liu, Jinglian Wu, Yifan Zhang, Yiming Liu, Yeyun Gong, Jin Song Dong

Link: [https://neurips.cc/virtual/2023/poster/71669](https://neurips.cc/virtual/2023/poster/71669)

Abstract:

 Deep learning models are emerging to summarize source code to comment,  facilitating tasks of code documentation and program comprehension.  Scaled-up large language models trained on large open corpus have achieved good performance in such tasks.  However, in practice, the subject code in one certain project can be specific,  which may not align with the overall training corpus.  Some code samples from other projects may be contradictory and introduce inconsistencies when the models try to fit all the samples.  In this work, we introduce a novel approach, Adacom, to improve the performance of comment generators by on-the-fly model adaptation.  This research is motivated by the observation that deep comment generators  often need to strike a balance as they need to fit all the training samples.  Specifically, for one certain target code $c$,  some training samples $S_p$ could have made more contributions while other samples $S_o$ could have counter effects.  However, the traditional fine-tuned models need to fit both $S_p$ and $S_o$ from a global perspective,   leading to compromised performance for one certain target code $c$.  In this context, we design Adacom to  (1) detect whether the model might have a compromised performance on a target code $c$ and  (2) retrieve a few helpful training samples $S_p$ that have contradictory samples in the training dataset and,  (3) adapt the model on the fly by re-training the $S_p$ to strengthen the helpful samples and unlearn the harmful samples.  Our extensive experiments on 7 comment generators and 4 public datasets show that  (1) can significantly boost the performance of comment generation (BLEU4 score by on average 14.9\%, METEOR by 12.2\%, and ROUGE-L by 7.4\%),  (2) the adaptation on one code sample is cost-effective and acceptable as an on-the-fly solution, and  (3) can adapt well on out-of-distribution code samples.

æ‘˜è¦:

æ·±åº¦å­¦ä¹ æ¨¡å‹æ­£åœ¨å…´èµ·ï¼Œå¯ä»¥æ€»ç»“æºä»£ç ä»¥è¿›è¡Œæ³¨é‡Šï¼Œä»è€Œä¿ƒè¿›ä»£ç æ–‡æ¡£å’Œç¨‹åºç†è§£çš„ä»»åŠ¡ã€‚åœ¨å¤§å‹å¼€æ”¾è¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼ŒæŸä¸ªé¡¹ç›®ä¸­çš„ä¸»é¢˜ä»£ç å¯èƒ½æ˜¯ç‰¹å®šçš„ï¼Œè¿™å¯èƒ½ä¸æ•´ä½“è®­ç»ƒè¯­æ–™åº“ä¸ä¸€è‡´ã€‚æ¥è‡ªå…¶ä»–é¡¹ç›®çš„æŸäº›ä»£ç ç¤ºä¾‹å¯èƒ½ä¼šç›¸äº’çŸ›ç›¾ï¼Œå¹¶ä¸”å½“æ¨¡å‹å°è¯•æ‹Ÿåˆæ‰€æœ‰ç¤ºä¾‹æ—¶ä¼šå¼•å…¥ä¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³• Adacomï¼Œé€šè¿‡åŠ¨æ€æ¨¡å‹é€‚åº”æ¥æé«˜è¯„è®ºç”Ÿæˆå™¨çš„æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶çš„åŠ¨æœºæ˜¯è§‚å¯Ÿåˆ°æ·±åº¦è¯„è®ºç”Ÿæˆå™¨é€šå¸¸éœ€è¦å–å¾—å¹³è¡¡ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦é€‚åˆæ‰€æœ‰è®­ç»ƒæ ·æœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæŸä¸ªç›®æ ‡ä»£ç $c$ï¼ŒæŸäº›è®­ç»ƒæ ·æœ¬$S_p$å¯èƒ½ä¼šåšå‡ºæ›´å¤šè´¡çŒ®ï¼Œè€Œå…¶ä»–æ ·æœ¬$S_o$å¯èƒ½ä¼šäº§ç”Ÿç›¸åçš„æ•ˆæœã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¾®è°ƒæ¨¡å‹éœ€è¦ä»å…¨å±€è§’åº¦åŒæ—¶æ‹Ÿåˆ$S_p$å’Œ$S_o$ï¼Œå¯¼è‡´æŸä¸ªç›®æ ‡ä»£ç $c$çš„æ€§èƒ½å—åˆ°å½±å“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¾è®¡ Adacom æ¥ (1) æ£€æµ‹æ¨¡å‹åœ¨ç›®æ ‡ä»£ç  $c$ ä¸Šçš„æ€§èƒ½æ˜¯å¦å¯èƒ½å—åˆ°å½±å“ï¼Œä»¥åŠ (2) æ£€ç´¢ä¸€äº›æœ‰ç”¨çš„è®­ç»ƒæ ·æœ¬ $S_p$ï¼Œè¿™äº›æ ·æœ¬åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å…·æœ‰çŸ›ç›¾çš„æ ·æœ¬ï¼Œå¹¶ä¸”ï¼Œ (3) é€šè¿‡é‡æ–°è®­ç»ƒ $S_p$ æ¥åŠ¨æ€è°ƒæ•´æ¨¡å‹ï¼Œä»¥åŠ å¼ºæœ‰ç”¨çš„æ ·æœ¬å¹¶å¿˜è®°æœ‰å®³çš„æ ·æœ¬ã€‚æˆ‘ä»¬å¯¹ 7 ä¸ªè¯„è®ºç”Ÿæˆå™¨å’Œ 4 ä¸ªå…¬å…±æ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œ(1) å¯ä»¥æ˜¾ç€æé«˜è¯„è®ºç”Ÿæˆçš„æ€§èƒ½ï¼ˆBLEU4 å¾—åˆ†å¹³å‡æé«˜ 14.9%ï¼ŒMETEOR å¹³å‡æé«˜ 12.2%ï¼ŒROUGE-L å¹³å‡æé«˜ 7.4%ï¼‰ï¼Œ (2) å¯¹ä¸€ä¸ªä»£ç æ ·æœ¬çš„é€‚åº”ä½œä¸ºä¸€ç§å³æ—¶è§£å†³æ–¹æ¡ˆæ˜¯å…·æœ‰æˆæœ¬æ•ˆç›Šä¸”å¯æ¥å—çš„ï¼Œå¹¶ä¸” (3) å¯ä»¥å¾ˆå¥½åœ°é€‚åº”åˆ†å¸ƒå¤–çš„ä»£ç æ ·æœ¬ã€‚

## Generating Images with Multimodal Language Models<sup>poster<sup>

Authors: Jing Yu Koh, Daniel Fried, Russ Salakhutdinov

Link: [https://neurips.cc/virtual/2023/poster/71499](https://neurips.cc/virtual/2023/poster/71499)

Abstract:

 We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text â€” outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.

æ‘˜è¦:

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥ç©ºé—´ä¹‹é—´çš„æ˜ å°„ï¼Œå°†å†»ç»“çš„çº¯æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨å’Œè§£ç å™¨æ¨¡å‹èåˆã€‚æˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†ä¸€ç³»åˆ—å¹¿æ³›çš„å¤šæ¨¡æ€åŠŸèƒ½ï¼šå›¾åƒæ£€ç´¢ã€æ–°é¢–å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€å¯¹è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿå¯¹ä»»æ„äº¤é”™çš„å›¾åƒå’Œæ–‡æœ¬è¾“å…¥è¿›è¡Œè°ƒèŠ‚ä»¥ç”Ÿæˆè¿è´¯çš„å›¾åƒï¼ˆå’Œæ–‡æœ¬ï¼‰è¾“å‡ºçš„æ–¹æ³•ã€‚ä¸ºäº†åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å®ç°å¼ºå¤§çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ˜ å°„ç½‘ç»œï¼Œå°†æ³•å­¦ç¡•å£«è½¬åŒ–ä¸ºç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ˜ å°„ç½‘ç»œå°†æ–‡æœ¬çš„éšè—è¡¨ç¤ºè½¬æ¢ä¸ºè§†è§‰æ¨¡å‹çš„åµŒå…¥ç©ºé—´ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨ LLM çš„å¼ºå¤§æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œè§†è§‰è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨æ›´é•¿ã€æ›´å¤æ‚çš„è¯­è¨€çš„ä»»åŠ¡ä¸Šä¼˜äºåŸºçº¿ç”Ÿæˆæ¨¡å‹ã€‚é™¤äº†æ–°é¢–çš„å›¾åƒç”Ÿæˆä¹‹å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜èƒ½å¤Ÿä»é¢„å…ˆæŒ‡å®šçš„æ•°æ®é›†ä¸­è¿›è¡Œå›¾åƒæ£€ç´¢ï¼Œå¹¶åœ¨æ¨ç†æ—¶å†³å®šæ˜¯æ£€ç´¢è¿˜æ˜¯ç”Ÿæˆã€‚è¿™æ˜¯é€šè¿‡å­¦ä¹ å†³ç­–æ¨¡å—å®Œæˆçš„ï¼Œè¯¥æ¨¡å—ä»¥æ³•å­¦ç¡•å£«çš„éšè—è¡¨ç¤ºä¸ºæ¡ä»¶ã€‚ä¸ä¹‹å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†æ›´å¹¿æ³›çš„åŠŸèƒ½ã€‚å®ƒå¯ä»¥å¤„ç†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ£€ç´¢çš„å›¾åƒã€ç”Ÿæˆçš„å›¾åƒå’Œç”Ÿæˆçš„æ–‡æœ¬â€”â€”åœ¨å¤šä¸ªæµ‹é‡ä¸Šä¸‹æ–‡ä¾èµ–æ€§çš„æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸­ï¼Œå…¶æ€§èƒ½ä¼˜äºéåŸºäº LLM çš„ç”Ÿæˆæ¨¡å‹ã€‚

## Emergent and Predictable Memorization in Large Language Models<sup>poster<sup>

Authors: Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, Edward Raff

Link: [https://neurips.cc/virtual/2023/poster/72096](https://neurips.cc/virtual/2023/poster/72096)

Abstract:

 Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization in the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia.

æ‘˜è¦:

è®°å¿†ï¼Œæˆ–è€…è¯´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»è®­ç»ƒæ•°æ®ä¸­é€å­—è¾“å‡ºæ•´ä¸ªåºåˆ—çš„è¶‹åŠ¿ï¼Œæ˜¯éƒ¨ç½²è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°½é‡å‡å°‘æ¨¡å‹å¯¹æ•æ„Ÿæ•°æ®ç‚¹ï¼ˆä¾‹å¦‚åŒ…å«ä¸ªäººèº«ä»½ä¿¡æ¯ (PII) çš„æ•°æ®ç‚¹ï¼‰çš„è®°å¿†è‡³å…³é‡è¦ã€‚è¿™ç§ä¸è‰¯è®°å¿†çš„æ™®éå­˜åœ¨ä¼šç»™æ¨¡å‹è®­ç»ƒè€…å¸¦æ¥é—®é¢˜ï¼Œç”šè‡³å¯èƒ½éœ€è¦ä¸¢å¼ƒå…¶ä»–åŠŸèƒ½æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡æ¨æ–­è¾ƒä½è®¡ç®—é‡è¯•è¿è¡Œçš„è®°å¿†è¡Œä¸ºæ¥é¢„æµ‹åœ¨å¤§å‹æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæ—¶é—´ä¹‹å‰å°†è®°ä½å“ªäº›åºåˆ—ã€‚æˆ‘ä»¬åœ¨ Pythia æ¨¡å‹å¥—ä»¶ä¸­æµ‹é‡è®°å¿†åŠ›ï¼Œå¹¶ç»˜åˆ¶ç”¨äºé¢„æµ‹è®°å¿†åŠ›çš„ç¼©æ”¾å®šå¾‹ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæä¾›ç­‰è®¡ç®—å»ºè®®ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜æ­¤ç±»é¢„æµ‹çš„å¯é æ€§ï¼ˆå¬å›ç‡ï¼‰ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å…³äºæ¨¡å‹å’Œæ•°æ®çš„è®°å¿†åˆ†æ•°åˆ†å¸ƒçš„è¿›ä¸€æ­¥æ–°å‘ç°ã€‚æˆ‘ä»¬åœ¨ https://github.com/EleutherAI/pythia å‘å¸ƒäº†é‡ç°æœ¬æ–‡ç»“æœæ‰€éœ€çš„æ‰€æœ‰ä»£ç å’Œæ•°æ®ã€‚

## OpenAssistant Conversations - Democratizing Large Language Model Alignment<sup>oral<sup>

Authors: Andreas KÃ¶pf, Yannic Kilcher, Dimitri von RÃ¼tte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, RichÃ¡rd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, Alexander Mattick

Link: [https://neurips.cc/virtual/2023/poster/73573](https://neurips.cc/virtual/2023/poster/73573)

Abstract:

 Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT.Alignment techniques such as supervised fine-tuning (\textit{SFT}) and  reinforcement learning from human feedback (\textit{RLHF}) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains.However, state-of-the-art alignment techniques like \textit{RLHF} rely on high-quality human feedback data, which is expensive to create and often remains proprietary.In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees.The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models.We release our code\footnote{\git} and data\footnote{\data} under a fully permissive licence.

æ‘˜è¦:

äº‹å®è¯æ˜ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹ (LLM) ä¸äººç±»åå¥½ç»“åˆèµ·æ¥å¯ä»¥æå¤§åœ°æé«˜å¯ç”¨æ€§ï¼Œå¹¶æ¨åŠ¨äº†å¿«é€Ÿé‡‡ç”¨ï¼Œå¦‚ ChatGPT æ‰€è¯æ˜çš„é‚£æ ·ã€‚å¯¹é½æŠ€æœ¯ï¼Œä¾‹å¦‚ç›‘ç£å¾®è°ƒ (\textit{SFT}) å’Œæ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  ( \textit{RLHF}) å¤§å¤§å‡å°‘äº†æœ‰æ•ˆåˆ©ç”¨æ³•å­¦ç¡•å£«èƒ½åŠ›æ‰€éœ€çš„æŠ€èƒ½å’Œé¢†åŸŸçŸ¥è¯†ï¼Œæé«˜äº†å…¶åœ¨å„ä¸ªé¢†åŸŸçš„å¯è®¿é—®æ€§å’Œå®ç”¨æ€§ã€‚ç„¶è€Œï¼Œåƒ \textit{RLHF} è¿™æ ·æœ€å…ˆè¿›çš„å¯¹é½æŠ€æœ¯ä¾èµ–äºé«˜è´¨é‡çš„äººç±»åé¦ˆæ•°æ®ï¼Œè¿™äº›æ•°æ®çš„åˆ›å»ºæˆæœ¬å¾ˆé«˜ï¼Œè€Œä¸”é€šå¸¸ä»ç„¶æ˜¯ä¸“æœ‰çš„ã€‚ä¸ºäº†ä½¿å¤§è§„æ¨¡å¯¹é½çš„ç ”ç©¶æ°‘ä¸»åŒ–ï¼Œæˆ‘ä»¬å‘å¸ƒäº† OpenAssistant Conversationsï¼Œè¿™æ˜¯ä¸€ä¸ªç”±äººç±»ç”Ÿæˆã€äººç±»æ³¨é‡Šçš„åŠ©ç†å¼å¯¹è¯è¯­æ–™åº“ï¼ŒåŒ…å« 161,443 ä¸ªå¯¹è¯è¯­æ–™åº“35 ç§ä¸åŒè¯­è¨€çš„æ¶ˆæ¯ï¼Œå¸¦æœ‰ 461,292 ä¸ªè´¨é‡è¯„çº§æ³¨é‡Šï¼Œä»è€Œäº§ç”Ÿäº†è¶…è¿‡ 10,000 ä¸ªå®Œæ•´ä¸”å¸¦æ³¨é‡Šçš„å¯¹è¯æ ‘ã€‚è¯¥è¯­æ–™åº“æ˜¯å…¨çƒä¼—åŒ…å·¥ä½œçš„æˆæœï¼Œæ¶‰åŠè¶…è¿‡ 13,500 åå¿—æ„¿è€…ã€‚åœ¨ OpenAssistant Conversations ä¸Šè®­ç»ƒçš„æ¨¡å‹æ˜¾ç¤ºå‡ºæ ‡å‡†çš„æŒç»­æ”¹è¿›å¯¹å„è‡ªåŸºæœ¬æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨å®Œå…¨è®¸å¯çš„è®¸å¯ä¸‹å‘å¸ƒæˆ‘ä»¬çš„ä»£ç \è„šæ³¨{\git}å’Œæ•°æ®\è„šæ³¨{\æ•°æ®}ã€‚

## RRHF: Rank Responses to Align Language Models with Human Feedback<sup>poster<sup>

Authors: Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang

Link: [https://neurips.cc/virtual/2023/poster/72308](https://neurips.cc/virtual/2023/poster/72308)

Abstract:

 Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts.In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss.RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them.RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling.Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-$n$ learner.

æ‘˜è¦:

äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æœ‰åŠ©äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼Œä»è€Œæ˜¾ç€æé«˜äººç±»ä¸æ¨¡å‹ä¹‹é—´çš„äº¤äº’è´¨é‡ã€‚ InstructGPT é€šè¿‡å¤šä¸ªé˜¶æ®µå®ç° RLHFï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒ (SFT)ã€å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO)ã€‚ç„¶è€Œï¼ŒPPO å¯¹è¶…å‚æ•°å¾ˆæ•æ„Ÿï¼Œå¹¶ä¸”åœ¨å…¶æ ‡å‡†å®ç°ä¸­éœ€è¦å¤šä¸ªæ¨¡å‹ï¼Œè¿™ä½¿å¾—è®­ç»ƒå’Œæ‰©å±•åˆ°æ›´å¤§çš„å‚æ•°æ•°é‡å˜å¾—å›°éš¾ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸º RRHF çš„æ–°é¢–å­¦ä¹ èŒƒå¼ï¼Œå®ƒé€šè¿‡æ¡ä»¶æ¦‚ç‡çš„å¯¹æ•°ï¼Œå¹¶å­¦ä¹ é€šè¿‡æ’åæŸå¤±å°†è¿™äº›æ¦‚ç‡ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚RRHF å¯ä»¥åˆ©ç”¨å„ç§æ¥æºçš„é‡‡æ ·å“åº”ï¼ŒåŒ…æ‹¬è‡ªèº«çš„æ¨¡å‹å“åº”ã€å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹å“åº”å’Œäººç±»ä¸“å®¶å“åº”æ¥å­¦ä¹ å¯¹å®ƒä»¬è¿›è¡Œæ’åã€‚ä»…é™ RRHFåœ¨è°ƒä¼˜è¿‡ç¨‹ä¸­éœ€è¦ 1 åˆ° 2 ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°å°†è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ç¨³å¥åœ°ç»“åˆèµ·æ¥ï¼Œè€Œæ— éœ€å¤æ‚çš„è¶…å‚æ•°è°ƒä¼˜ã€‚æ­¤å¤–ï¼ŒRRHF å¯ä»¥è¢«è®¤ä¸ºæ˜¯ SFT å’Œå¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æ‰©å±•ï¼ŒåŒæ—¶åœ¨ç¼–ç ã€æ¨¡å‹è®¡æ•°å’Œè¶…å‚æ•°æ–¹é¢æ¯” PPO æ›´ç®€å•ã€‚æˆ‘ä»¬åœ¨ Helpful and Harmless æ•°æ®é›†ä¸Šè¯„ä¼° RRHFï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹è¯„åˆ†å’Œäººå·¥æ ‡è®°å±•ç¤ºäº†ä¸ PPO ç›¸å½“çš„å¯¹é½æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ RRHF çš„æ€§èƒ½ä¸é‡‡æ ·è´¨é‡é«˜åº¦ç›¸å…³ï¼Œè¿™è¡¨æ˜ RRHF æ˜¯ $n$ ä¸­æœ€å¥½çš„å­¦ä¹ è€…ã€‚

## Can Language Models Solve Graph Problems in Natural Language?<sup>poster<sup>

Authors: Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov

Link: [https://neurips.cc/virtual/2023/poster/71520](https://neurips.cc/virtual/2023/poster/71520)

Abstract:

 Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºå…·æœ‰éšå¼å›¾å½¢ç»“æ„çš„å„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººè§„åˆ’ã€å¤šè·³é—®ç­”æˆ–çŸ¥è¯†æ¢ç´¢ã€ç»“æ„åŒ–å¸¸è¯†æ¨ç†ç­‰ã€‚è™½ç„¶æ³•å­¦ç¡•å£«åœ¨è¿™äº›å…·æœ‰ç»“æ„å«ä¹‰çš„ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä½†æ³•å­¦ç¡•å£«æ˜¯å¦å¯ä»¥æ˜¾å¼å¤„ç†å›¾å½¢å’Œç»“æ„çš„æ–‡æœ¬æè¿°ã€å°†å®ƒä»¬æ˜ å°„åˆ°æ‰æ ¹çš„æ¦‚å¿µç©ºé—´å¹¶æ‰§è¡Œç»“æ„åŒ–æ“ä½œä»å¾…æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†NLGraphï¼ˆè‡ªç„¶è¯­è¨€å›¾ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨è‡ªç„¶è¯­è¨€è®¾è®¡çš„åŸºäºå›¾çš„é—®é¢˜è§£å†³çš„ç»¼åˆåŸºå‡†ã€‚ NLGraph åŒ…å« 29,370 ä¸ªé—®é¢˜ï¼Œæ¶µç›–å…«ç§å¤æ‚ç¨‹åº¦ä¸åŒçš„å›¾æ¨ç†ä»»åŠ¡ï¼Œä»è¿æ¥æ€§å’Œæœ€çŸ­è·¯å¾„ç­‰ç®€å•ä»»åŠ¡åˆ°æœ€å¤§æµå’Œæ¨¡æ‹Ÿå›¾ç¥ç»ç½‘ç»œç­‰å¤æ‚é—®é¢˜ã€‚æˆ‘ä»¬åœ¨ NLGraph åŸºå‡†ä¸Šä½¿ç”¨å„ç§æç¤ºæ–¹æ³•è¯„ä¼°æ³•å­¦ç¡•å£« (GPT-3/4)ï¼Œå‘ç° 1) è¯­è¨€æ¨¡å‹ç¡®å®å±•ç¤ºäº†åˆæ­¥çš„å›¾å½¢æ¨ç†èƒ½åŠ›ï¼Œ2) é«˜çº§æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„å¥½å¤„åœ¨æ›´å¤æ‚çš„å›¾å½¢é—®é¢˜ä¸Šä¼šå‡å¼±ï¼Œè€Œ3ï¼‰æ³•å­¦ç¡•å£«åœ¨é¢å¯¹å›¾å½¢å’Œé—®é¢˜è®¾ç½®ä¸­çš„è™šå‡ç›¸å…³æ€§æ—¶ä¹Ÿï¼ˆå¹¶ä¸ï¼‰ä»¤äººæƒŠè®¶åœ°è„†å¼±ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºæ„å»ºå›¾æç¤ºå’Œç®—æ³•æç¤ºï¼Œè¿™ä¸¤ç§åŸºäºæŒ‡ä»¤çš„æ–¹æ³•å¯ä»¥å¢å¼ºæ³•å­¦ç¡•å£«è§£å†³è‡ªç„¶è¯­è¨€å›¾é—®é¢˜çš„èƒ½åŠ›ã€‚æ„å»ºå›¾å’Œç®—æ³•æç¤ºå°† NLGraph ä¸Šçš„æ³•å­¦ç¡•å£«åœ¨å¤šä¸ªä»»åŠ¡å’Œè®¾ç½®ä¸­çš„æ€§èƒ½æé«˜äº† 3.07% è‡³ 16.85%ï¼Œè€Œå¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹è§£å†³æˆ‘ä»¬è®¾ç½®ä¸­æœ€å¤æ‚çš„å›¾æ¨ç†ä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜ã€‚

## Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning<sup>poster<sup>

Authors: Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Zhang, Jun Zhou, Chenhao Tan, Hongyuan Mei

Link: [https://neurips.cc/virtual/2023/poster/71194](https://neurips.cc/virtual/2023/poster/71194)

Abstract:

 Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction performance of event sequence models. We design LAMP, a framework that integrates a large language model in event prediction. Particularly, the language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on several challenging real-world datasets, we demonstrate that our framework---thanks to the reasoning capabilities of large language models---could significantly outperform the state-of-the-art event sequence models.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¹¿æ³›çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å®ƒä»¬æ˜¯å¦å¯ä»¥æ¨ç†ç°å®ä¸–ç•Œçš„äº‹ä»¶å¹¶å¸®åŠ©æé«˜äº‹ä»¶åºåˆ—æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬è®¾è®¡äº†LAMPï¼Œä¸€ä¸ªåœ¨äº‹ä»¶é¢„æµ‹ä¸­é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ã€‚ç‰¹åˆ«åœ°ï¼Œè¯­è¨€æ¨¡å‹æ‰§è¡Œæº¯å› æ¨ç†æ¥è¾…åŠ©äº‹ä»¶åºåˆ—æ¨¡å‹ï¼šäº‹ä»¶æ¨¡å‹æå‡ºå¯¹ç»™å®šè¿‡å»çš„æœªæ¥äº‹ä»¶çš„é¢„æµ‹ï¼›åœ¨ä¸€äº›ä¸“å®¶æ³¨é‡Šçš„æ¼”ç¤ºçš„æŒ‡å¯¼ä¸‹ï¼Œè¯­è¨€æ¨¡å‹å­¦ä¼šä¸ºæ¯ä¸ªææ¡ˆæå‡ºå¯èƒ½çš„åŸå› ï¼›æœç´¢æ¨¡å—æ‰¾å‡ºä¸åŸå› åŒ¹é…çš„å…ˆå‰äº‹ä»¶ï¼›è¯„åˆ†å‡½æ•°å­¦ä¹ æ£€æŸ¥æ£€ç´¢åˆ°çš„äº‹ä»¶æ˜¯å¦ç¡®å®ä¼šå¯¼è‡´ææ¡ˆã€‚é€šè¿‡å¯¹å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•Œæ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶â€”â€”å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›â€”â€”å¯ä»¥æ˜¾ç€ä¼˜äºæœ€å…ˆè¿›çš„äº‹ä»¶åºåˆ—æ¨¡å‹ã€‚

## Skill-it! A data-driven skills framework for understanding and training language models<sup>poster<sup>

Authors: Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue WANG, Ce Zhang, Frederic Sala, Christopher RÃ©

Link: [https://neurips.cc/virtual/2023/poster/72098](https://neurips.cc/virtual/2023/poster/72098)

Abstract:

 The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 37.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.

æ‘˜è¦:

è®­ç»ƒæ•°æ®çš„è´¨é‡å½±å“é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„æ€§èƒ½ã€‚ç»™å®šå›ºå®šçš„ä»£å¸é¢„ç®—ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•æœ€å¥½åœ°é€‰æ‹©æ•°æ®ï¼Œä»è€Œåœ¨ä»»åŠ¡ä¸­å®ç°è‰¯å¥½çš„ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åŸºäºä¸€ä¸ªç®€å•çš„å‡è®¾å¼€å‘äº†ä¸€ä¸ªæ–°æ¡†æ¶ï¼šæ­£å¦‚äººç±»ä»¥æœ‰æ„çš„é¡ºåºè·å¾—ç›¸äº’ä¾èµ–çš„æŠ€èƒ½ä¸€æ ·ï¼Œè¯­è¨€æ¨¡å‹åœ¨ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ä¸€ç»„æŠ€èƒ½æ—¶ä¹Ÿéµå¾ªè‡ªç„¶é¡ºåºã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„é¡ºåºï¼Œå®ƒå¯ä»¥ç”¨äºå¢è¿›å¯¹ LM çš„ç†è§£å’Œæ•°æ®é«˜æ•ˆçš„è®­ç»ƒã€‚åˆ©ç”¨è¿™ç§ç›´è§‰ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ ¹æ®ç›¸å…³æ•°æ®å½¢å¼åŒ–äº†æŠ€èƒ½å’Œæœ‰åºæŠ€èƒ½é›†çš„æ¦‚å¿µã€‚é¦–å…ˆï¼Œä½¿ç”¨åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™äº›æœ‰åºæŠ€èƒ½é›†çš„å­˜åœ¨ï¼Œå¹¶ä¸”å½“æˆ‘ä»¬è®­ç»ƒå…¶å¿…å¤‡æŠ€èƒ½æ—¶ï¼Œå®ƒä»¬çš„å­˜åœ¨ä½¿å¾—èƒ½å¤Ÿç”¨æ›´å°‘çš„æ•°æ®å­¦ä¹ æ›´é«˜çº§çš„æŠ€èƒ½ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„æ¡†æ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åœ¨çº¿æ•°æ®é‡‡æ ·ç®—æ³• Skill-Itï¼Œè¯¥ç®—æ³•æ··åˆäº†æŒç»­é¢„è®­ç»ƒå’Œå¾®è°ƒåˆ¶åº¦çš„æŠ€èƒ½ï¼Œå…¶ç›®æ ‡æ˜¯æœ‰æ•ˆåœ°å­¦ä¹ å‰è€…å’Œä¸ªäººçš„å¤šç§æŠ€èƒ½åè€…çš„æŠ€èƒ½ã€‚åœ¨è¿ç»­é¢„è®­ç»ƒè®¾ç½®ä¸­çš„ä¹é«˜åˆæˆä¸Šï¼ŒSkill-It çš„å‡†ç¡®åº¦æ¯”éšæœºé‡‡æ ·é«˜ 37.5 ä¸ªç™¾åˆ†ç‚¹ã€‚åœ¨å¾®è°ƒè®¾ç½®çš„è‡ªç„¶æŒ‡ä»¤æ•°æ®é›†ä¸Šï¼Œä¸ç›®æ ‡æŠ€èƒ½æœ¬èº«ç›¸å…³æ•°æ®çš„è®­ç»ƒç›¸æ¯”ï¼ŒSkill-It å°†ç›®æ ‡æŠ€èƒ½çš„éªŒè¯æŸå¤±å‡å°‘äº† 13.6%ã€‚æˆ‘ä»¬åœ¨ RedPajama æ•°æ®é›†ä¸Šåº”ç”¨æˆ‘ä»¬çš„æŠ€èƒ½æ¡†æ¶æ¥æŒç»­é¢„è®­ç»ƒ 3B å‚æ•° LMï¼Œä½¿ç”¨ 1B ä»¤ç‰Œåœ¨ LM è¯„ä¼°å·¥å…·ä¸Šå®ç°æ¯”ä½¿ç”¨ 3B ä»¤ç‰Œåœ¨æ•°æ®æºä¸Šå‡åŒ€é‡‡æ ·çš„åŸºçº¿æ–¹æ³•æ›´é«˜çš„å‡†ç¡®åº¦ã€‚

## Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias<sup>poster<sup>

Authors: Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, Chao Zhang

Link: [https://neurips.cc/virtual/2023/poster/73693](https://neurips.cc/virtual/2023/poster/73693)

Abstract:

 Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\% of the querying cost of ChatGPT associated with the latter. The data and code are available on {\url{https://github.com/yueyu1030/AttrPrompt}}.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘è¢«ç”¨ä½œå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶æ¢ç´¢äº†ä½¿ç”¨ç”Ÿæˆæ•°æ®è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ–¹æ³•ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç®€å•çš„ç±»æ¡ä»¶æç¤ºï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å¹¶ç»§æ‰¿æ³•å­¦ç¡•å£«çš„ç³»ç»Ÿåå·®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶å…·æœ‰ä¸åŒå±æ€§æç¤ºçš„è®­ç»ƒæ•°æ®ç”Ÿæˆï¼ˆä¾‹å¦‚ï¼ŒæŒ‡å®šé•¿åº¦å’Œæ ·å¼ç­‰å±æ€§ï¼‰ï¼Œè¿™æœ‰å¯èƒ½äº§ç”Ÿä¸åŒçš„å±æ€§ç”Ÿæˆæ•°æ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶é‡ç‚¹æ˜¯å…·æœ‰é«˜åŸºæ•°å’Œä¸åŒé¢†åŸŸçš„æ•°æ®é›†ï¼Œå…¶ä¸­æˆ‘ä»¬è¯æ˜ï¼Œå°±ç”Ÿæˆçš„æ¨¡å‹æ€§èƒ½è€Œè¨€ï¼Œå½’å› æç¤ºä¼˜äºç®€å•çš„ç±»æ¡ä»¶æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ•°æ®ç”Ÿæˆè¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæ¶µç›–äº†åå·®ã€å¤šæ ·æ€§å’Œæ•ˆç‡ç­‰é‡è¦æ–¹é¢ï¼Œå¹¶å¼ºè°ƒäº†ä¸‰ä¸ªå…³é”®è§‚å¯Ÿç»“æœï¼šé¦–å…ˆï¼Œé€šè¿‡ç®€å•æç¤ºç”Ÿæˆçš„åˆæˆæ•°æ®é›†è¡¨ç°å‡ºæ˜¾ç€çš„åå·®ï¼Œä¾‹å¦‚åŒºåŸŸåå·®ï¼›å…¶æ¬¡ï¼Œå±æ€§å¤šæ ·æ€§å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½èµ·ç€å…³é”®ä½œç”¨ï¼›æœ€åï¼Œå±æ€§æç¤ºå®ç°äº†ç®€å•ç±»æ¡ä»¶æç¤ºçš„æ€§èƒ½ï¼Œè€Œä»…åˆ©ç”¨ä¸åè€…ç›¸å…³çš„ ChatGPT çš„ 5\% æŸ¥è¯¢æˆæœ¬ã€‚æ•°æ®å’Œä»£ç å¯ä»¥åœ¨ {\url{https://github.com/yueyu1030/AttrPrompt}} ä¸Šæ‰¾åˆ°ã€‚

## Distributed Inference and Fine-tuning of Large Language Models Over The Internet<sup>poster<sup>

Authors: Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel

Link: [https://neurips.cc/virtual/2023/poster/71336](https://neurips.cc/virtual/2023/poster/71336)

Abstract:

 Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals â€” a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to $10\times$ faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨è®¸å¤š NLP ä»»åŠ¡ä¸­éå¸¸æœ‰ç”¨ï¼Œå¹¶ä¸”éšç€è§„æ¨¡çš„å¢å¤§è€Œå˜å¾—æ›´åŠ å¼ºå¤§ï¼Œæœ€å¥½çš„å¼€æºæ¨¡å‹æ‹¥æœ‰è¶…è¿‡ 500 äº¿ä¸ªå‚æ•°ã€‚ç„¶è€Œï¼Œä½¿ç”¨è¿™äº› 50B+ æ¨¡å‹éœ€è¦é«˜ç«¯ç¡¬ä»¶ï¼Œè¿™ä½¿å¾—å¤§å¤šæ•°ç ”ç©¶äººå‘˜æ— æ³•ä½¿ç”¨å®ƒä»¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº† LLM çš„ç»æµæœ‰æ•ˆçš„æ¨ç†å’Œå¾®è°ƒæ–¹æ³•ï¼Œæ¯”è¾ƒäº†æœ¬åœ°å’Œåˆ†å¸ƒå¼ç­–ç•¥ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå³ä½¿åœ¨æ¶ˆè´¹çº§ç½‘ç»œä¸­çš„åœ°ç†åˆ†å¸ƒå¼è®¾å¤‡ä¸Šï¼Œè¶³å¤Ÿå¤§çš„æ¨¡å‹ï¼ˆ50B+ï¼‰ä¹Ÿå¯ä»¥é«˜æ•ˆè¿è¡Œã€‚è¿™å¯ä»¥é€šè¿‡å°†å¤šä¸ªç ”ç©¶å°ç»„å’Œå¿—æ„¿è€…çš„é—²ç½®è®¡ç®—èµ„æºé›†ä¸­åœ¨ä¸€èµ·æ¥æœ‰æ•ˆåœ°è¿è¡Œæ³•å­¦ç¡•å£«ã€‚æˆ‘ä»¬è§£å†³äº†ä¸¤ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼šï¼ˆ1ï¼‰å¦‚æœä»»ä½•è®¾å¤‡çªç„¶æ–­å¼€è¿æ¥ï¼Œå¦‚ä½•å¯é åœ°æ‰§è¡Œæ¨ç†å’Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰å¦‚ä½•åœ¨ç¡¬ä»¶ä¸å‡åŒ€çš„è®¾å¤‡ä¹‹é—´åˆ’åˆ†LLMï¼Œéšæ„åŠ å…¥å’Œç¦»å¼€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ç‰¹æ®Šçš„å®¹é”™æ¨ç†ç®—æ³•å’Œè´Ÿè½½å¹³è¡¡åè®®ï¼Œå¯ä»¥è‡ªåŠ¨åˆ†é…è®¾å¤‡ä»¥æœ€å¤§åŒ–ç³»ç»Ÿæ€»ååé‡ã€‚æˆ‘ä»¬åœ¨ Petals ä¸­å±•ç¤ºäº†è¿™äº›ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªå»ä¸­å¿ƒåŒ–ç³»ç»Ÿï¼Œé€šè¿‡äº’è”ç½‘è¿è¡Œ Llama 2 (70B) å’Œ BLOOM (176B)ï¼Œæ¯”äº¤äº’å¼ç”Ÿæˆå¸è½½é€Ÿåº¦å¿« 10 å€ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿæ¡ä»¶å’Œè·¨è¶Šä¸¤å¤§æ´²çš„ç°å®ä¸–ç•Œè®¾ç½®ä¸­è¯„ä¼°ç³»ç»Ÿçš„æ€§èƒ½ã€‚

## TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models<sup>poster<sup>

Authors: Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau BÃ¶lÃ¶ni, Qian Lou

Link: [https://neurips.cc/virtual/2023/poster/71224](https://neurips.cc/virtual/2023/poster/71224)

Abstract:

 Large Language Models (LLMs) are progressively being utilized as machine learning services and interface tools for various applications. However, the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined. In this paper, we propose TrojLLM, an automatic and black-box framework to effectively generate universal and stealthy triggers. When these triggers are incorporated into the input data, the LLMs' outputs can be maliciously manipulated.   Moreover, the framework also supports embedding Trojans within discrete prompts, enhancing the overall effectiveness and precision of the triggers' attacks.  Specifically, we propose a trigger discovery algorithm for generating universal triggers for various inputs by querying victim LLM-based APIs using few-shot data samples. Furthermore, we introduce a novel progressive Trojan poisoning algorithm designed to generate poisoned prompts that retain efficacy and transferability across a diverse range of models. Our experiments and results demonstrate TrojLLM's capacity to effectively insert Trojans into text prompts in real-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining exceptional performance on clean test sets. Our work sheds light on the potential security risks in current models and offers a potential defensive approach. The source code of TrojLLM is available at https://github.com/UCF-ML-Research/TrojLLM.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£é€æ¸è¢«ç”¨ä½œå„ç§åº”ç”¨ç¨‹åºçš„æœºå™¨å­¦ä¹ æœåŠ¡å’Œæ¥å£å·¥å…·ã€‚ç„¶è€Œï¼Œæ³•å­¦ç¡•å£«çš„å®‰å…¨å½±å“ï¼Œç‰¹åˆ«æ˜¯ä¸å¯¹æŠ—æ€§å’Œç‰¹æ´›ä¼Šæœ¨é©¬æ”»å‡»æœ‰å…³çš„å½±å“ï¼Œä»æœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† TrojLLMï¼Œä¸€ç§è‡ªåŠ¨é»‘ç›’æ¡†æ¶ï¼Œå¯æœ‰æ•ˆç”Ÿæˆé€šç”¨ä¸”éšç§˜çš„è§¦å‘å™¨ã€‚å½“è¿™äº›è§¦å‘å™¨è¢«åˆå¹¶åˆ°è¾“å…¥æ•°æ®ä¸­æ—¶ï¼Œæ³•å­¦ç¡•å£«çš„è¾“å‡ºå¯èƒ½ä¼šè¢«æ¶æ„æ“çºµã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æ”¯æŒåœ¨ç¦»æ•£æç¤ºä¸­åµŒå…¥æœ¨é©¬ï¼Œä»è€Œæé«˜è§¦å‘å™¨æ”»å‡»çš„æ•´ä½“æœ‰æ•ˆæ€§å’Œç²¾ç¡®åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§¦å‘å™¨å‘ç°ç®—æ³•ï¼Œé€šè¿‡ä½¿ç”¨å°‘é‡æ•°æ®æ ·æœ¬æŸ¥è¯¢å—å®³äººåŸºäº LLM çš„ APIï¼Œä¸ºå„ç§è¾“å…¥ç”Ÿæˆé€šç”¨è§¦å‘å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ¸è¿›å¼æœ¨é©¬ä¸­æ¯’ç®—æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆä¸­æ¯’æç¤ºï¼Œè¯¥æç¤ºåœ¨å„ç§æ¨¡å‹ä¸­ä¿æŒæœ‰æ•ˆæ€§å’Œå¯è½¬ç§»æ€§ã€‚æˆ‘ä»¬çš„å®éªŒå’Œç»“æœè¯æ˜äº† TrojLLM èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æœ¨é©¬æ’å…¥ç°å®ä¸–ç•Œé»‘ç›’ LLM APIï¼ˆåŒ…æ‹¬ GPT-3.5 å’Œ GPT-4ï¼‰çš„æ–‡æœ¬æç¤ºä¸­ï¼ŒåŒæ—¶åœ¨å¹²å‡€çš„æµ‹è¯•é›†ä¸Šä¿æŒå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†å½“å‰æ¨¡å‹ä¸­çš„æ½œåœ¨å®‰å…¨é£é™©ï¼Œå¹¶æä¾›äº†ä¸€ç§æ½œåœ¨çš„é˜²å¾¡æ–¹æ³•ã€‚ TrojLLM çš„æºä»£ç å¯åœ¨ https://github.com/UCF-ML-Research/TrojLLM è·å–ã€‚

## Likelihood-Based Diffusion Language Models<sup>poster<sup>

Authors: Ishaan Gulrajani, Tatsunori Hashimoto

Link: [https://neurips.cc/virtual/2023/poster/70985](https://neurips.cc/virtual/2023/poster/70985)

Abstract:

 Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.

æ‘˜è¦:

å°½ç®¡äººä»¬å¯¹åŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œä½†ç°æœ‰çš„å·¥ä½œå°šæœªè¡¨æ˜è¿™äº›æ¨¡å‹å¯ä»¥åœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†ä¸Šè·å¾—éå¹³å‡¡çš„å¯èƒ½æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿ˆå‡ºäº†ç¼©å°è‡ªå›å½’å’ŒåŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å¯èƒ½æ€§å·®è·çš„ç¬¬ä¸€æ­¥ï¼Œç›®æ ‡æ˜¯æ„å»ºå’Œå‘å¸ƒä¸€ä¸ªä¼˜äºå°å‹ä½†å¹¿ä¸ºäººçŸ¥çš„è‡ªå›å½’æ¨¡å‹çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡ç®—æ³•æ”¹è¿›ã€ç¼©æ”¾æ³•åˆ™å’Œå¢åŠ è®¡ç®—æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚åœ¨ç®—æ³•æ–¹é¢ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ‰©æ•£è¯­è¨€æ¨¡å‹æœ€å¤§ä¼¼ç„¶è®­ç»ƒçš„å‡ ç§æ–¹æ³•æ”¹è¿›ã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶æ‰©æ•£æ¨¡å‹çš„ç¼©æ”¾å®šå¾‹ï¼Œå¹¶æ‰¾åˆ°ä¸è‡ªå›å½’æ¨¡å‹æœ‰å¾ˆå¤§ä¸åŒçš„è®¡ç®—æœ€ä½³è®­ç»ƒæ–¹æ¡ˆã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å’Œå°ºåº¦åˆ†æï¼Œæˆ‘ä»¬è®­ç»ƒå¹¶å‘å¸ƒäº† Plaid 1Bï¼Œè¿™æ˜¯ä¸€ç§å¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ƒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¯èƒ½æ€§ä¼˜äº GPT-2 124Mï¼Œå¹¶åœ¨æ— æ¡ä»¶å’Œé›¶æ ·æœ¬æ§åˆ¶è®¾ç½®ä¸­ç”Ÿæˆæµç•…çš„æ ·æœ¬ã€‚

## Scaling Data-Constrained Language Models<sup>oral<sup>

Authors: Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, Colin Raffel

Link: [https://neurips.cc/virtual/2023/poster/70706](https://neurips.cc/virtual/2023/poster/70706)

Abstract:

 The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.

æ‘˜è¦:

å½“å‰ç¼©æ”¾è¯­è¨€æ¨¡å‹çš„è¶‹åŠ¿æ¶‰åŠå¢åŠ å‚æ•°æ•°é‡å’Œè®­ç»ƒæ•°æ®é›†å¤§å°ã€‚æ¨æ–­è¿™ä¸€è¶‹åŠ¿è¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®é›†çš„å¤§å°å¯èƒ½å¾ˆå¿«å°±ä¼šå—åˆ°äº’è”ç½‘ä¸Šå¯ç”¨æ–‡æœ¬æ•°æ®é‡çš„é™åˆ¶ã€‚å—æ­¤é™åˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ•°æ®å—é™æƒ…å†µä¸‹çš„æ‰©å±•è¯­è¨€æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¿è¡Œäº†å¤§é‡ä¸åŒæ•°æ®é‡å¤ç¨‹åº¦å’Œè®¡ç®—é¢„ç®—çš„å®éªŒï¼ŒèŒƒå›´é«˜è¾¾ 9000 äº¿ä¸ªè®­ç»ƒä»¤ç‰Œå’Œ 90 äº¿ä¸ªå‚æ•°æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å›ºå®šè®¡ç®—é¢„ç®—çš„çº¦æŸæ•°æ®ä¸‹ï¼Œä¸æ‹¥æœ‰å”¯ä¸€æ•°æ®ç›¸æ¯”ï¼Œä½¿ç”¨æœ€å¤š 4 ä¸ªå‘¨æœŸçš„é‡å¤æ•°æ®è¿›è¡Œè®­ç»ƒæ‰€äº§ç”Ÿçš„æŸå¤±å˜åŒ–å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚ç„¶è€Œï¼Œéšç€é‡å¤æ¬¡æ•°çš„å¢åŠ ï¼Œå¢åŠ è®¡ç®—çš„ä»·å€¼æœ€ç»ˆä¼šè¡°å‡åˆ°é›¶ã€‚æˆ‘ä»¬æå‡ºå¹¶å‡­ç»éªŒéªŒè¯äº†è®¡ç®—æœ€ä¼˜æ€§çš„ç¼©æ”¾æ³•åˆ™ï¼Œè¯¥æ³•åˆ™è§£é‡Šäº†é‡å¤æ ‡è®°å’Œå¤šä½™å‚æ•°çš„å€¼é€’å‡çš„æƒ…å†µã€‚æœ€åï¼Œæˆ‘ä»¬å°è¯•äº†ç¼“è§£æ•°æ®ç¨€ç¼ºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä»£ç æ•°æ®å¢å¼ºè®­ç»ƒæ•°æ®é›†æˆ–åˆ é™¤å¸¸ç”¨çš„è¿‡æ»¤å™¨ã€‚æˆ‘ä»¬ 400 æ¬¡è®­ç»ƒè¿è¡Œçš„æ¨¡å‹å’Œæ•°æ®é›†å¯åœ¨ https://github.com/huggingface/datablations ä¸Šå…è´¹è·å–ã€‚

## Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models<sup>poster<sup>

Authors: Lin Li, Jun Xiao, Guikun Chen, Jian Shao, Yueting Zhuang, Long Chen

Link: [https://neurips.cc/virtual/2023/poster/70046](https://neurips.cc/virtual/2023/poster/70046)

Abstract:

 Pretrained vision-language models, such as CLIP, have demonstrated strong generalization capabilities, making them promising tools in the realm of zero-shot visual recognition. Visual relation detection (VRD) is a typical task that identifies relationship (or interaction) types between object pairs within an image. However, naively utilizing CLIP with prevalent class-based prompts for zero-shot VRD has several weaknesses, e.g., it struggles to distinguish between different fine-grained relation types and it neglects essential spatial information of two objects. To this end, we propose a novel method for zero-shot VRD: RECODE, which solves RElation detection via COmposite DEscription prompts. Specifically, RECODE first decomposes each predicate category into subject, object, and spatial components. Then, it leverages large language models (LLMs) to generate description-based prompts (or visual cues) for each component. Different visual cues enhance the discriminability of similar relation categories from different perspectives, which significantly boosts performance in VRD. To dynamically fuse different cues, we further introduce a chain-of-thought method that prompts LLMs to generate reasonable weights for different visual cues. Extensive experiments on four VRD benchmarks have demonstrated the effectiveness and interpretability of RECODE.

æ‘˜è¦:

é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ CLIPï¼Œå·²ç»è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºé›¶æ ·æœ¬è§†è§‰è¯†åˆ«é¢†åŸŸçš„æœ‰å‰é€”çš„å·¥å…·ã€‚è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰æ˜¯è¯†åˆ«å›¾åƒä¸­å¯¹è±¡å¯¹ä¹‹é—´çš„å…³ç³»ï¼ˆæˆ–äº¤äº’ï¼‰ç±»å‹çš„å…¸å‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤©çœŸåœ°å°† CLIP ä¸æµè¡Œçš„åŸºäºç±»çš„æç¤ºä¸€èµ·ç”¨äºé›¶æ ·æœ¬ VRD æœ‰å‡ ä¸ªå¼±ç‚¹ï¼Œä¾‹å¦‚ï¼Œå®ƒå¾ˆéš¾åŒºåˆ†ä¸åŒçš„ç»†ç²’åº¦å…³ç³»ç±»å‹ï¼Œå¹¶ä¸”å¿½ç•¥äº†ä¸¤ä¸ªå¯¹è±¡çš„åŸºæœ¬ç©ºé—´ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬ VRD çš„æ–°æ–¹æ³•ï¼šRECODEï¼Œå®ƒé€šè¿‡ COmposite DEscription æç¤ºè§£å†³ RElation æ£€æµ‹ã€‚å…·ä½“æ¥è¯´ï¼ŒRECODE é¦–å…ˆå°†æ¯ä¸ªè°“è¯ç±»åˆ«åˆ†è§£ä¸ºä¸»è¯­ã€å®¾è¯­å’Œç©ºé—´åˆ†é‡ã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸ºæ¯ä¸ªç»„ä»¶ç”ŸæˆåŸºäºæè¿°çš„æç¤ºï¼ˆæˆ–è§†è§‰æç¤ºï¼‰ã€‚ä¸åŒçš„è§†è§‰çº¿ç´¢å¢å¼ºäº†ä»ä¸åŒè§’åº¦å¯¹ç›¸ä¼¼å…³ç³»ç±»åˆ«çš„è¾¨åˆ«èƒ½åŠ›ï¼Œè¿™æ˜¾ç€æé«˜äº† VRD çš„æ€§èƒ½ã€‚ä¸ºäº†åŠ¨æ€èåˆä¸åŒçš„çº¿ç´¢ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æ€æƒ³é“¾æ–¹æ³•ï¼Œæç¤ºæ³•å­¦ç¡•å£«ä¸ºä¸åŒçš„è§†è§‰çº¿ç´¢ç”Ÿæˆåˆç†çš„æƒé‡ã€‚å¯¹å››ä¸ª VRD åŸºå‡†çš„å¤§é‡å®éªŒè¯æ˜äº† RECODE çš„æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚

## In-Context Impersonation Reveals Large Language Models' Strengths and Biases<sup>poster<sup>

Authors: Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata

Link: [https://neurips.cc/virtual/2023/poster/72422](https://neurips.cc/virtual/2023/poster/72422)

Abstract:

 In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their strengths and hidden biases. Our code is available at https://github.com/ExplainableML/in-context-impersonation.

æ‘˜è¦:

åœ¨æ—¥å¸¸å¯¹è¯ä¸­ï¼Œäººç±»å¯ä»¥æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¹¶æ ¹æ®è‡ªå·±é€‰æ‹©çš„è§’è‰²è°ƒæ•´è¯æ±‡ã€‚æˆ‘ä»¬æ¢è®¨æ³•å­¦ç¡•å£«åœ¨ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæ–‡æœ¬æ—¶æ˜¯å¦å¯ä»¥æ‰®æ¼”ï¼ˆå³æ¨¡æ‹Ÿï¼‰ä¸åŒçš„è§’è‰²ã€‚æˆ‘ä»¬è¦æ±‚æ³•å­¦ç¡•å£«åœ¨è§£å†³è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¹‹å‰å‡è®¾ä¸åŒçš„è§’è‰²ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æç¤ºå‰æ·»åŠ ä¸ç¤¾äº¤èº«ä»½æˆ–é¢†åŸŸä¸“ä¸šçŸ¥è¯†ç›¸å…³çš„è§’è‰²æ¥å®ç°æ­¤ç›®çš„ã€‚åœ¨å¤šè‡‚è€è™æœºä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‘ç°å‡è£…æˆä¸åŒå¹´é¾„å„¿ç«¥çš„æ³•å­¦ç¡•å£«æ¢å¤äº†ç±»ä¼¼äººç±»çš„æ¢ç´¢å‘å±•é˜¶æ®µã€‚åœ¨åŸºäºè¯­è¨€çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‘ç°å†’å……é¢†åŸŸä¸“å®¶çš„æ³•å­¦ç¡•å£«æ¯”å†’å……éé¢†åŸŸä¸“å®¶çš„æ³•å­¦ç¡•å£«è¡¨ç°æ›´å¥½ã€‚æœ€åï¼Œæˆ‘ä»¬æµ‹è¯•æ³•å­¦ç¡•å£«åœ¨æè¿°ä¸åŒç±»åˆ«æ—¶çš„æ¨¡ä»¿æ˜¯å¦ä¸è§†è§‰ä¿¡æ¯äº’è¡¥ã€‚æˆ‘ä»¬å‘ç°æ¨¡ä»¿å¯ä»¥æé«˜è¡¨ç°ï¼šè¢«æç¤ºæˆä¸ºé¸Ÿç±»ä¸“å®¶çš„æ³•å­¦ç¡•å£«æ¯”è¢«æç¤ºæˆä¸ºæ±½è½¦ä¸“å®¶çš„æ³•å­¦ç¡•å£«æ›´èƒ½æè¿°é¸Ÿç±»ã€‚ç„¶è€Œï¼Œå†’å……ä¹Ÿå¯ä»¥æ­éœ²æ³•å­¦ç¡•å£«çš„åè§ï¼šè¢«æç¤ºä¸ºç”·æ€§çš„æ³•å­¦ç¡•å£«æ¯”è¢«æç¤ºä¸ºå¥³æ€§çš„æ³•å­¦ç¡•å£«æ›´èƒ½æè¿°æ±½è½¦ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ³•å­¦ç¡•å£«æœ‰èƒ½åŠ›æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¹¶ä¸”è¿™ç§åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ¨¡ä»¿å¯ä»¥ç”¨æ¥æ­ç¤ºä»–ä»¬çš„ä¼˜åŠ¿å’Œéšè—çš„åè§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ https://github.com/ExplainableML/in-context-impersonation è·å–ã€‚

## Towards Efficient Pre-Trained Language Model via Feature Correlation Distillation<sup>poster<sup>

Authors: Kun Huang, Xin Guo, Meng Wang

Link: [https://neurips.cc/virtual/2023/poster/70640](https://neurips.cc/virtual/2023/poster/70640)

Abstract:

 Knowledge Distillation (KD) has emerged as a promising approach for compressing large Pre-trained Language Models (PLMs). The performance of KD relies on how to effectively formulate and transfer the knowledge from the teacher model to the student model. Prior arts mainly focus on directly aligning output features from the transformer block, which may impose overly strict constraints on the student model's learning process and complicate the training process by introducing extra parameters and computational cost. Moreover, our analysis indicates that the different relations within self-attention, as adopted in other works, involves more computation complexities and can easily be constrained by the number of heads, potentially leading to suboptimal solutions.  To address these issues, we propose a novel approach that builds relationships directly from output features. Specifically, we introduce token-level and sequence-level relations concurrently  to fully exploit the knowledge from the teacher model. Furthermore, we propose a correlation-based distillation loss to alleviate the exact match properties inherent in traditional KL divergence or MSE loss functions. Our method, dubbed FCD, presents a simple yet effective method to compress various architectures (BERT, RoBERTa, and GPT) and model sizes (base-size and large-size).  Extensive experimental results demonstrate that our distilled, smaller language models significantly surpass existing KD methods across various NLP tasks.

æ‘˜è¦:

çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å·²æˆä¸ºå‹ç¼©å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ KDçš„æ€§èƒ½å–å†³äºå¦‚ä½•æœ‰æ•ˆåœ°åˆ¶å®šçŸ¥è¯†å¹¶å°†å…¶ä»æ•™å¸ˆæ¨¡å‹è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ã€‚ç°æœ‰æŠ€æœ¯ä¸»è¦é›†ä¸­äºç›´æ¥å¯¹é½æ¥è‡ªå˜å‹å™¨å—çš„è¾“å‡ºç‰¹å¾ï¼Œè¿™å¯èƒ½å¯¹å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹æ–½åŠ è¿‡äºä¸¥æ ¼çš„çº¦æŸï¼Œå¹¶ä¸”é€šè¿‡å¼•å…¥é¢å¤–çš„å‚æ•°å’Œè®¡ç®—æˆæœ¬è€Œä½¿è®­ç»ƒè¿‡ç¨‹å¤æ‚åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè‡ªæ³¨æ„åŠ›ä¸­çš„ä¸åŒå…³ç³»ï¼ˆå¦‚å…¶ä»–ä½œå“ä¸­æ‰€é‡‡ç”¨çš„é‚£æ ·ï¼‰æ¶‰åŠæ›´å¤šçš„è®¡ç®—å¤æ‚æ€§ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“å—åˆ°å¤´æ•°é‡çš„é™åˆ¶ï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´æ¥ä»è¾“å‡ºç‰¹å¾å»ºç«‹å…³ç³»çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åŒæ—¶å¼•å…¥ä»¤ç‰Œçº§å’Œåºåˆ—çº§å…³ç³»ï¼Œä»¥å……åˆ†åˆ©ç”¨æ•™å¸ˆæ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç›¸å…³æ€§çš„è’¸é¦æŸå¤±ï¼Œä»¥å‡è½»ä¼ ç»Ÿ KL æ•£åº¦æˆ– MSE æŸå¤±å‡½æ•°å›ºæœ‰çš„ç²¾ç¡®åŒ¹é…ç‰¹æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸º FCDï¼Œæä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥å‹ç¼©å„ç§æ¶æ„ï¼ˆBERTã€RoBERTa å’Œ GPTï¼‰å’Œæ¨¡å‹å¤§å°ï¼ˆåŸºæœ¬å¤§å°å’Œå¤§å¤§å°ï¼‰ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬ç»è¿‡æç‚¼çš„è¾ƒå°è¯­è¨€æ¨¡å‹åœ¨å„ç§ NLP ä»»åŠ¡ä¸­æ˜¾ç€è¶…è¶Šäº†ç°æœ‰çš„ KD æ–¹æ³•ã€‚

## Connecting Pre-trained Language Model and Downstream Task via Properties of Representation<sup>poster<sup>

Authors: Chenwei Wu, Holden Lee, Rong Ge

Link: [https://neurips.cc/virtual/2023/poster/71303](https://neurips.cc/virtual/2023/poster/71303)

Abstract:

 Recently, researchers have found that representations learned by large-scale pre-trained language models are useful in various downstream tasks. However, there is little theoretical understanding of how pre-training performance is related to downstream task performance. In this paper, we analyze how this performance transfer depends on the properties of the downstream task and the structure of the representations. We consider a log-linear model where a word can be predicted from its context through a network having softmax as its last layer. We show that even if the downstream task is highly structured and depends on a simple function of the hidden representation, there are still cases when a low pre-training loss cannot guarantee good performance on the downstream task. On the other hand, we propose and empirically validate the existence of an ``anchor vector'' in the representation space, and show that this assumption, together with properties of the downstream task,  guarantees performance transfer.

æ‘˜è¦:

æœ€è¿‘ï¼Œç ”ç©¶äººå‘˜å‘ç°ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨ç¤ºåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­éå¸¸æœ‰ç”¨ã€‚ç„¶è€Œï¼Œå¯¹äºé¢„è®­ç»ƒæ€§èƒ½ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œç†è®ºç†è§£å¾ˆå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™ç§æ€§èƒ½è½¬ç§»å¦‚ä½•å–å†³äºä¸‹æ¸¸ä»»åŠ¡çš„å±æ€§å’Œè¡¨ç¤ºçš„ç»“æ„ã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå¯¹æ•°çº¿æ€§æ¨¡å‹ï¼Œå…¶ä¸­å¯ä»¥é€šè¿‡ä»¥ softmax ä½œä¸ºæœ€åä¸€å±‚çš„ç½‘ç»œä»ä¸Šä¸‹æ–‡ä¸­é¢„æµ‹å•è¯ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿ä¸‹æ¸¸ä»»åŠ¡æ˜¯é«˜åº¦ç»“æ„åŒ–çš„å¹¶ä¸”ä¾èµ–äºéšè—è¡¨ç¤ºçš„ç®€å•å‡½æ•°ï¼Œä»ç„¶å­˜åœ¨ä½é¢„è®­ç»ƒæŸå¤±ä¸èƒ½ä¿è¯ä¸‹æ¸¸ä»»åŠ¡è‰¯å¥½æ€§èƒ½çš„æƒ…å†µã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºå¹¶å‡­ç»éªŒéªŒè¯äº†è¡¨ç¤ºç©ºé—´ä¸­â€œé”šå‘é‡â€çš„å­˜åœ¨ï¼Œå¹¶è¡¨æ˜è¯¥å‡è®¾ä¸ä¸‹æ¸¸ä»»åŠ¡çš„å±æ€§ä¸€èµ·ä¿è¯äº†æ€§èƒ½è½¬ç§»ã€‚

## Exposing Attention Glitches with Flip-Flop Language Modeling<sup>poster<sup>

Authors: Bingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang

Link: [https://neurips.cc/virtual/2023/poster/71426](https://neurips.cc/virtual/2023/poster/71426)

Abstract:

 Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs.

æ‘˜è¦:

ä¸ºä»€ä¹ˆå¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ—¶ä¼šè¾“å‡ºä¸å‡†ç¡®çš„äº‹å®å¹¶è¡¨ç°å‡ºé”™è¯¯çš„æ¨ç†ï¼Ÿè¿™äº›æ¨¡å‹çš„è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰§è¡Œé•¿é“¾æ¨ç†æ—¶ï¼Œç›®å‰ä¼¼ä¹æ˜¯ä¸ºå…¶è¿è´¯åœ°ç»¼åˆçŸ¥è¯†ã€è¯­ç”¨å­¦å’ŒæŠ½è±¡æ€ç»´çš„å…ˆè¿›èƒ½åŠ›ä»˜å‡ºçš„ä¸å¯é¿å…çš„ä»£ä»·ã€‚ä¸ºäº†ç†è§£è¿™ä¸ªæ ¹æœ¬ä¸Šæœªè§£å†³çš„é—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œè¯†åˆ«å¹¶åˆ†æäº†æ³¨æ„åŠ›æ•…éšœç°è±¡ï¼Œå…¶ä¸­ Transformer æ¶æ„çš„å½’çº³åå·®é—´æ­‡æ€§åœ°æ— æ³•æ•è·ç¨³å¥çš„æ¨ç†ã€‚ä¸ºäº†éš”ç¦»è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§¦å‘å™¨è¯­è¨€æ¨¡å‹ï¼ˆFFLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°åŒ–çš„ç»¼åˆåŸºå‡†ç³»åˆ—ï¼Œæ—¨åœ¨æ¢æµ‹ç¥ç»è¯­è¨€æ¨¡å‹çš„å¤–æ¨è¡Œä¸ºã€‚è¿™ä¸ªç®€å•çš„ç”Ÿæˆä»»åŠ¡éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥å¤åˆ¶è¿œç¨‹ä¾èµ–å…³ç³»ä¸Šçš„äºŒè¿›åˆ¶ç¬¦å·ï¼Œå¿½ç•¥ä¹‹é—´çš„æ ‡è®°ã€‚æˆ‘ä»¬å‘ç° Transformer FFLM å­˜åœ¨é•¿å°¾çš„é›¶æ˜Ÿæ¨ç†é”™è¯¯ï¼Œå…¶ä¸­ä¸€äº›é”™è¯¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å„ç§æ­£åˆ™åŒ–æŠ€æœ¯æ¥æ¶ˆé™¤ã€‚æˆ‘ä»¬çš„åˆæ­¥æœºåˆ¶åˆ†æè¡¨æ˜äº†ä¸ºä»€ä¹ˆå‰©ä½™çš„é”™è¯¯å¯èƒ½å¾ˆéš¾è¯Šæ–­å’Œè§£å†³ã€‚æˆ‘ä»¬å‡è®¾æ³¨æ„åŠ›ç¼ºé™·æ˜¯è‡ªç„¶æ³•å­¦ç¡•å£«ä¸­ï¼ˆéƒ¨åˆ†ï¼‰å°é—­åŸŸå¹»è§‰çš„åŸå› ã€‚

## WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding<sup>poster<sup>

Authors: Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang

Link: [https://neurips.cc/virtual/2023/poster/71216](https://neurips.cc/virtual/2023/poster/71216)

Abstract:

 Graphs are widely used to model interconnected entities and improve downstream predictions in various real-world applications. However, real-world graphs nowadays are often associated with complex attributes on multiple types of nodes and even links that are hard to model uniformly, while the widely used graph neural networks (GNNs) often require sufficient training toward specific downstream predictions to achieve strong performance. In this work, we take a fundamentally different approach than GNNs, to simultaneously achieve deep joint modeling of complex attributes and flexible structures of real-world graphs and obtain unsupervised generic graph representations that are not limited to specific downstream predictions. Our framework, built on a natural integration of language models (LMs) and random walks (RWs), is straightforward, powerful and data-efficient. Specifically, we first perform attributed RWs on the graph and design an automated program to compose roughly meaningful textual sequences directly from the attributed RWs; then we fine-tune an LM using the RW-based textual sequences and extract embedding vectors from the LM, which encapsulates both attribute semantics and graph structures. In our experiments, we evaluate the learned node embeddings towards different downstream prediction tasks on multiple real-world attributed graph datasets and observe significant improvements over a comprehensive set of state-of-the-art unsupervised node embedding methods. We believe this work opens a door for more sophisticated technical designs and empirical evaluations toward the leverage of LMs for the modeling of real-world graphs.

æ‘˜è¦:

å›¾è¢«å¹¿æ³›ç”¨äºå¯¹äº’è¿å®ä½“è¿›è¡Œå»ºæ¨¡å¹¶æ”¹è¿›å„ç§ç°å®åº”ç”¨ä¸­çš„ä¸‹æ¸¸é¢„æµ‹ã€‚ç„¶è€Œï¼Œå½“ä»Šç°å®ä¸–ç•Œçš„å›¾é€šå¸¸ä¸å¤šç§ç±»å‹çš„èŠ‚ç‚¹ç”šè‡³é“¾æ¥ä¸Šçš„å¤æ‚å±æ€§ç›¸å…³è”ï¼Œéš¾ä»¥ç»Ÿä¸€å»ºæ¨¡ï¼Œè€Œå¹¿æ³›ä½¿ç”¨çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä¸‹æ¸¸é¢„æµ‹è¿›è¡Œå……åˆ†çš„è®­ç»ƒæ‰èƒ½è·å¾—å¼ºå¤§çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸ GNN æ ¹æœ¬ä¸åŒçš„æ–¹æ³•ï¼ŒåŒæ—¶å®ç°ç°å®ä¸–ç•Œå›¾çš„å¤æ‚å±æ€§å’Œçµæ´»ç»“æ„çš„æ·±åº¦è”åˆå»ºæ¨¡ï¼Œå¹¶è·å¾—ä¸é™äºç‰¹å®šä¸‹æ¸¸é¢„æµ‹çš„æ— ç›‘ç£é€šç”¨å›¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹åœ¨è¯­è¨€æ¨¡å‹ (LM) å’Œéšæœºæ¸¸èµ° (RW) çš„è‡ªç„¶é›†æˆä¹‹ä¸Šï¼Œç®€å•ã€å¼ºå¤§ä¸”æ•°æ®é«˜æ•ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨å›¾ä¸Šæ‰§è¡Œå½’å›  RWï¼Œå¹¶è®¾è®¡ä¸€ä¸ªè‡ªåŠ¨åŒ–ç¨‹åºï¼Œç›´æ¥ä»å½’å›  RW ç»„æˆå¤§è‡´æœ‰æ„ä¹‰çš„æ–‡æœ¬åºåˆ—ï¼›ç„¶åæˆ‘ä»¬ä½¿ç”¨åŸºäº RW çš„æ–‡æœ¬åºåˆ—å¯¹ LM è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä» LM ä¸­æå–åµŒå…¥å‘é‡ï¼Œå…¶ä¸­å°è£…äº†å±æ€§è¯­ä¹‰å’Œå›¾ç»“æ„ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªçœŸå®ä¸–ç•Œå±æ€§å›¾æ•°æ®é›†ä¸Šä¸åŒä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡çš„å­¦ä¹ èŠ‚ç‚¹åµŒå…¥ï¼Œå¹¶è§‚å¯Ÿåˆ°ç›¸å¯¹äºä¸€ç»„æœ€å…ˆè¿›çš„æ— ç›‘ç£èŠ‚ç‚¹åµŒå…¥æ–¹æ³•çš„æ˜¾ç€æ”¹è¿›ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œä¸ºæ›´å¤æ‚çš„æŠ€æœ¯è®¾è®¡å’Œå®è¯è¯„ä¼°æ‰“å¼€äº†ä¸€æ‰‡å¤§é—¨ï¼Œä»¥åˆ©ç”¨è¯­è¨€æ¨¡å‹å¯¹ç°å®ä¸–ç•Œçš„å›¾å½¢è¿›è¡Œå»ºæ¨¡ã€‚

## Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision<sup>poster<sup>

Authors: Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan

Link: [https://neurips.cc/virtual/2023/poster/70433](https://neurips.cc/virtual/2023/poster/70433)

Abstract:

 Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.

æ‘˜è¦:

æœ€è¿‘çš„äººå·¥æ™ºèƒ½åŠ©ç†ä»£ç†ï¼Œä¾‹å¦‚ ChatGPTï¼Œä¸»è¦ä¾é å¸¦æœ‰äººç±»æ³¨é‡Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ¥ä½¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»æ„å›¾ä¿æŒä¸€è‡´ï¼Œç¡®ä¿ä»–ä»¬ä¹äºåŠ©äººã€æœ‰é“å¾·ã€å¯é ã€‚ç„¶è€Œï¼Œç”±äºè·å¾—äººç±»ç›‘ç£çš„æˆæœ¬é«˜æ˜‚ä»¥åŠè´¨é‡ã€å¯é æ€§ã€å¤šæ ·æ€§ã€è‡ªæˆ‘ä¸€è‡´æ€§å’Œä¸è‰¯åå·®ç­‰ç›¸å…³é—®é¢˜ï¼Œè¿™ç§ä¾èµ–æ€§å¯èƒ½ä¼šä¸¥é‡é™åˆ¶äººå·¥æ™ºèƒ½åŠ©ç†ä»£ç†çš„çœŸæ­£æ½œåŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œè‡ªè°ƒæ•´â€çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸåˆ™é©±åŠ¨çš„æ¨ç†å’Œæ³•å­¦ç¡•å£«çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ä»¥åœ¨æœ€å°‘çš„äººç±»ç›‘ç£ä¸‹å®ç°äººå·¥æ™ºèƒ½ä»£ç†çš„è‡ªè°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å››ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ³•å­¦ç¡•å£«ç”Ÿæˆåˆæˆæç¤ºï¼Œå¹¶ä½¿ç”¨ä¸»é¢˜å¼•å¯¼æ–¹æ³•æ¥å¢å¼ºæç¤ºå¤šæ ·æ€§ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€å°ç»„äººç±»ç¼–å†™çš„äººå·¥æ™ºèƒ½æ¨¡å‹éµå¾ªçš„åŸåˆ™ï¼Œå¹¶æŒ‡å¯¼æ³•å­¦ç¡•å£«é€šè¿‡æ¼”ç¤ºï¼ˆåŸåˆ™åº”ç”¨ï¼‰è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»¥å¯¹ç”¨æˆ·çš„æŸ¥è¯¢äº§ç”Ÿæœ‰ç”¨çš„ã€é“å¾·çš„å’Œå¯é çš„å“åº”ï¼›ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ç”¨é«˜è´¨é‡çš„è‡ªå¯¹é½å“åº”å¯¹åŸå§‹LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥ä¾¿ç”Ÿæˆçš„æ¨¡å‹å¯ä»¥ç›´æ¥ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆç†æƒ³çš„å“åº”ï¼Œè€Œä¸å†éœ€è¦åŸç†é›†å’Œæ¼”ç¤ºï¼›æœ€åï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ”¹è¿›æ­¥éª¤æ¥è§£å†³è¿‡äºç®€çŸ­æˆ–é—´æ¥çš„å›åº”é—®é¢˜ã€‚å°†SELF-ALIGNåº”ç”¨äºLLaMA-65båŸºç¡€è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºDromedaryçš„AIåŠ©æ‰‹ã€‚äººå·¥æ³¨é‡Šå°‘äº 300 è¡Œï¼ˆåŒ…æ‹¬ < 200 æ¡ç§å­æç¤ºã€16 æ¡é€šç”¨åŸåˆ™å’Œ 5 ä¸ªä¸Šä¸‹æ–‡å­¦ä¹ ç¤ºä¾‹ï¼‰ã€‚ Dromedary åœ¨å„ç§è®¾ç½®çš„åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾ç€è¶…è¶Šäº†å‡ ä¸ªæœ€å…ˆè¿›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ Text-Davinci-003 å’Œ Alpacaã€‚

## StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models<sup>poster<sup>

Authors: Yinghao Aaron Li, Cong Han, Vinay Raghavan, Gavin Mischler, Nima Mesgarani

Link: [https://neurips.cc/virtual/2023/poster/70566](https://neurips.cc/virtual/2023/poster/70566)

Abstract:

 In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.

æ‘˜è¦:

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† StyleTTS 2ï¼Œè¿™æ˜¯ä¸€ç§æ–‡æœ¬è½¬è¯­éŸ³ (TTS) æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ (SLM) çš„é£æ ¼æ‰©æ•£å’Œå¯¹æŠ—æ€§è®­ç»ƒæ¥å®ç°äººç±»æ°´å¹³çš„ TTS åˆæˆã€‚ StyleTTS 2 ä¸å…¶å‰èº«ä¸åŒï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å°†é£æ ¼å»ºæ¨¡ä¸ºæ½œåœ¨éšæœºå˜é‡ï¼Œæ— éœ€å‚è€ƒè¯­éŸ³å³å¯ç”Ÿæˆæœ€é€‚åˆæ–‡æœ¬çš„é£æ ¼ï¼Œå®ç°é«˜æ•ˆçš„æ½œåœ¨æ‰©æ•£ï¼ŒåŒæ—¶å—ç›Šäºæ‰©æ•£æ¨¡å‹æä¾›çš„å¤šæ ·åŒ–è¯­éŸ³åˆæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹é¢„è®­ç»ƒ SLMï¼ˆä¾‹å¦‚ WavLMï¼‰ä½œä¸ºåˆ¤åˆ«å™¨ï¼Œå¹¶é‡‡ç”¨æ–°é¢–çš„å¯å¾®æŒç»­æ—¶é—´æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä»è€Œæé«˜è¯­éŸ³è‡ªç„¶åº¦ã€‚æ ¹æ®ä»¥è‹±è¯­ä¸ºæ¯è¯­çš„äººçš„åˆ¤æ–­ï¼ŒStyleTTS 2 åœ¨å•è¯´è¯äºº LJSpeech æ•°æ®é›†ä¸Šè¶…è¶Šäº†äººç±»å½•éŸ³ï¼Œå¹¶åœ¨å¤šè¯´è¯äºº VCTK æ•°æ®é›†ä¸ŠåŒ¹é…ã€‚æ­¤å¤–ï¼Œå½“åœ¨ LibriTTS æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯´è¯äººé€‚åº”æ–¹é¢ä¼˜äºä¹‹å‰å…¬å¼€å¯ç”¨çš„æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œåœ¨å•è¯´è¯äººå’Œå¤šè¯´è¯äººæ•°æ®é›†ä¸Šå®ç°äº†ç¬¬ä¸€ä¸ªäººç±»æ°´å¹³çš„ TTSï¼Œå±•ç¤ºäº†é£æ ¼æ‰©æ•£å’Œå¤§å‹ SLM å¯¹æŠ—è®­ç»ƒçš„æ½œåŠ›ã€‚éŸ³é¢‘æ¼”ç¤ºå’Œæºä»£ç å¯åœ¨ https://styletts2.github.io/ è·å–ã€‚

## 3D-LLM: Injecting the 3D World into Large Language Models<sup>poster<sup>

Authors: Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan

Link: [https://neurips.cc/virtual/2023/poster/71298](https://neurips.cc/virtual/2023/poster/71298)

Abstract:

 Large language models (LLMs) and Vision-Language Models (VLMs) have been proved to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3Dgrounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs could better capture 3D spatial information.  Experiments on ScanQA  show that our model outperforms state-of-the-art baselines by a large margin (\textit{e.g.}, the BLEU-1 score surpasses state-of-the-art score by 9\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Our model and data will be publicly available.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å·²è¢«è¯æ˜åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚å¸¸è¯†æ¨ç†ã€‚å°½ç®¡è¿™äº›æ¨¡å‹éå¸¸å¼ºå¤§ï¼Œä½†å®ƒä»¬å¹¶ä¸ä»¥ 3D ç‰©ç†ä¸–ç•Œä¸ºåŸºç¡€ï¼Œè€Œ 3D ç‰©ç†ä¸–ç•Œæ¶‰åŠæ›´ä¸°å¯Œçš„æ¦‚å¿µï¼Œä¾‹å¦‚ç©ºé—´å…³ç³»ã€å¯ä¾›æ€§ã€ç‰©ç†ã€å¸ƒå±€ç­‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®å°† 3D ä¸–ç•Œæ³¨å…¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¹¶å¼•å…¥å…¨æ–°çš„ 3D-LLM ç³»åˆ—ã€‚å…·ä½“æ¥è¯´ï¼Œ3D-LLM å¯ä»¥å°† 3D ç‚¹äº‘åŠå…¶ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ‰§è¡Œå„ç§ 3D ç›¸å…³ä»»åŠ¡ï¼ŒåŒ…æ‹¬å­—å¹•ã€å¯†é›†å­—å¹•ã€3D é—®ç­”ã€ä»»åŠ¡åˆ†è§£ã€3D åŸºç¡€ã€3D è¾…åŠ©å¯¹è¯ã€å¯¼èˆªç­‰åœ¨ã€‚ä½¿ç”¨æˆ‘ä»¬è®¾è®¡çš„ä¸‰ç§ç±»å‹çš„æç¤ºæœºåˆ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ”¶é›†è¶…è¿‡ 30 ä¸‡ä¸ªæ¶µç›–è¿™äº›ä»»åŠ¡çš„ 3D è¯­è¨€æ•°æ®ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒ 3D-LLMï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨ 3D ç‰¹å¾æå–å™¨ä»æ¸²æŸ“çš„å¤šè§†å›¾å›¾åƒä¸­è·å– 3D ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ 2D VLM ä½œä¸ºéª¨å¹²æ¥è®­ç»ƒ 3D-LLMã€‚é€šè¿‡å¼•å…¥ 3D å®šä½æœºåˆ¶ï¼Œ3D-LLM å¯ä»¥æ›´å¥½åœ°æ•è· 3D ç©ºé—´ä¿¡æ¯ã€‚ ScanQA ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¤§å¹…ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼ˆ\textit{eg}ï¼ŒBLEU-1 åˆ†æ•°è¶…è¿‡æœ€å…ˆè¿›çš„åˆ†æ•° 9\%ï¼‰ã€‚æ­¤å¤–ï¼Œå¯¹æˆ‘ä»¬ä¿ç•™çš„ 3D å­—å¹•ã€ä»»åŠ¡ç»„åˆå’Œ 3D è¾…åŠ©å¯¹è¯æ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äº 2D VLMã€‚å®šæ€§ç¤ºä¾‹è¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥æ‰§è¡Œè¶…å‡ºç°æœ‰ LLM å’Œ VLM èŒƒå›´çš„æ›´å¤šä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®å°†å…¬å¼€ã€‚

## Large Language Models can Implement Policy Iteration<sup>poster<sup>

Authors: Ethan Brooks, Logan Walls, Richard L Lewis, Satinder Singh

Link: [https://neurips.cc/virtual/2023/poster/71955](https://neurips.cc/virtual/2023/poster/71955)

Abstract:

 In this work, we demonstrate a method for implementing policy iteration using a large language model. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the â€œfew-shotâ€ quality that makes in-context learning attractive to begin with. Our method demonstrates that a large language model can be used to implement policy iteration using the machinery of in-context learning, enabling it to learn to perform RL tasks without expert demonstrations or gradients. Our approach iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our method using Codex (M. Chen et al. 2021b), a language model with no prior knowledge of the domains on which we evaluate it.

æ‘˜è¦:

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°ç­–ç•¥è¿­ä»£çš„æ–¹æ³•ã€‚è™½ç„¶åŸºç¡€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨å—åˆ°äº†ç›¸å½“å¤šçš„å…³æ³¨ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºï¼ˆ1ï¼‰ä¸“å®¶æ¼”ç¤ºçš„ç­–åˆ’ï¼ˆé€šè¿‡æ‰‹åŠ¨è®¾è®¡æˆ–ç‰¹å®šäºä»»åŠ¡çš„é¢„è®­ç»ƒï¼‰æˆ–ï¼ˆ2ï¼‰ä½¿ç”¨æ¢¯åº¦æ–¹æ³•é€‚åº”æ„Ÿå…´è¶£çš„ä»»åŠ¡ï¼ˆé€‚é…å™¨å±‚çš„å¾®è°ƒæˆ–è®­ç»ƒï¼‰ã€‚è¿™ä¸¤ç§æŠ€æœ¯éƒ½æœ‰ç¼ºç‚¹ã€‚æ”¶é›†æ¼”ç¤ºæ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹å·¥ä½œï¼Œä¾èµ–äºè¿™äº›æ¼”ç¤ºçš„ç®—æ³•çš„æ€§èƒ½å¹¶ä¸æ¯”å¾—å‡ºæ¼”ç¤ºçš„ä¸“å®¶æ›´å¥½ã€‚æ‰€æœ‰æ¢¯åº¦æŠ€æœ¯æœ¬è´¨ä¸Šéƒ½å¾ˆæ…¢ï¼Œç‰ºç‰²äº†â€œå°‘æ ·æœ¬â€çš„è´¨é‡ï¼Œè€Œè¿™ä½¿å¾—ä¸Šä¸‹æ–‡å­¦ä¹ ä¸€å¼€å§‹å°±å¾ˆæœ‰å¸å¼•åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ç”¨äºä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶æ¥å®ç°ç­–ç•¥è¿­ä»£ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ‰§è¡Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œè€Œæ— éœ€ä¸“å®¶æ¼”ç¤ºæˆ–æ¢¯åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸ RL ç¯å¢ƒçš„è¯•é”™äº¤äº’æ¥è¿­ä»£æ›´æ–°æç¤ºçš„å†…å®¹ï¼Œå¹¶ä»ä¸­å¯¼å‡ºç­–ç•¥ã€‚ä¸ºäº†æ¶ˆé™¤æƒé‡å­¦ä¹ çš„ä½œç”¨ï¼ˆåƒ Decision Transformer è¿™æ ·çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºæƒé‡å­¦ä¹ ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨ Codexï¼ˆM. Chen ç­‰äººï¼Œ2021bï¼‰æ¼”ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§è¯­è¨€æ¨¡å‹ï¼Œæ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„é¢†åŸŸæˆ‘ä»¬å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚

## Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models<sup>poster<sup>

Authors: Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao

Link: [https://neurips.cc/virtual/2023/poster/72147](https://neurips.cc/virtual/2023/poster/72147)

Abstract:

 Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.

æ‘˜è¦:

ç”±äºæ–°å…´çš„æ¨ç†èƒ½åŠ›ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾ç€çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œæ³•å­¦ç¡•å£«æœ‰å…¶å›ºæœ‰çš„å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•è®¿é—®æœ€æ–°ä¿¡æ¯ï¼ˆå­˜å‚¨åœ¨ç½‘ç»œä¸Šæˆ–ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†åº“ä¸­ï¼‰ã€ä½¿ç”¨å¤–éƒ¨å·¥å…·ä»¥åŠæ‰§è¡Œç²¾ç¡®çš„æ•°å­¦å’Œé€»è¾‘æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Chameleonï¼Œè¿™æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå®ƒé€šè¿‡ä½¿ç”¨ç”¨äºç»„åˆæ¨ç†çš„å³æ’å³ç”¨æ¨¡å—æ¥å¢å¼ºæ³•å­¦ç¡•å£«ï¼Œä»è€Œç¼“è§£è¿™äº›é™åˆ¶ã€‚ Chameleon é€šè¿‡ç»„åˆå„ç§å·¥å…·ï¼ˆä¾‹å¦‚ï¼ŒLLMã€ç°æˆçš„è§†è§‰æ¨¡å‹ã€ç½‘ç»œæœç´¢å¼•æ“ã€Python å‡½æ•°å’ŒåŸºäºå¯å‘å¼çš„æ¨¡å—ï¼‰æ¥ç»¼åˆç¨‹åºï¼Œä»¥å®Œæˆå¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ Chameleon çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäº LLM çš„è§„åˆ’å™¨ï¼Œå®ƒç»„è£…äº†ä¸€ç³»åˆ—å·¥å…·æ¥æ‰§è¡Œä»¥ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚æˆ‘ä»¬å±•ç¤ºäº† Chameleon åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€çŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼šScienceQA å’Œ TabMWPã€‚ Chameleon ç”± GPT-4 æä¾›æ”¯æŒï¼Œåœ¨ ScienceQA ä¸Šå®ç°äº† 86.54% çš„æ•´ä½“å‡†ç¡®ç‡ï¼Œå°†æœ€ä½³å·²å‘è¡¨çš„å°‘æ•°ç»“æœæé«˜äº† 11.37%ã€‚åœ¨ TabMWP ä¸Šï¼Œç”± GPT-4 é©±åŠ¨çš„ Chameleon å°†å‡†ç¡®ç‡æé«˜äº† 17.0%ï¼Œå°†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡æå‡è‡³ 98.78%ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œä¸ ChatGPT æ”¯æŒçš„è§„åˆ’å™¨ç›¸æ¯”ï¼ŒGPT-4 æ”¯æŒçš„è§„åˆ’å™¨é€šè¿‡ä»æŒ‡ä»¤æ¨æ–­æ½œåœ¨çº¦æŸæ¥è¡¨ç°å‡ºæ›´åŠ ä¸€è‡´å’Œåˆç†çš„å·¥å…·é€‰æ‹©ã€‚

## Direct Preference Optimization: Your Language Model is Secretly a Reward Model<sup>oral<sup>

Authors: Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn

Link: [https://neurips.cc/virtual/2023/poster/72164](https://neurips.cc/virtual/2023/poster/72164)

Abstract:

 While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.

æ‘˜è¦:

è™½ç„¶å¤§è§„æ¨¡æ— ç›‘ç£è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å­¦ä¹ å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œä¸€äº›æ¨ç†æŠ€èƒ½ï¼Œä½†ç”±äºå…¶è®­ç»ƒçš„å®Œå…¨æ— ç›‘ç£æ€§è´¨ï¼Œå®ç°å¯¹å…¶è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶å¾ˆå›°éš¾ã€‚è·å¾—è¿™ç§å¯æ“çºµæ€§çš„ç°æœ‰æ–¹æ³•æ”¶é›†æ¨¡å‹ç”Ÿæˆç›¸å¯¹è´¨é‡çš„äººç±»æ ‡ç­¾ï¼Œå¹¶å¾®è°ƒæ— ç›‘ç£çš„ LM ä»¥ç¬¦åˆè¿™äº›åå¥½ï¼Œé€šå¸¸é€šè¿‡äººç±»åé¦ˆï¼ˆRLHFï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚ç„¶è€Œï¼ŒRLHF æ˜¯ä¸€ä¸ªå¤æ‚ä¸”é€šå¸¸ä¸ç¨³å®šçš„è¿‡ç¨‹ï¼Œé¦–å…ˆæ‹Ÿåˆåæ˜ äººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¯¹å¤§å‹æ— ç›‘ç£ LM è¿›è¡Œå¾®è°ƒï¼Œä»¥æœ€å¤§åŒ–ä¼°è®¡çš„å¥–åŠ±ï¼Œè€Œä¸ä¼šåç¦»åŸå§‹æ¨¡å‹å¤ªè¿œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å¥–åŠ±å‡½æ•°å’Œæœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„æ˜ å°„æ¥è¡¨æ˜ï¼Œè¿™ç§çº¦æŸå¥–åŠ±æœ€å¤§åŒ–é—®é¢˜å¯ä»¥é€šè¿‡å•é˜¶æ®µçš„ç­–ç•¥è®­ç»ƒæ¥ç²¾ç¡®ä¼˜åŒ–ï¼Œæœ¬è´¨ä¸Šè§£å†³äº†äººç±»åå¥½æ•°æ®çš„åˆ†ç±»é—®é¢˜ã€‚ç”±æ­¤äº§ç”Ÿçš„ç®—æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç›´æ¥åå¥½ä¼˜åŒ– (DPO)ï¼Œç¨³å®šã€é«˜æ€§èƒ½ä¸”è®¡ç®—é‡è½»ï¼Œæ— éœ€æ‹Ÿåˆå¥–åŠ±æ¨¡å‹ã€åœ¨å¾®è°ƒæœŸé—´ä» LM é‡‡æ ·æˆ–æ‰§è¡Œé‡è¦çš„è¶…å‚æ•°è°ƒæ•´ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDPO å¯ä»¥å¾®è°ƒ LM ä»¥ç¬¦åˆäººç±»åå¥½ï¼Œå¹¶ä¸”æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ DPO è¿›è¡Œå¾®è°ƒè¶…å‡ºäº† RLHF æ§åˆ¶å„ä»£æƒ…ç»ªçš„èƒ½åŠ›ï¼Œå¹¶æé«˜äº†æ‘˜è¦å’Œå•è½®å¯¹è¯ä¸­çš„å“åº”è´¨é‡ï¼ŒåŒæ—¶å¤§å¤§ç®€åŒ–äº†å®æ–½å’Œè®­ç»ƒã€‚

## Tree of Thoughts: Deliberate Problem Solving with Large Language Models<sup>oral<sup>

Authors: Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan

Link: [https://neurips.cc/virtual/2023/poster/72797](https://neurips.cc/virtual/2023/poster/72797)

Abstract:

 Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.Our experiments show that ToT significantly enhances language modelsâ€™ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.

æ‘˜è¦:

è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ç”¨äºè§£å†³å„ç§ä»»åŠ¡ä¸­çš„ä¸€èˆ¬é—®é¢˜ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»ç„¶ä»…é™äºæ ‡è®°çº§ã€ä»å·¦åˆ°å³çš„å†³ç­–è¿‡ç¨‹ã€‚è¿™æ„å‘³ç€ä»–ä»¬å¯èƒ½æ— æ³•å®Œæˆéœ€è¦æ¢ç´¢ã€æˆ˜ç•¥å‰ç»æ€§æˆ–åˆå§‹å†³ç­–å‘æŒ¥å…³é”®ä½œç”¨çš„ä»»åŠ¡ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶â€”â€”æ€æƒ³æ ‘ï¼ˆToTï¼‰ï¼Œå®ƒæ¦‚æ‹¬äº†æµè¡Œçš„æ€æƒ³é“¾æ–¹æ³•æ¥æç¤ºè¯­è¨€æ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿæ¢ç´¢ä½œä¸ºæ–‡æœ¬ï¼ˆæ€æƒ³ï¼‰çš„è¿è´¯å•å…ƒã€‚è§£å†³é—®é¢˜çš„ä¸­é—´æ­¥éª¤ã€‚ ToT å…è®¸ LM è¿›è¡Œæ·±æ€ç†Ÿè™‘çš„å†³ç­–ï¼Œé€šè¿‡è€ƒè™‘å¤šç§ä¸åŒçš„æ¨ç†è·¯å¾„å’Œè‡ªæˆ‘è¯„ä¼°é€‰æ‹©æ¥å†³å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨ï¼Œä»¥åŠåœ¨å¿…è¦æ—¶å‘å‰çœ‹æˆ–å›æº¯ä»¥åšå‡ºå…¨å±€é€‰æ‹©ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒToT æ˜¾ç€å¢å¼ºäº†è¯­è¨€æ¨¡å‹â€ è§£å†³ä¸‰é¡¹éœ€è¦ä¸å¹³å‡¡çš„è®¡åˆ’æˆ–æœç´¢çš„æ–°é¢–ä»»åŠ¡çš„é—®é¢˜çš„èƒ½åŠ›ï¼š24 äººæ¸¸æˆã€åˆ›æ„å†™ä½œå’Œè¿·ä½ å¡«å­—æ¸¸æˆã€‚ä¾‹å¦‚ï¼Œåœ¨ Game of 24 ä¸­ï¼Œè™½ç„¶å¸¦æœ‰æ€ç»´é“¾æç¤ºçš„ GPT-4 ä»…è§£å†³äº† 4% çš„ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº† 74% çš„æˆåŠŸç‡ã€‚åŒ…å«æ‰€æœ‰æç¤ºçš„ä»£ç å­˜å‚¨åº“ï¼šhttps://github.com/princeton-nlp/tree-of-thought-llmã€‚

## H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models<sup>poster<sup>

Authors: Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher RÃ©, Clark Barrett, Zhangyang "Atlas" Wang, Beidi Chen

Link: [https://neurips.cc/virtual/2023/poster/71645](https://neurips.cc/virtual/2023/poster/71645)

Abstract:

 Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the $\mathsf{KV}$ $\mathsf{cache}$, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the  $\mathsf{KV}$ $\mathsf{cache}$ which significantly reduces its memory footprint.  Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores.  We call these tokens Heavy Hitters ($\mathsf{H_2}$). Through a comprehensive investigation, we find that ($i$) the emergence of $\mathsf{H_2}$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and ($ii$) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle ($\mathsf{H_2O}$), a $\mathsf{KV}$ $\mathsf{cache}$ eviction policy that dynamically retains a balance of recent and $\mathsf{H_2}$ tokens. We formulate the $\mathsf{KV}$ $\mathsf{cache}$ eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of $\mathsf{H_2O}$ with 20\% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to $29\times$, $29\times$, and $3\times$ on OPT-6.7B and OPT-30B. With the same batch size, $\mathsf{H_2O}$ can reduce the latency by up to $1.9\times$.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°½ç®¡æœ€è¿‘å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆå°±ï¼Œä½†å…¶éƒ¨ç½²æˆæœ¬æ˜æ˜¾è¿‡é«˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¶‰åŠé•¿å†…å®¹ç”Ÿæˆçš„åº”ç”¨ç¨‹åºï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿå’Œæ•…äº‹å†™ä½œã€‚é€šå¸¸ï¼Œé™¤äº†æ¨¡å‹å‚æ•°ä¹‹å¤–ï¼Œå¤§é‡ç¬æ€ä¿¡æ¯ï¼ˆç§°ä¸º $\mathsf{KV}$ $\mathsf{cache}$ï¼‰ä¹Ÿå­˜å‚¨åœ¨ GPU å†…å­˜ä¸­ï¼Œå¹¶éšåºåˆ—é•¿åº¦å’Œæ‰¹é‡å¤§å°çº¿æ€§ç¼©æ”¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å®ç° $\mathsf{KV}$ $\mathsf{cache}$ çš„æ–°é¢–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¾ç€å‡å°‘äº†å…¶å†…å­˜å ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºå€¼å¾—æ³¨æ„çš„è§‚å¯Ÿï¼Œå³åœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶ï¼Œä¸€å°éƒ¨åˆ†ä»¤ç‰Œè´¡çŒ®äº†å¤§éƒ¨åˆ†ä»·å€¼ã€‚æˆ‘ä»¬ç§°è¿™äº›ä»£å¸ä¸ºé‡å‡»è€… ($\mathsf{H_2}$)ã€‚é€šè¿‡å…¨é¢çš„è°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°($i$)$\mathsf{H_2}$çš„å‡ºç°æ˜¯è‡ªç„¶çš„ï¼Œå¹¶ä¸”ä¸æ–‡æœ¬ä¸­æ ‡è®°çš„é¢‘ç¹å…±ç°æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œè€Œ($ii$)åˆ é™¤å®ƒä»¬ä¼šå¯¼è‡´æ€§èƒ½æ˜¾ç€ä¸‹é™ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº† Heavy Hitter Oracle ($\mathsf{H_2O}$)ï¼Œè¿™æ˜¯ä¸€ä¸ª $\mathsf{KV}$ $\mathsf{cache}$ é©±é€ç­–ç•¥ï¼ŒåŠ¨æ€åœ°ä¿æŒæœ€è¿‘å’Œ $\mathsf{H_2 çš„å¹³è¡¡}$ ä»£å¸ã€‚æˆ‘ä»¬å°† $\mathsf{KV}$ $\mathsf{cache}$ é©±é€åˆ¶å®šä¸ºåŠ¨æ€å­æ¨¡é—®é¢˜ï¼Œå¹¶è¯æ˜ï¼ˆåœ¨æ¸©å’Œçš„å‡è®¾ä¸‹ï¼‰æˆ‘ä»¬æ–°é¢–çš„é©±é€ç®—æ³•çš„ç†è®ºä¿è¯ï¼Œè¿™æœ‰åŠ©äºæŒ‡å¯¼æœªæ¥çš„å·¥ä½œã€‚æˆ‘ä»¬ä½¿ç”¨ OPTã€LLaMA å’Œ GPT-NeoX åœ¨å„ç§ä»»åŠ¡ä¸­éªŒè¯äº†ç®—æ³•çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨ 20% é‡é‡çº§çš„ $\mathsf{H_2O}$ å®ç°ï¼Œä¸ä¸‰ä¸ªé¢†å…ˆçš„æ¨ç†ç³»ç»Ÿ DeepSpeed Zero-Inferenceã€Hugging Face Accelerate å’Œ FlexGen ç›¸æ¯”ï¼Œååé‡æé«˜äº†é«˜è¾¾ $29\times$ã€$29\times$ å’Œ $3 OPT-6.7B å’Œ OPT-30B ä¸Šçš„ \times$ã€‚åœ¨ç›¸åŒæ‰¹é‡å¤§å°çš„æƒ…å†µä¸‹ï¼Œ$\mathsf{H_2O}$ æœ€å¤šå¯ä»¥å°†å»¶è¿Ÿé™ä½ $1.9\times$ã€‚

## PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change<sup>poster<sup>

Authors: Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati

Link: [https://neurips.cc/virtual/2023/poster/73553](https://neurips.cc/virtual/2023/poster/73553)

Abstract:

 Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasksâ€“where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge.  There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilitiesâ€“including plan generationâ€“LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.

æ‘˜è¦:

ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’å’Œæ¨ç†å˜åŒ–é•¿æœŸä»¥æ¥ä¸€ç›´è¢«è®¤ä¸ºæ˜¯æ™ºèƒ½ä»£ç†çš„æ ¸å¿ƒèƒ½åŠ›ã€‚å› æ­¤ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›å·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…³äºæ³•å­¦ç¡•å£«è§„åˆ’èƒ½åŠ›çš„è¯´æ³•éƒ½æ˜¯åŸºäºå¸¸è¯†æ€§ä»»åŠ¡â€”â€”å¾ˆéš¾åˆ¤æ–­æ³•å­¦ç¡•å£«æ˜¯åœ¨è§„åˆ’è¿˜æ˜¯åªæ˜¯ä»ä»–ä»¬å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†ä¸­æ£€ç´¢ã€‚å¼ºçƒˆéœ€è¦ç³»ç»Ÿçš„ã€å¯æ‰©å±•çš„ã€å…·æœ‰è¶³å¤Ÿå¤šæ ·æ€§çš„è§„åˆ’åŸºå‡†æ¥è¯„ä¼°æ³•å­¦ç¡•å£«æ˜¯å¦å…·æœ‰å…ˆå¤©çš„è§„åˆ’èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† PlanBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†å¥—ä»¶ï¼ŒåŸºäºè‡ªåŠ¨åŒ–è§„åˆ’ç¤¾åŒºï¼ˆå°¤å…¶æ˜¯å›½é™…è§„åˆ’ç«èµ›ï¼‰ä¸­ä½¿ç”¨çš„é¢†åŸŸç±»å‹ï¼Œç”¨äºæµ‹è¯•æ³•å­¦ç¡•å£«åœ¨è§„åˆ’æˆ–æ¨ç†è¡ŒåŠ¨å’Œå˜æ›´æ–¹é¢çš„èƒ½åŠ›ã€‚ PlanBench åœ¨ä»»åŠ¡é¢†åŸŸå’Œç‰¹å®šè§„åˆ’åŠŸèƒ½æ–¹é¢éƒ½æä¾›äº†è¶³å¤Ÿçš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨ SOTA æ¨¡å‹ï¼ŒLLM çš„è®¸å¤šå…³é”®åŠŸèƒ½ï¼ˆåŒ…æ‹¬è®¡åˆ’ç”Ÿæˆï¼‰çš„æ€§èƒ½ä¹Ÿç›¸å½“ä¸è¶³ã€‚å› æ­¤ï¼ŒPlanBench å¯ä»¥ä½œä¸ºæ³•å­¦ç¡•å£«åœ¨è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„è¿›å±•çš„æœ‰ç”¨æ ‡è®°ã€‚

## Mass-Producing Failures of Multimodal Systems with Language Models<sup>poster<sup>

Authors: Shengbang Tong, Erik Jones, Jacob Steinhardt

Link: [https://neurips.cc/virtual/2023/poster/71572](https://neurips.cc/virtual/2023/poster/71572)

Abstract:

 Deployed multimodal models can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures---generalizable, natural-language descriptions that describe categories of individual failures. To uncover systematic failures, MultiMon scrapes for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model to identify common categories and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g."ignores quantifiers'') of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g."a shelf with a few/many books''). Because CLIP is the backbone for most state-of-the-art multimodal models, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-driving cars. We see MultiMon as a step towards evaluation that autonomously explores the long-tail of potential system failures.

æ‘˜è¦:

éƒ¨ç½²çš„å¤šæ¨¡å¼æ¨¡å‹å¯èƒ½ä¼šä»¥è¯„ä¼°è€…æ²¡æœ‰é¢„æ–™åˆ°çš„æ–¹å¼å¤±è´¥ã€‚ä¸ºäº†åœ¨éƒ¨ç½²ä¹‹å‰å‘ç°è¿™äº›æ•…éšœï¼Œæˆ‘ä»¬å¼•å…¥äº† MultiMonï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯†åˆ«ç³»ç»Ÿæ•…éšœçš„ç³»ç»Ÿâ€”â€”æè¿°å•ä¸ªæ•…éšœç±»åˆ«çš„å¯æ¦‚æ‹¬çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚ä¸ºäº†å‘ç°ç³»ç»Ÿæ€§æ•…éšœï¼ŒMultiMon æœé›†äº†é”™è¯¯åè®®çš„ç¤ºä¾‹ï¼šè¾“å…¥äº§ç”Ÿç›¸åŒçš„è¾“å‡ºï¼Œä½†ä¸åº”è¯¥äº§ç”Ÿç›¸åŒçš„è¾“å‡ºã€‚ç„¶åï¼Œå®ƒæç¤ºè¯­è¨€æ¨¡å‹è¯†åˆ«å¸¸è§ç±»åˆ«å¹¶ç”¨è‡ªç„¶è¯­è¨€æè¿°å®ƒä»¬ã€‚æˆ‘ä»¬ä½¿ç”¨ MultiMon æ‰¾åˆ°äº† CLIP æ–‡æœ¬ç¼–ç å™¨çš„ 14 ä¸ªç³»ç»Ÿæ•…éšœï¼ˆä¾‹å¦‚â€œå¿½ç•¥é‡è¯â€ï¼‰ï¼Œæ¯ä¸ªæ•…éšœéƒ½åŒ…å«æ•°ç™¾ä¸ªä¸åŒçš„è¾“å…¥ï¼ˆä¾‹å¦‚â€œä¸€ä¸ªä¹¦æ¶ä¸Šæœ‰å‡ æœ¬ä¹¦/å¾ˆå¤šä¹¦â€ï¼‰ã€‚ç”±äº CLIP æ˜¯å¤§å¤šæ•°æœ€å…ˆè¿›çš„å¤šæ¨¡å¼æ¨¡å‹çš„æ”¯æŸ±ï¼Œå› æ­¤è¿™äº›è¾“å…¥ä¼šåœ¨ Midjourney 5.1ã€DALL-Eã€VideoFusion ç­‰ä¸­äº§ç”Ÿå¤±è´¥ã€‚ MultiMon è¿˜å¯ä»¥å¼•å¯¼ä¸ç‰¹å®šç”¨ä¾‹ç›¸å…³çš„æ•…éšœï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚æˆ‘ä»¬å°† MultiMon è§†ä¸ºè¯„ä¼°çš„ä¸€ä¸ªæ­¥éª¤ï¼Œå¯ä»¥è‡ªä¸»æ¢ç´¢æ½œåœ¨ç³»ç»Ÿæ•…éšœçš„é•¿å°¾ã€‚

## FELM: Benchmarking Factuality Evaluation of Large Language Models<sup>poster<sup>

Authors: shiqi chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He

Link: [https://neurips.cc/virtual/2023/poster/73491](https://neurips.cc/virtual/2023/poster/73491)

Abstract:

 Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as FELM. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g. information from Wikipedia), FELM focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on FELM, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.

æ‘˜è¦:

è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æ–‡æœ¬çš„çœŸå®æ€§æ˜¯ä¸€ä¸ªæ–°å…´ä½†é‡è¦çš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨æé†’ç”¨æˆ·æ½œåœ¨çš„é”™è¯¯å¹¶æŒ‡å¯¼æ›´å¯é çš„ LLM çš„å¼€å‘ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¯„ä¼°äº‹å®æ€§çš„è¯„ä¼°è€…æœ¬èº«ä¹Ÿéœ€è¦è¿›è¡Œé€‚å½“çš„è¯„ä¼°ï¼Œä»¥è¡¡é‡è¿›å±•å¹¶ä¿ƒè¿›è¿›æ­¥ã€‚è¿™ä¸€æ–¹å‘ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™å¯¹äº‹å®è¯„ä¼°è€…çš„è¿›æ­¥é€ æˆäº†é‡å¤§éšœç¢ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤§å‹è¯­è¨€æ¨¡å‹äº‹å®æ€§è¯„ä¼°çš„åŸºå‡†ï¼Œç§°ä¸º FELMã€‚åœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†æ³•å­¦ç¡•å£«ç”Ÿæˆçš„å›å¤ï¼Œå¹¶ä»¥ç»†ç²’åº¦çš„æ–¹å¼æ³¨é‡Šäº‹å®æ ‡ç­¾ã€‚ä¸ä¹‹å‰ä¸»è¦å…³æ³¨ä¸–ç•ŒçŸ¥è¯†çš„çœŸå®æ€§ï¼ˆä¾‹å¦‚æ¥è‡ªç»´åŸºç™¾ç§‘çš„ä¿¡æ¯ï¼‰çš„ç ”ç©¶ç›¸åï¼ŒFELM å…³æ³¨è·¨ä¸åŒé¢†åŸŸçš„çœŸå®æ€§ï¼Œæ¶µç›–ä»ä¸–ç•ŒçŸ¥è¯†åˆ°æ•°å­¦å’Œæ¨ç†ã€‚æˆ‘ä»¬çš„æ³¨é‡ŠåŸºäºæ–‡æœ¬ç‰‡æ®µï¼Œè¿™å¯ä»¥å¸®åŠ©æŸ¥æ˜ç‰¹å®šçš„äº‹å®é”™è¯¯ã€‚äº‹å®æ€§æ³¨é‡Šç”±é¢„å®šä¹‰çš„é”™è¯¯ç±»å‹å’Œæ”¯æŒæˆ–åé©³è¯¥é™ˆè¿°çš„å‚è€ƒé“¾æ¥è¿›ä¸€æ­¥è¡¥å……ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å‡ ä¸ªåŸºäº LLM çš„äº‹å®æ€§è¯„ä¼°å™¨åœ¨ FELM ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ™®é€šçš„ LLM å’Œé‚£äº›é€šè¿‡æ£€ç´¢æœºåˆ¶å’Œæ€ç»´è¿‡ç¨‹å¢å¼ºçš„ LLMã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ£€ç´¢æœ‰åŠ©äºäº‹å®è¯„ä¼°ï¼Œä½†ç›®å‰çš„æ³•å­¦ç¡•å£«åœ¨å¿ å®åœ°æ£€æµ‹äº‹å®é”™è¯¯æ–¹é¢è¿˜è¿œè¿œä¸èƒ½ä»¤äººæ»¡æ„ã€‚

## Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks<sup>poster<sup>

Authors: Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, Sung Ju Hwang

Link: [https://neurips.cc/virtual/2023/poster/70015](https://neurips.cc/virtual/2023/poster/70015)

Abstract:

 Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy.Previous studies have focused on building task-specific small Language Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required.Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantly improves the performance of small T5 and GPT models on the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE, StrategyQA, and OpenbookQA.Notably, our method makes the 250M T5 models achieve superior performance against the fine-tuned 3B models, having 12 times larger parameters, on both MedQA-USMLE and StrategyQA benchmarks.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦å¯¹çŸ¥è¯†è¿›è¡Œå¤åˆç†è§£çš„çŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äº LLM çš„é«˜è®¡ç®—è¦æ±‚å’Œå¯¹æ•°æ®éšç§çš„æ‹…å¿§ï¼Œåœ¨å®é™…åº”ç”¨ä¸­éƒ¨ç½² LLM å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡ä½¿ç”¨æ ‡è®°æ•°æ®æˆ–æ•°æ®è¿›è¡Œå¾®è°ƒæ¥æ„å»ºç‰¹å®šäºä»»åŠ¡çš„å°å‹è¯­è¨€æ¨¡å‹ (LM)ã€‚è’¸é¦æ³•å­¦ç¡•å£«ã€‚ç„¶è€Œï¼Œç”±äºå°å‹ LM è®°å¿†æ‰€éœ€çŸ¥è¯†çš„èƒ½åŠ›æœ‰é™ï¼Œè¿™äº›æ–¹æ³•ä¸é€‚åˆçŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ã€‚åœ¨æˆ‘ä»¬å¯¹è®°å¿†çš„ç†è®ºåˆ†æçš„æ¨åŠ¨ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å¢å¼ºæ¨ç†è’¸é¦ï¼ˆKARDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•å¾®è°ƒå°å‹ LMï¼Œä»¥ç”Ÿæˆä» LLM è·å¾—çš„åŸºæœ¬åŸç†ï¼Œå¹¶ä½¿ç”¨ä»å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢çš„å¢å¼ºçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ç¥ç»é‡æ–°æ’åºå™¨æ¥è·å–ä¸åŸºæœ¬åŸç†ç”Ÿæˆç›¸å…³çš„æ–‡æ¡£ã€‚æˆ‘ä»¬çš„ç»éªŒè¡¨æ˜ï¼ŒKARD æ˜¾ç€æé«˜äº†å°å‹ T5 å’Œ GPT æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çŸ¥è¯†å¯†é›†å‹æ¨ç†æ•°æ®é›†ï¼ˆå³ MedQA-USMLEã€StrategyQA å’Œ OpenbookQAï¼‰ä¸Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ 250M T5 æ¨¡å‹åœ¨ç²¾ç»†æ¨ç†æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨ MedQA-USMLE å’Œ StrategyQA åŸºå‡†ä¸Šè°ƒæ•´äº† 3B æ¨¡å‹ï¼Œå‚æ•°å¢åŠ äº† 12 å€ã€‚

## QuIP: 2-Bit Quantization of Large Language Models With Guarantees<sup>poster<sup>

Authors: Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa

Link: [https://neurips.cc/virtual/2023/poster/69982](https://neurips.cc/virtual/2023/poster/69982)

Abstract:

 This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP.

æ‘˜è¦:

è¿™é¡¹å·¥ä½œç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è®­ç»ƒåå‚æ•°é‡åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸ç›¸å¹²å¤„ç†é‡åŒ–ï¼ˆQuIPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒåŸºäºè¿™æ ·çš„è§è§£ï¼šé‡åŒ–å—ç›Šäºä¸ç›¸å¹²æƒé‡å’Œ Hessian çŸ©é˜µï¼Œå³æƒé‡çš„å¤§å°å‡åŒ€ï¼Œå¹¶ä¸”å‡†ç¡®èˆå…¥å®ƒä»¬çš„æ–¹å‘ä¸å¯¹é½å¾ˆé‡è¦ä¸åæ ‡è½´ã€‚ QuIP åŒ…å«ä¸¤ä¸ªæ­¥éª¤ï¼š(1) è‡ªé€‚åº”èˆå…¥è¿‡ç¨‹ï¼Œæœ€å°åŒ–äºŒæ¬¡ä»£ç†ç›®æ ‡ï¼› (2) é«˜æ•ˆçš„é¢„å¤„ç†å’Œåå¤„ç†ï¼Œé€šè¿‡éšæœºæ­£äº¤çŸ©é˜µçš„ä¹˜æ³•ç¡®ä¿æƒé‡å’Œ Hessian ä¸ç›¸å¹²æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ LLM è§„æ¨¡é‡åŒ–ç®—æ³•çš„é¦–æ¬¡ç†è®ºåˆ†ææ¥è¡¥å…… QuIPï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„ç†è®ºä¹Ÿé€‚ç”¨äºç°æœ‰æ–¹æ³• OPTQã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„ä¸ç›¸å¹²é¢„å¤„ç†æ”¹è¿›äº†å‡ ç§ç°æœ‰çš„é‡åŒ–ç®—æ³•ï¼Œå¹¶äº§ç”Ÿäº†ç¬¬ä¸€ä¸ª LLM é‡åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æ¯ä¸ªæƒé‡ä¸¤ä½å³å¯äº§ç”Ÿå¯è¡Œçš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ https://github.com/jerry-chee/QuIP æ‰¾åˆ°ã€‚

## Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples<sup>poster<sup>

Authors: Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He

Link: [https://neurips.cc/virtual/2023/poster/71923](https://neurips.cc/virtual/2023/poster/71923)

Abstract:

 Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.

æ‘˜è¦:

é‰´äºè¯æ˜ç©ºé—´æå…¶åºå¤§ï¼Œä»»ä½•èƒ½å¤Ÿè¿›è¡Œä¸€èˆ¬æ¼”ç»æ¨ç†çš„æ¨¡å‹éƒ½å¿…é¡»æ¨å¹¿åˆ°æ›´å¤æ‚çš„è¯æ˜ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»™å®šæ€ç»´é“¾æç¤ºçš„æƒ…å†µä¸‹å…·æœ‰ä¸€å®šçš„æŠ½è±¡æ¼”ç»æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦æ˜¯åœ¨ä½¿ç”¨å‰ä»¶æˆ–ç‰¹å®šå¤§å°çš„è¯æ˜ä¸Šè¿›è¡Œæµ‹è¯•çš„ï¼Œå¹¶ä¸”æ¥è‡ªä¸ä¸Šä¸‹æ–‡ç¤ºä¾‹ç›¸åŒçš„åˆ†å¸ƒã€‚ä¸ºäº†è¡¡é‡æ³•å­¦ç¡•å£«çš„ä¸€èˆ¬æ¼”ç»æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸€ç³»åˆ—å¹¿æ³›çš„æ¼”ç»è§„åˆ™ï¼Œå¹¶è¡¡é‡ä»–ä»¬ä»å¤šä¸ªè§’åº¦ï¼ˆæ·±åº¦ã€å®½åº¦å’Œç»„åˆæ¦‚æ‹¬ï¼‰ä»æ›´ç®€å•çš„æ¼”ç¤ºæ¨å¹¿åˆ°æ›´å¤æ‚è¯æ˜çš„èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›ç³»ç»Ÿæ¢ç´¢ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åˆæˆå’Œå¯ç¼–ç¨‹æ¨ç†æ•°æ®é›†ï¼Œå¯ä»¥æ§åˆ¶æ¼”ç»è§„åˆ™å’Œè¯æ˜å¤æ‚æ€§ã€‚æˆ‘ä»¬å¯¹å››ä¸ªä¸åŒè§„æ¨¡å’ŒåŸ¹è®­ç›®æ ‡çš„æ³•å­¦ç¡•å£«è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œå®ƒä»¬èƒ½å¤Ÿæ¨å¹¿åˆ°ç»„åˆè¯æ˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾ˆéš¾æ¨å¹¿åˆ°æ›´é•¿çš„è¯æ˜ï¼Œå¹¶ä¸”éœ€è¦æ˜ç¡®çš„è®ºè¯æ¥äº§ç”Ÿå‡è®¾çš„å­è¯æ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¡ˆä¾‹è¯æ˜å’ŒçŸ›ç›¾è¯æ˜ä¸­ã€‚

## Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning<sup>poster<sup>

Authors: Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang

Link: [https://neurips.cc/virtual/2023/poster/72494](https://neurips.cc/virtual/2023/poster/72494)

Abstract:

 In recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the real-world LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information.

æ‘˜è¦:

è¿‘å¹´æ¥ï¼Œé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®ç°æ¨ç†æ—¶é—´å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ˆå³ä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾ç€çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®å¼ºè°ƒäº†è¿™ç§èƒ½åŠ›å¯¹å°æ ·æœ¬æ¼”ç¤ºé€‰æ‹©çš„æ•æ„Ÿæ€§ã€‚ç›®å‰å¯¹å¸¸è§„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç›®æ ‡äº§ç”Ÿè¿™ç§èƒ½åŠ›çš„åº•å±‚æœºåˆ¶çš„ç†è§£ä»ç„¶ä¸ç°å®ä¸–ç•Œçš„æ³•å­¦ç¡•å£«è„±èŠ‚ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡è´å¶æ–¯é€é•œæ£€éªŒæƒ…å¢ƒå­¦ä¹ ç°è±¡ï¼Œå°†ç°å®ä¸–ç•Œçš„æ³•å­¦ç¡•å£«è§†ä¸ºæ½œåœ¨å˜é‡æ¨¡å‹ã€‚åœ¨æ­¤å‰æä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œä½¿ç”¨å°å‹ LM ä»ä¸€ç»„å¸¦æ³¨é‡Šçš„æ•°æ®ä¸­é€‰æ‹©æœ€ä½³æ¼”ç¤ºï¼Œç„¶åç›´æ¥å°†æ‰€é€‰æ¼”ç¤ºæ¨å¹¿åˆ°æ›´å¤§çš„ LMã€‚æˆ‘ä»¬å±•ç¤ºäº†ç›¸å¯¹äºåŸºçº¿çš„æ˜¾ç€æ”¹è¿›ï¼Œåœ¨å…«ä¸ªçœŸå®æ–‡æœ¬åˆ†ç±»æ•°æ®é›†ä¸Šå¯¹å…«ä¸ª GPT æ¨¡å‹è¿›è¡Œå¹³å‡ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„ç®—æ³•åœ¨æ•°å­¦åº”ç”¨é¢˜æ•°æ®é›† GSM8K ä¸Šçš„ç°å®å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶ç»“æœæ”¯æŒæˆ‘ä»¬çš„å‡è®¾ï¼Œå³æ³•å­¦ç¡•å£«éšå¼æ¨æ–­å‡ºåŒ…å«ä»»åŠ¡ä¿¡æ¯çš„æ½œåœ¨å˜é‡ã€‚

## Understanding Social Reasoning in Language Models with Language Models<sup>poster<sup>

Authors: Kanishk Gandhi, Jan-Philipp Franken, Tobias Gerstenberg, Noah Goodman

Link: [https://neurips.cc/virtual/2023/poster/73680](https://neurips.cc/virtual/2023/poster/73680)

Abstract:

 As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.

æ‘˜è¦:

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šèå…¥æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œäº†è§£å®ƒä»¬ç†è§£äººç±»å¿ƒç†çŠ¶æ€çš„èƒ½åŠ›å¯¹äºç¡®ä¿æœ‰æ•ˆçš„äº¤äº’å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ€è¿‘å°è¯•è¯„ä¼°æ³•å­¦ç¡•å£«çš„å¿ƒæ™ºç†è®º (ToM) æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™äº›æ¨¡å‹ä¸äººç±»å¿ƒæ™ºç†è®º (ToM) çš„ä¸€è‡´æ€§ç¨‹åº¦ä»ç„¶æ˜¯ä¸€ä¸ªå¾®å¦™çš„æ¢ç´¢è¯é¢˜ã€‚è¿™ä¸»è¦æ˜¯ç”±äºä¸¤ä¸ªä¸åŒçš„æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ä»¥å‰çš„è¯„ä¼°ç»“æœä¸ä¸€è‡´ï¼Œï¼ˆ2ï¼‰å¯¹ç°æœ‰è¯„ä¼°æ–¹æ³•æœ‰æ•ˆæ€§çš„æ‹…å¿§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å¡«å……å› æœæ¨¡æ¿æ¥ç¨‹åºåŒ–åœ°ç”Ÿæˆæ³•å­¦ç¡•å£«çš„è¯„ä¼°ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬ä¸ºæ³•å­¦ç¡•å£«åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ç¤¾ä¼šæ¨ç†åŸºå‡† (BigToM)ï¼Œå…¶ä¸­åŒ…å« 25 ä¸ªå¯¹ç…§å’Œ 5,000 ä¸ªæ¨¡å‹ç¼–å†™çš„è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œäººç±»å‚ä¸è€…å¯¹æˆ‘ä»¬åŸºå‡†çš„è´¨é‡è¯„ä»·é«˜äºä»¥å‰çš„ä¼—åŒ…è¯„ä¼°ï¼Œå¹¶ä¸”ä¸ä¸“å®¶æ’°å†™çš„è¯„ä¼°ç›¸å½“ã€‚ä½¿ç”¨ BigToMï¼Œæˆ‘ä»¬è¯„ä¼°äº†å„ç§æ³•å­¦ç¡•å£«çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›ï¼Œå¹¶å°†æ¨¡å‹è¡¨ç°ä¸äººç±»è¡¨ç°è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT4 å…·æœ‰åæ˜ äººç±»æ¨ç†æ¨¡å¼çš„ ToM åŠŸèƒ½ï¼Œå°½ç®¡ä¸å¤ªå¯é ï¼Œè€Œå…¶ä»–æ³•å­¦ç¡•å£«åˆ™è‹¦è‹¦æŒ£æ‰ã€‚

## PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance<sup>poster<sup>

Authors: Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, Jimin Huang

Link: [https://neurips.cc/virtual/2023/poster/73431](https://neurips.cc/virtual/2023/poster/73431)

Abstract:

 Although large language models (LLMs) have shown great performance in natural language processing (NLP) in the financial domain, there are no publicly available financially tailored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 128K data samples to support the fine-tuning, and an evaluation benchmark with 8 tasks and 15 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including six financial NLP tasks and two financial prediction tasks. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI.

æ‘˜è¦:

è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡‘èé¢†åŸŸçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹é¢è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰å…¬å¼€çš„é‡‘èå®šåˆ¶LLMã€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ï¼Œè¿™å¯¹äºæŒç»­æ¨åŠ¨é‡‘èé¢†åŸŸçš„LLMã€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†è‡³å…³é‡è¦ã€‚é‡‘èäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¼€æºå¼€å‘ã€‚æœ¬æ–‡ä»‹ç»äº†PIXIUï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼ŒåŒ…æ‹¬ç¬¬ä¸€ä¸ªåŸºäºæŒ‡ä»¤æ•°æ®å¾®è°ƒLLaMAçš„é‡‘èLLMã€ç¬¬ä¸€ä¸ªå…·æœ‰128Kæ•°æ®æ ·æœ¬æ”¯æŒå¾®è°ƒçš„æŒ‡ä»¤æ•°æ®ä»¥åŠå…·æœ‰8ä¸ªä»»åŠ¡å’Œ15ä¸ªæ•°æ®é›†çš„è¯„ä¼°åŸºå‡†ã€‚æˆ‘ä»¬é¦–å…ˆè€ƒè™‘å„ç§è´¢åŠ¡ä»»åŠ¡ã€è´¢åŠ¡æ–‡æ¡£ç±»å‹å’Œè´¢åŠ¡æ•°æ®æ¨¡å¼æ¥æ„å»ºå¤§è§„æ¨¡å¤šä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ„å»ºçš„æ•°æ®é›†å¯¹ LLaMA è¿›è¡Œå¾®è°ƒï¼Œæå‡ºäº†ä¸€ç§åä¸º FinMA çš„é‡‘èæ³•å­¦ç¡•å£«ï¼Œä»¥ä¾¿èƒ½å¤Ÿéµå¾ªå„ç§é‡‘èä»»åŠ¡çš„è¯´æ˜ã€‚ä¸ºäº†æ”¯æŒé‡‘èæ³•å­¦ç¡•å£«çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–åŸºå‡†ï¼Œæ¶µç›–ä¸€ç»„å…³é”®çš„é‡‘èä»»åŠ¡ï¼ŒåŒ…æ‹¬å…­ä¸ªé‡‘è NLP ä»»åŠ¡å’Œä¸¤ä¸ªé‡‘èé¢„æµ‹ä»»åŠ¡ã€‚å€ŸåŠ©è¿™ä¸€åŸºå‡†ï¼Œæˆ‘ä»¬å¯¹ FinMA å’Œå‡ ä½ç°æœ‰çš„æ³•å­¦ç¡•å£«è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œæ­ç¤ºäº†ä»–ä»¬åœ¨å¤„ç†å…³é”®è´¢åŠ¡ä»»åŠ¡æ–¹é¢çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚è¯¥æ¨¡å‹ã€æ•°æ®é›†ã€åŸºå‡†å’Œå®éªŒç»“æœå‡å¼€æºï¼Œä»¥ä¿ƒè¿›é‡‘èäººå·¥æ™ºèƒ½çš„æœªæ¥ç ”ç©¶ã€‚

## Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks<sup>poster<sup>

Authors: Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang

Link: [https://neurips.cc/virtual/2023/poster/72193](https://neurips.cc/virtual/2023/poster/72193)

Abstract:

 Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using benchmark datasets and representative attacks validates the efficacy of MDP. The code of MDP is publicly available.

æ‘˜è¦:

é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰åœ¨å°æ ·æœ¬å­¦ä¹ å™¨æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§è®¾ç½®ä¸‹å®ƒä»¬çš„å®‰å…¨é£é™©å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹è¯•ç‚¹ç ”ç©¶ï¼Œè¡¨æ˜ PLM ä½œä¸ºå°‘æ ·æœ¬å­¦ä¹ å™¨éå¸¸å®¹æ˜“å—åˆ°åé—¨æ”»å‡»ï¼Œè€Œç”±äºå°‘æ ·æœ¬åœºæ™¯çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç°æœ‰é˜²å¾¡æªæ–½ä¸è¶³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå€¡ MDPï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è½»é‡çº§ã€å¯æ’æ‹”ä¸”æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ³•ï¼Œç”¨äºå°† PLM ä½œä¸ºå°æ ·æœ¬å­¦ä¹ å™¨ã€‚å…·ä½“æ¥è¯´ï¼ŒMDPåˆ©ç”¨äº†ä¸­æ¯’æ ·æœ¬å’Œå¹²å‡€æ ·æœ¬çš„æ©è”½æ•æ„Ÿæ€§ä¹‹é—´çš„å·®è·ï¼šå‚è€ƒæœ‰é™çš„å°‘æ ·æœ¬æ•°æ®ä½œä¸ºåˆ†å¸ƒé”šï¼Œæ¯”è¾ƒç»™å®šæ ·æœ¬åœ¨ä¸åŒæ©è”½ä¸‹çš„è¡¨ç¤ºï¼Œå¹¶å°†ä¸­æ¯’æ ·æœ¬è¯†åˆ«ä¸ºå…·æœ‰æ˜¾ç€å˜åŒ–çš„æ ·æœ¬ã€‚æˆ‘ä»¬åˆ†æè¡¨æ˜ï¼ŒMDP ä¸ºæ”»å‡»è€…åœ¨æ”»å‡»æœ‰æ•ˆæ€§å’Œæ£€æµ‹è§„é¿æ€§ä¹‹é—´åšå‡ºé€‰æ‹©åˆ›é€ äº†ä¸€ä¸ªæœ‰è¶£çš„å›°å¢ƒã€‚ä½¿ç”¨åŸºå‡†æ•°æ®é›†å’Œä»£è¡¨æ€§æ”»å‡»çš„å®è¯è¯„ä¼°éªŒè¯äº† MDP çš„æœ‰æ•ˆæ€§ã€‚ MDP çš„ä»£ç æ˜¯å…¬å¼€çš„ã€‚

## How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model<sup>poster<sup>

Authors: Michael Hanna, Ollie Liu, Alexandre Variengien

Link: [https://neurips.cc/virtual/2023/poster/70432](https://neurips.cc/virtual/2023/poster/70432)

Abstract:

 Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we find related tasks that activate our circuit. Our results suggest that GPT-2 small computes greater-than using a complex but general mechanism that activates across diverse contexts.

æ‘˜è¦:

é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯èƒ½å‡ºäººæ„æ–™åœ°æ“…é•¿æ‰§è¡Œå®ƒä»¬æœªç»è¿‡æ˜ç¡®è®­ç»ƒçš„ä»»åŠ¡ï¼Œä½†äººä»¬å¯¹å®ƒä»¬å¦‚ä½•å®ç°è¿™äº›åŠŸèƒ½å´çŸ¥ä¹‹ç”šå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é€šå¸¸è·å¾—çš„åŸºæœ¬æ•°å­¦èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨æœºæ¢°å¯è§£é‡Šæ€§æŠ€æœ¯æ¥è§£é‡Š GPT-2 å°çš„ï¼ˆæœ‰é™çš„ï¼‰æ•°å­¦èƒ½åŠ›ã€‚ä½œä¸ºä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†å®ƒæ¥å—è¯¸å¦‚â€œæˆ˜äº‰ä» 1732 å¹´æŒç»­åˆ°ç¬¬ 17 å¹´â€ä¹‹ç±»çš„å¥å­çš„èƒ½åŠ›ï¼Œå¹¶é¢„æµ‹æœ‰æ•ˆçš„ä¸¤ä½æ•°ç»“æŸå¹´ä»½ï¼ˆå¹´ä»½ > 32ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆç¡®å®šä¸€ä¸ªç”µè·¯ï¼Œå®ƒæ˜¯ GPT-2 Small è®¡ç®—å›¾çš„ä¸€ä¸ªå°å­é›†ï¼Œç”¨äºè®¡ç®—è¯¥ä»»åŠ¡çš„è¾“å‡ºã€‚ç„¶åï¼Œæˆ‘ä»¬è§£é‡Šäº†æ¯ä¸ªç”µè·¯ç»„ä»¶çš„ä½œç”¨ï¼Œè¡¨æ˜ GPT-2 Small çš„æœ€ç»ˆå¤šå±‚æ„ŸçŸ¥å™¨ä½¿ç»“æŸå¹´çš„æ¦‚ç‡å¤§äºå¼€å§‹å¹´çš„æ¦‚ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬æ‰¾åˆ°æ¿€æ´»æˆ‘ä»¬ç”µè·¯çš„ç›¸å…³ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT-2 å°å‹è®¡ç®—é‡å¤§äºä½¿ç”¨è·¨ä¸åŒä¸Šä¸‹æ–‡æ¿€æ´»çš„å¤æ‚ä½†é€šç”¨çš„æœºåˆ¶ã€‚

## Language Models Meet World Models: Embodied Experiences Enhance Language Models<sup>poster<sup>

Authors: Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu

Link: [https://neurips.cc/virtual/2023/poster/71603](https://neurips.cc/virtual/2023/poster/71603)

Abstract:

 While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical elastic weight consolidation (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).

æ‘˜è¦:

è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¸¸å¸¸éš¾ä»¥åœ¨ç‰©ç†ç¯å¢ƒä¸­è¿›è¡Œç®€å•çš„æ¨ç†å’Œè§„åˆ’ï¼Œä¾‹å¦‚ç†è§£ç‰©ä½“çš„æŒä¹…æ€§æˆ–è§„åˆ’å®¶åº­æ´»åŠ¨ã€‚å…¶å±€é™æ€§åœ¨äºï¼ŒLM ä»…æ¥å—ä¹¦é¢æ–‡æœ¬åŸ¹è®­ï¼Œè€Œå¿½ç•¥äº†å¿…è¦çš„å…·ä½“çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä¸–ç•Œæ¨¡å‹å¯¹å…¶è¿›è¡Œå¾®è°ƒæ¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ–°èŒƒå¼ï¼Œä»¥è·å¾—å¤šæ ·åŒ–çš„å…·ä½“çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™å…¶é€šç”¨è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸–ç•Œæ¨¡å‹ä¸­éƒ¨ç½²ä¸€ä¸ªå…·ä½“ä»£ç†ï¼Œç‰¹åˆ«æ˜¯ç‰©ç†ä¸–ç•Œçš„æ¨¡æ‹Ÿå™¨ï¼ˆVirtualHomeï¼‰ï¼Œå¹¶é€šè¿‡ç›®æ ‡å¯¼å‘çš„è§„åˆ’å’Œéšæœºæ¢ç´¢è·å¾—ä¸€ç»„å¤šæ ·åŒ–çš„å…·ä½“ç»éªŒã€‚ç„¶åï¼Œè¿™äº›ç»éªŒè¢«ç”¨æ¥å¾®è°ƒ LMï¼Œä»¥æ•™æˆç‰©ç†ä¸–ç•Œä¸­çš„å„ç§æ¨ç†å’Œè¡ŒåŠ¨èƒ½åŠ›ï¼Œä¾‹å¦‚è§„åˆ’å’Œå®Œæˆç›®æ ‡ã€å¯¹è±¡æŒä¹…æ€§å’Œè·Ÿè¸ªç­‰ã€‚æ­¤å¤–ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æœ€å¥½ä¿ç•™ LM çš„é€šç”¨æ€§ï¼Œè¿™æœ‰åŠ©äºæ¦‚æ‹¬è·¨ä»»åŠ¡çš„å…·ä½“çŸ¥è¯†ï¼Œè€Œä¸æ˜¯ä¸ç‰¹å®šçš„æ¨¡æ‹Ÿè”ç³»åœ¨ä¸€èµ·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥ç»å…¸çš„å¼¹æ€§æƒé‡åˆå¹¶ï¼ˆEWCï¼‰æ¥è¿›è¡Œé€‰æ‹©æ€§æƒé‡æ›´æ–°ï¼Œå¹¶ç»“åˆä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰æ¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°† 18 ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€ LM å¹³å‡æ˜¾ç€æé«˜äº† 64.28%ã€‚ç‰¹åˆ«æ˜¯ï¼Œé€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºçš„å°å‹ LMï¼ˆ1.3Bã€6B å’Œ 13Bï¼‰å¯ä»¥åŒ¹é…ç”šè‡³ä¼˜äºæ›´å¤§çš„ LMï¼ˆä¾‹å¦‚ ChatGPTï¼‰ã€‚

## Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models<sup>poster<sup>

Authors: Haonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch

Link: [https://neurips.cc/virtual/2023/poster/70167](https://neurips.cc/virtual/2023/poster/70167)

Abstract:

 Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with $(\varepsilon=0.147, \delta=10^{-6})$-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial~APIs.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ä¼˜ç§€çš„ä¸Šä¸‹æ–‡å­¦ä¹ å™¨ã€‚ç„¶è€Œï¼Œæç¤ºä¸­åŒ…å«çš„æ•°æ®çš„æ•æ„Ÿæ€§å¼•èµ·äº†éšç§é—®é¢˜ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–å…ˆè¡¨æ˜è¿™äº›æ‹…å¿§æ˜¯æœ‰æ•ˆçš„ï¼šæˆ‘ä»¬é’ˆå¯¹ç”¨äºæç¤ºæ³•å­¦ç¡•å£«çš„æ•°æ®å®ä¾‹åŒ–äº†ä¸€ä¸ªç®€å•ä½†é«˜æ•ˆçš„æˆå‘˜æ¨ç†æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ¼æ´ï¼Œäººä»¬å¯ä»¥æ”¾å¼ƒæç¤ºï¼Œè½¬è€Œä½¿ç”¨å·²çŸ¥çš„ç§æœ‰æ¢¯åº¦ä¸‹é™ç®—æ³•æ¥å¾®è°ƒ LLMã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä»¥ç‰ºç‰²æç¤ºæ‰€æä¾›çš„å®ç”¨æ€§å’Œæ•ˆç‡ä¸ºä»£ä»·çš„ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å»ºè®®ç§ä¸‹å­¦ä¹ æç¤ºã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜å¯ä»¥é€šè¿‡ä¸‹æ¸¸æ•°æ®çš„æ¢¯åº¦ä¸‹é™æ¥ç§ä¸‹è·å¾—è½¯æç¤ºã€‚ç„¶è€Œï¼Œç¦»æ•£æç¤ºçš„æƒ…å†µå¹¶éå¦‚æ­¤ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ä¸€ç¾¤æå‡ºä¸åŒæç¤ºçš„æ³•å­¦ç¡•å£«ï¼ˆå³ä¸€ç¾¤éšæœºé¹¦é¹‰ï¼‰ä¸­ç²¾å¿ƒç­–åˆ’äº†ä¸€åœºå–§é—¹çš„æŠ•ç¥¨ã€‚æŠ•ç¥¨ç§ä¸‹å°†ç¾¤ä½“çš„çŸ¥è¯†è½¬åŒ–ä¸ºå•ä¸€çš„å…¬å…±æç¤ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„ç§æœ‰ç®—æ³•æç¤ºçš„æ³•å­¦ç¡•å£«ä¸éç§æœ‰åŸºçº¿éå¸¸åŒ¹é…ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ GPT3 ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ sst2 æ•°æ®é›†ä¸Šä½¿ç”¨ $(\varepsilon=0.147, \delta=10^{-6})$-å·®åˆ†éšç§å®ç°äº† 92.7% çš„ä¸‹æ¸¸å‡†ç¡®ç‡ï¼Œè€Œåœ¨éç§äººåŸºçº¿ã€‚é€šè¿‡æˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜æˆ‘ä»¬åŸºäºæç¤ºçš„æ–¹æ³•å¯ä»¥è½»æ¾åœ°ä¸ç°æœ‰çš„å•†ä¸š API ä¸€èµ·éƒ¨ç½²ã€‚

## Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset<sup>poster<sup>

Authors: Saeid Alavi Naeini, Raeid Saqur, Mozhgan Saeidi, John Giorgi, Babak Taati

Link: [https://neurips.cc/virtual/2023/poster/73547](https://neurips.cc/virtual/2023/poster/73547)

Abstract:

 The quest for human imitative AI has been an enduring topic in AI research since inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's `human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli --- distractors dubbed red herrings --- impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incorrect words to subsequent word-fragments or clues. The popular British quiz show Only Connect's Connecting Wall segment essentially mimics Mednick's Remote Associates Test (RAT) formulation with built-in, deliberate red herrings, that makes it an ideal proxy dataset to explore and study fixation effect and Einstellung paradigm from cognitive neuroscience in LLMs. In addition to presenting the novel Only Connect Wall (OCW) dataset, we also report results from our evaluation of selected pre-trained language models and LLMs (including OpenAI's GPT series) on creative problem solving tasks like grouping clue words by heterogeneous connections, and identifying correct open knowledge domain connections in respective groups. We synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to further analyze our red-herrings hypothesis in language models.The code and link to the dataset is available at url.

æ‘˜è¦:

è‡ªäººå·¥æ™ºèƒ½ç ”ç©¶è¯ç”Ÿä»¥æ¥ï¼Œå¯»æ±‚æ¨¡ä»¿äººç±»çš„äººå·¥æ™ºèƒ½ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­ç»ä¹…ä¸è¡°çš„è¯é¢˜ã€‚æœ€æ–°ä¸€æ‰¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„æŠ€æœ¯å‘å±•å’Œæ–°å…´åŠŸèƒ½ä½¿è¿™ä¸€å­¦ç§‘ä»å­¦æœ¯ç•Œèµ°å‘æ–‡åŒ–æ—¶ä»£ç²¾ç¥ã€‚è™½ç„¶æœ€è¿‘çš„ NLP è¯„ä¼°åŸºå‡†ä»»åŠ¡æµ‹è¯•äº†äººç±»æ¨¡ä»¿è¡Œä¸ºçš„æŸäº›æ–¹é¢ï¼ˆä¾‹å¦‚ï¼ŒBIG-bench çš„â€œç±»äººè¡Œä¸ºâ€ä»»åŠ¡ï¼‰ï¼Œä½†å¾ˆå°‘ï¼ˆå¦‚æœä¸æ˜¯æ²¡æœ‰çš„è¯ï¼‰æ£€æŸ¥åˆ›é€ æ€§è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚äººç±»çš„åˆ›é€ æ€§é—®é¢˜è§£å†³æ˜¯è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸­ä¸€ä¸ªç»è¿‡å……åˆ†ç ”ç©¶çš„ä¸»é¢˜ï¼Œå…¶æ ‡å‡†åŒ–æµ‹è¯•ä¸»è¦ä½¿ç”¨å…³è”çº¿ç´¢è¯ä¹‹é—´ï¼ˆå¼‚è´¨ï¼‰è”ç³»çš„èƒ½åŠ›ä½œä¸ºåˆ›é€ åŠ›çš„è¡¡é‡æ ‡å‡†ã€‚æš´éœ²äºè¯¯å¯¼æ€§åˆºæ¿€ï¼ˆè¢«ç§°ä¸ºè½¬ç§»æ³¨æ„åŠ›çš„å¹²æ‰°ç‰©ï¼‰ä¼šé€šè¿‡å›ºç€æ•ˆåº”å’Œå›ºå®šèŒƒå¼é˜»ç¢äººç±»åœ¨æ­¤ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚åœ¨è®¤çŸ¥ç¥ç»ç§‘å­¦ç ”ç©¶ä¸­ï¼Œè¿™ç§å›ºå®šæ˜¯é€šè¿‡é¢„å…ˆè®©å‚ä¸è€…æ¥è§¦ä¸åç»­å•è¯ç‰‡æ®µæˆ–çº¿ç´¢æ‹¼å†™ç›¸ä¼¼çš„ä¸æ­£ç¡®å•è¯æ¥å®éªŒè¯±å¯¼çš„ã€‚è‹±å›½æµè¡Œçš„é—®ç­”èŠ‚ç›® Only Connect çš„ Connecting Wall éƒ¨åˆ†æœ¬è´¨ä¸Šæ¨¡ä»¿äº† Mednick çš„è¿œç¨‹å…³è”æµ‹è¯• (RAT) å…¬å¼ï¼Œå¹¶å†…ç½®äº†æ•…æ„çš„è½¬ç§»æ³¨æ„åŠ›çš„å†…å®¹ï¼Œè¿™ä½¿å…¶æˆä¸ºæ¢ç´¢å’Œç ”ç©¶æ³•å­¦ç¡•å£«è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸­çš„æ³¨è§†æ•ˆåº”å’Œ Einstellung èŒƒå¼çš„ç†æƒ³ä»£ç†æ•°æ®é›†ã€‚é™¤äº†å±•ç¤ºæ–°é¢–çš„ Only Connect Wall (OCW) æ•°æ®é›†å¤–ï¼Œæˆ‘ä»¬è¿˜æŠ¥å‘Šäº†å¯¹é€‰å®šçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œ LLMï¼ˆåŒ…æ‹¬ OpenAI çš„ GPT ç³»åˆ—ï¼‰åœ¨åˆ›é€ æ€§é—®é¢˜è§£å†³ä»»åŠ¡ï¼ˆä¾‹å¦‚é€šè¿‡å¼‚æ„è¿æ¥å¯¹çº¿ç´¢è¯è¿›è¡Œåˆ†ç»„ï¼‰ä¸Šçš„è¯„ä¼°ç»“æœè¯†åˆ«å„ä¸ªç»„ä¸­æ­£ç¡®çš„å¼€æ”¾çŸ¥è¯†åŸŸè¿æ¥ã€‚æˆ‘ä»¬ç»¼åˆç”Ÿæˆäº†ä¸¤ä¸ªé¢å¤–çš„æ•°æ®é›†ï¼šOCW-Randomizedã€OCW-WordNetï¼Œä»¥è¿›ä¸€æ­¥åˆ†æè¯­è¨€æ¨¡å‹ä¸­çš„è½¬ç§»æ³¨æ„åŠ›çš„å‡è®¾ã€‚æ•°æ®é›†çš„ä»£ç å’Œé“¾æ¥å¯åœ¨ url ä¸­æ‰¾åˆ°ã€‚

## SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models<sup>poster<sup>

Authors: XIAOSONG MA, Jie ZHANG, Song Guo, Wenchao Xu

Link: [https://neurips.cc/virtual/2023/poster/72303](https://neurips.cc/virtual/2023/poster/72303)

Abstract:

 Test-time adaptation (TTA) is a special and practical setting in unsupervised domain adaptation, which allows a pre-trained model in a source domain to adapt to unlabeled test data in another target domain. To avoid the computation-intensive backbone fine-tuning process, the zero-shot generalization potentials of the emerging pre-trained vision-language models (e.g., CLIP, CoOp) are leveraged to only tune the run-time prompt for unseen test domains. However, existing solutions have yet to fully exploit the representation capabilities of pre-trained models as they only focus on the entropy-based optimization and the performance is far below the supervised prompt adaptation methods, e.g., CoOp. In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning. Specifically, we use the online prompt together with an augmented view of the input image to predict the class assignment generated by the target prompt together with an alternative augmented view of the same image. The proposed SwapPrompt can be easily deployed on vision-language models without additional requirement, and experimental results show that it achieves state-of-the-art test-time adaptation performance on ImageNet and nine other datasets. It is also shown that SwapPrompt can even achieve comparable performance with supervised prompt adaptation methods.

æ‘˜è¦:

æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ˜¯æ— ç›‘ç£åŸŸé€‚åº”ä¸­çš„ä¸€ç§ç‰¹æ®Šä¸”å®ç”¨çš„è®¾ç½®ï¼Œå®ƒå…è®¸æºåŸŸä¸­çš„é¢„è®­ç»ƒæ¨¡å‹é€‚åº”å¦ä¸€ä¸ªç›®æ ‡åŸŸä¸­çš„æœªæ ‡è®°æµ‹è¯•æ•°æ®ã€‚ä¸ºäº†é¿å…è®¡ç®—å¯†é›†å‹ä¸»å¹²å¾®è°ƒè¿‡ç¨‹ï¼Œåˆ©ç”¨æ–°å…´é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ CLIPã€CoOpï¼‰çš„é›¶æ ·æœ¬æ³›åŒ–æ½œåŠ›ï¼Œä»…è°ƒæ•´æœªè§è¿‡çš„æµ‹è¯•åŸŸçš„è¿è¡Œæ—¶æç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆå°šæœªå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå› ä¸ºå®ƒä»¬åªå…³æ³¨åŸºäºç†µçš„ä¼˜åŒ–ï¼Œå¹¶ä¸”æ€§èƒ½è¿œè¿œä½äºæœ‰ç›‘ç£çš„å³æ—¶é€‚åº”æ–¹æ³•ï¼Œä¾‹å¦‚CoOpã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† SwapPromptï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥ä¿ƒè¿›æµ‹è¯•æ—¶æç¤ºçš„é€‚åº”ã€‚ SwapPrompté‡‡ç”¨åŒé‡æç¤ºèŒƒä¾‹ï¼Œå³åœ¨çº¿æç¤ºå’Œå¯¹åœ¨çº¿æç¤ºè¿›è¡Œå¹³å‡ä»¥ä¿ç•™å†å²ä¿¡æ¯çš„ç›®æ ‡æç¤ºã€‚æ­¤å¤–ï¼ŒSwapPromptåº”ç”¨äº†äº¤æ¢é¢„æµ‹æœºåˆ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºåœ¨çº¿æç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åœ¨çº¿æç¤ºå’Œè¾“å…¥å›¾åƒçš„å¢å¼ºè§†å›¾æ¥é¢„æµ‹ç›®æ ‡æç¤ºå’ŒåŒä¸€å›¾åƒçš„æ›¿ä»£å¢å¼ºè§†å›¾ç”Ÿæˆçš„ç±»åˆ†é…ã€‚æ‰€æå‡ºçš„ SwapPrompt å¯ä»¥è½»æ¾éƒ¨ç½²åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šï¼Œæ— éœ€é¢å¤–è¦æ±‚ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨ ImageNet å’Œå…¶ä»–ä¹ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æµ‹è¯•æ—¶é€‚åº”æ€§èƒ½ã€‚å®ƒè¿˜è¡¨æ˜ï¼ŒSwapPrompt ç”šè‡³å¯ä»¥è¾¾åˆ°ä¸ç›‘ç£æç¤ºé€‚åº”æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚

## Fine-Grained Human Feedback Gives Better Rewards for Language Model Training<sup>poster<sup>

Authors: Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj (Raj) Ammanabrolu, Noah Smith, Mari Ostendorf, Hannaneh Hajishirzi

Link: [https://neurips.cc/virtual/2023/poster/72428](https://neurips.cc/virtual/2023/poster/72428)

Abstract:

 Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF)---where human preference judgments on LM outputs are transformed into a learning signal---has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with this reward function leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.

æ‘˜è¦:

è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ç»å¸¸è¡¨ç°å‡ºä¸è‰¯çš„æ–‡æœ¬ç”Ÿæˆè¡Œä¸ºï¼ŒåŒ…æ‹¬ç”Ÿæˆé”™è¯¯çš„ã€æœ‰æ¯’çš„æˆ–ä¸ç›¸å…³çš„è¾“å‡ºã€‚æ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰â€”â€”äººç±»å¯¹ LM è¾“å‡ºçš„åå¥½åˆ¤æ–­è¢«è½¬åŒ–ä¸ºå­¦ä¹ ä¿¡å·â€”â€”æœ€è¿‘åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºäº†å¸Œæœ›ã€‚ç„¶è€Œï¼Œè¿™ç§æ•´ä½“åé¦ˆä¼ è¾¾çš„é•¿æ–‡æœ¬è¾“å‡ºä¿¡æ¯æœ‰é™ï¼›å®ƒæ²¡æœ‰è¡¨æ˜è¾“å‡ºçš„å“ªäº›æ–¹é¢å½±å“äº†ç”¨æˆ·çš„åå¥½ï¼›ä¾‹å¦‚ï¼Œå“ªäº›éƒ¨åˆ†åŒ…å«å“ªäº›ç±»å‹çš„é”™è¯¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»†ç²’åº¦çš„äººç±»åé¦ˆï¼ˆä¾‹å¦‚ï¼Œå“ªä¸ªå¥å­æ˜¯é”™è¯¯çš„ï¼Œå“ªä¸ªå­å¥å­ä¸ç›¸å…³ï¼‰ä½œä¸ºæ˜¾å¼è®­ç»ƒä¿¡å·ã€‚æˆ‘ä»¬å¼•å…¥äº† Fine-Grained RLHFï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå¯ä»¥ä»ä¸¤ä¸ªæ–¹é¢ç»†ç²’åº¦çš„å¥–åŠ±å‡½æ•°ä¸­è¿›è¡Œè®­ç»ƒå’Œå­¦ä¹ ï¼šï¼ˆ1ï¼‰å¯†åº¦ï¼Œåœ¨ç”Ÿæˆæ¯ä¸ªç‰‡æ®µï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªå¥å­ï¼‰åæä¾›å¥–åŠ±ï¼› (2) åˆå¹¶ä¸ä¸åŒåé¦ˆç±»å‹ï¼ˆä¾‹å¦‚ï¼Œäº‹å®ä¸æ­£ç¡®ã€ä¸ç›¸å…³å’Œä¿¡æ¯ä¸å®Œæ•´ï¼‰ç›¸å…³çš„å¤šç§å¥–åŠ±æ¨¡å‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†æ’æ¯’å’Œé•¿ç¯‡é—®ç­”å®éªŒï¼Œä»¥è¯´æ˜åœ¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°çš„æ”¯æŒä¸‹ï¼Œåˆ©ç”¨è¿™ç§å¥–åŠ±å‡½æ•°è¿›è¡Œå­¦ä¹ å¦‚ä½•æé«˜è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜å¯ä»¥ä½¿ç”¨ç»†ç²’åº¦å¥–åŠ±æ¨¡å‹çš„ä¸åŒç»„åˆæ¥å®šåˆ¶ LM è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨ https://FineGrainedRLHF.github.io ä¸Šå‘å¸ƒäº†æ‰€æœ‰æ•°æ®ã€æ”¶é›†çš„äººç±»åé¦ˆå’Œä»£ç ã€‚

## SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models<sup>poster<sup>

Authors: Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, ZHAO-XIANG ZHANG

Link: [https://neurips.cc/virtual/2023/poster/70193](https://neurips.cc/virtual/2023/poster/70193)

Abstract:

 Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page: https://sheetcopilot.github.io/.

æ‘˜è¦:

è®¡ç®—æœºæœ€ç»ˆç”¨æˆ·èŠ±è´¹äº†æ•°åäº¿å°æ—¶æ¥å®Œæˆè¡¨æ ¼æ•°æ®å¤„ç†å’Œé¡¹ç›®æ—¶é—´å®‰æ’ç­‰æ—¥å¸¸ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡å¤§å¤šæ•°éƒ½æ˜¯é‡å¤æ€§çš„ä¸”å®¹æ˜“å‡ºé”™ï¼Œä½†å¤§å¤šæ•°æœ€ç»ˆç”¨æˆ·ç¼ºä¹è‡ªåŠ¨åŒ–è¿™äº›ç¹é‡å·¥ä½œçš„æŠ€èƒ½ã€‚éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œç”¨è‡ªç„¶è¯­è¨€ç”¨æˆ·è¯·æ±‚æŒ‡å¯¼è½¯ä»¶æˆä¸ºä¸€ä¸ªå¯ä»¥å®ç°çš„ç›®æ ‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª SheetCopilot ä»£ç†ï¼Œå®ƒæ¥å—è‡ªç„¶è¯­è¨€ä»»åŠ¡å’Œæ§åˆ¶ç”µå­è¡¨æ ¼æ¥æ»¡è¶³è¦æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç»„åŸå­æ“ä½œä½œä¸ºç”µå­è¡¨æ ¼è½¯ä»¶åŠŸèƒ½çš„æŠ½è±¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä¸ºæ³•å­¦ç¡•å£«è®¾è®¡äº†ä¸€ä¸ªåŸºäºçŠ¶æ€æœºçš„ä»»åŠ¡è§„åˆ’æ¡†æ¶ï¼Œä»¥ä¾¿ä¸ç”µå­è¡¨æ ¼è¿›è¡Œç¨³å¥çš„äº¤äº’ã€‚æˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å« 221 ä¸ªç”µå­è¡¨æ ¼æ§åˆ¶ä»»åŠ¡çš„ä»£è¡¨æ€§æ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„è¯„ä¼°ç®¡é“ï¼Œä»¥ä¸¥æ ¼å¯¹æ³•å­¦ç¡•å£«åœ¨è½¯ä»¶æ§åˆ¶ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ SheetCopilot åœ¨å•ä»£ä¸­æ­£ç¡®å®Œæˆäº† 44.3% çš„ä»»åŠ¡ï¼Œè¿œè¿œè¶…è¿‡äº†å¼ºå¤§çš„ä»£ç ç”ŸæˆåŸºçº¿ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼šhttps://sheetcopilot.github.io/ã€‚

## Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots<sup>poster<sup>

Authors: Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen, Xia Hu

Link: [https://neurips.cc/virtual/2023/poster/72945](https://neurips.cc/virtual/2023/poster/72945)

Abstract:

 In the field of natural language processing, the prevalent approach involves fine-tuning pretrained language models (PLMs) using local samples. Recent research has exposed the susceptibility of PLMs to backdoor attacks, wherein the adversaries can embed malicious prediction behaviors by manipulating a few training samples. In this study, our objective is to develop a backdoor-resistant tuning procedure that yields a backdoor-free model, no matter whether the fine-tuning dataset contains poisoned samples. To this end, we propose and integrate an \emph{honeypot module} into the original PLM, specifically designed to absorb backdoor information exclusively. Our design is motivated by the observation that lower-layer representations in PLMs carry sufficient backdoor features while carrying minimal information about the original tasks. Consequently, we can impose penalties on the information acquired by the honeypot module to inhibit backdoor creation during the fine-tuning process of the stem network. Comprehensive experiments conducted on benchmark datasets substantiate the effectiveness and robustness of our defensive strategy. Notably, these results indicate a substantial reduction in the attack success rate ranging from 10\% to 40\% when compared to prior state-of-the-art methods.

æ‘˜è¦:

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæ™®éçš„æ–¹æ³•æ˜¯ä½¿ç”¨æœ¬åœ°æ ·æœ¬å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ã€‚æœ€è¿‘çš„ç ”ç©¶æš´éœ²äº† PLM å®¹æ˜“å—åˆ°åé—¨æ”»å‡»ï¼Œå…¶ä¸­æ”»å‡»è€…å¯ä»¥é€šè¿‡æ“çºµä¸€äº›è®­ç»ƒæ ·æœ¬æ¥åµŒå…¥æ¶æ„é¢„æµ‹è¡Œä¸ºã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§æŠ—åé—¨çš„è°ƒæ•´ç¨‹åºï¼Œæ— è®ºå¾®è°ƒæ•°æ®é›†æ˜¯å¦åŒ…å«ä¸­æ¯’æ ·æœ¬ï¼Œéƒ½å¯ä»¥ç”Ÿæˆæ— åé—¨æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå¹¶å°†\emph{honeypot module}é›†æˆåˆ°åŸå§‹PLMä¸­ï¼Œä¸“é—¨ç”¨äºå¸æ”¶åé—¨ä¿¡æ¯ã€‚æˆ‘ä»¬çš„è®¾è®¡åŠ¨æœºæ˜¯è§‚å¯Ÿåˆ° PLM ä¸­çš„è¾ƒä½å±‚è¡¨ç¤ºå…·æœ‰è¶³å¤Ÿçš„åé—¨åŠŸèƒ½ï¼ŒåŒæ—¶æºå¸¦æœ‰å…³åŸå§‹ä»»åŠ¡çš„æœ€å°‘ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹èœœç½æ¨¡å—è·å–çš„ä¿¡æ¯è¿›è¡Œæƒ©ç½šï¼Œä»¥æŠ‘åˆ¶ä¸»å¹²ç½‘ç»œå¾®è°ƒè¿‡ç¨‹ä¸­åé—¨çš„åˆ›å»ºã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¯å®äº†æˆ‘ä»¬é˜²å¾¡ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ”»å‡»æˆåŠŸç‡å¤§å¹…é™ä½äº† 10% åˆ° 40%ã€‚

## Are Emergent Abilities of Large Language Models a Mirage?<sup>oral<sup>

Authors: Rylan Schaeffer, Brando Miranda, Sanmi Koyejo

Link: [https://neurips.cc/virtual/2023/poster/72117](https://neurips.cc/virtual/2023/poster/72117)

Abstract:

 Recent work claims that large language models display \textit{emergent abilities}, abilities not present in smaller-scale models that are present in larger-scale models.What makes emergent abilities intriguing is two-fold: their \textit{sharpness}, transitioning seemingly instantaneously from not present to present, and their \textit{unpredictability}, appearing at seemingly unforeseeable model scales.Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcherâ€™s choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance.We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks.Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.

æ‘˜è¦:

æœ€è¿‘çš„å·¥ä½œå£°ç§°å¤§å‹è¯­è¨€æ¨¡å‹æ˜¾ç¤ºå‡º \textit{emergent èƒ½åŠ›}ï¼Œè¿™ç§èƒ½åŠ›åœ¨è¾ƒå°è§„æ¨¡çš„æ¨¡å‹ä¸­ä¸å­˜åœ¨ï¼Œä½†åœ¨è¾ƒå¤§è§„æ¨¡çš„æ¨¡å‹ä¸­å´å­˜åœ¨ã€‚ä½¿æ¶Œç°èƒ½åŠ›æœ‰è¶£çš„åŸå› æœ‰ä¸¤ä¸ªï¼šå®ƒä»¬çš„ \textit{sharpness} ï¼Œä¼¼ä¹ç¬é—´ä»ä¸å­˜åœ¨è¿‡æ¸¡åˆ°å­˜åœ¨ï¼Œä»¥åŠå®ƒä»¬çš„ \textit{ä¸å¯é¢„æµ‹æ€§}ï¼Œä»¥çœ‹ä¼¼ä¸å¯é¢„è§çš„æ¨¡å‹è§„æ¨¡å‡ºç°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹æ¶Œç°èƒ½åŠ›æå‡ºäº†å¦ä¸€ç§è§£é‡Šï¼šå¯¹äºç‰¹å®šä»»åŠ¡å’Œæ¨¡å‹æ—ï¼Œåœ¨åˆ†æå›ºå®šæ¨¡å‹è¾“å‡ºæ—¶ï¼Œæ¶Œç°èƒ½åŠ›çš„å‡ºç°æ˜¯ç”±äºç ”ç©¶äººå‘˜å¯¹åº¦é‡çš„é€‰æ‹©ï¼Œè€Œä¸æ˜¯ç”±äºæ¨¡å‹è¡Œä¸ºéšè§„æ¨¡çš„æ ¹æœ¬å˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œéçº¿æ€§æˆ–ä¸è¿ç»­æŒ‡æ ‡ä¼šäº§ç”Ÿæ˜æ˜¾çš„æ¶Œç°èƒ½åŠ›ï¼Œè€Œçº¿æ€§æˆ–è¿ç»­æŒ‡æ ‡ä¼šåœ¨æ¨¡å‹æ€§èƒ½ä¸­äº§ç”Ÿå¹³æ»‘ã€è¿ç»­ã€å¯é¢„æµ‹çš„å˜åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªç®€å•çš„æ•°å­¦æ¨¡å‹ä¸­æå‡ºæˆ‘ä»¬çš„æ›¿ä»£è§£é‡Šï¼Œç„¶åä»¥ä¸‰ç§äº’è¡¥çš„æ–¹å¼å¯¹å…¶è¿›è¡Œæµ‹è¯•ï¼šæˆ‘ä»¬ï¼ˆ1ï¼‰ä½¿ç”¨ InstructGPT/GPT-3 ç³»åˆ—å¯¹å…·æœ‰å£°ç§°çš„æ–°å…´èƒ½åŠ›çš„ä»»åŠ¡åšå‡ºã€æµ‹è¯•å’Œç¡®è®¤å…³äºåº¦é‡é€‰æ‹©å½±å“çš„ä¸‰ä¸ªé¢„æµ‹ï¼Œ(2) åœ¨å¯¹æ–°å…´èƒ½åŠ›çš„èŸèƒåˆ†æä¸­åšå‡ºã€æµ‹è¯•å’Œç¡®è®¤å…³äºåº¦é‡é€‰æ‹©çš„ä¸¤ä¸ªé¢„æµ‹å¤§é•¿å‡³ï¼› ï¼ˆ3ï¼‰å±•ç¤ºå¦‚ä½•é€‰æ‹©æŒ‡æ ‡æ¥åœ¨ä¸åŒæ·±åº¦ç½‘ç»œçš„å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­äº§ç”Ÿå‰æ‰€æœªè§çš„çœ‹ä¼¼çªç°çš„èƒ½åŠ›ã€‚é€šè¿‡æ‰€æœ‰ä¸‰é¡¹åˆ†æï¼Œæˆ‘ä»¬æä¾›çš„è¯æ®è¡¨æ˜ï¼Œæ‰€è°“çš„çªç°èƒ½åŠ›ä¼šéšç€ä¸åŒçš„æŒ‡æ ‡æˆ–æ›´å¥½çš„ç»Ÿè®¡æ•°æ®è€Œæ¶ˆå¤±ï¼Œå¹¶ä¸”å¯èƒ½ä¸æ˜¯æ‰©å±•äººå·¥æ™ºèƒ½æ¨¡å‹çš„åŸºæœ¬å±æ€§ã€‚

## On the Planning Abilities of Large Language Models - A Critical Investigation<sup>poster<sup>

Authors: Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati

Link: [https://neurips.cc/virtual/2023/poster/71377](https://neurips.cc/virtual/2023/poster/71377)

Abstract:

 Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMsâ€™ ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.

æ‘˜è¦:

å‡ºäºå¯¹åœ¨é€šç”¨ç½‘ç»œè¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ³•å­¦ç¡•å£«çš„ç´§æ€¥æ¨ç†èƒ½åŠ›çš„è¯´æ³•çš„å…´è¶£ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç€æ‰‹ç ”ç©¶ä»–ä»¬çš„è§„åˆ’èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°ï¼ˆ1ï¼‰LLM åœ¨å¸¸è¯†æ€§è§„åˆ’ä»»åŠ¡ä¸­è‡ªä¸»ç”Ÿæˆè®¡åˆ’çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠï¼ˆ2ï¼‰LLM ä½œä¸ºå…¶ä»–æ™ºèƒ½ä½“ï¼ˆAI è§„åˆ’è€…ï¼‰åœ¨è§„åˆ’ä»»åŠ¡ä¸­å¯å‘å¼æŒ‡å¯¼æ¥æºçš„æ½œåŠ›ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç±»ä¼¼äºå›½é™…è§„åˆ’ç«èµ›ä¸­ä½¿ç”¨çš„é¢†åŸŸç”Ÿæˆä¸€ç»„å®ä¾‹æ¥è¿›è¡Œç³»ç»Ÿç ”ç©¶ï¼Œå¹¶ä»¥ä¸¤ç§ä¸åŒçš„æ¨¡å¼è¯„ä¼°æ³•å­¦ç¡•å£«ï¼šè‡ªä¸»æ¨¡å¼å’Œå¯å‘å¼æ¨¡å¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ³•å­¦ç¡•å£«è‡ªä¸»ç”Ÿæˆå¯æ‰§è¡Œè®¡åˆ’çš„èƒ½åŠ›ç›¸å½“æœ‰é™ï¼Œæœ€ä½³æ¨¡å‹ (GPT-4) åœ¨å„ä¸ªé¢†åŸŸçš„å¹³å‡æˆåŠŸç‡çº¦ä¸º 12%ã€‚ç„¶è€Œï¼Œå¯å‘å¼æ¨¡å¼çš„ç»“æœæ˜¾ç¤ºå‡ºæ›´å¤šçš„å¸Œæœ›ã€‚åœ¨å¯å‘å¼æ¨¡å¼ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº† LLM ç”Ÿæˆçš„è®¡åˆ’å¯ä»¥æ”¹è¿›åº•å±‚åˆç†è§„åˆ’å™¨çš„æœç´¢è¿‡ç¨‹ï¼Œå¹¶ä¸”è¿˜è¡¨æ˜å¤–éƒ¨éªŒè¯è€…å¯ä»¥å¸®åŠ©æä¾›å¯¹ç”Ÿæˆçš„è®¡åˆ’çš„åé¦ˆï¼Œå¹¶åå‘æç¤º LLM ä»¥æ›´å¥½åœ°ç”Ÿæˆè®¡åˆ’ã€‚

## DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models<sup>poster<sup>

Authors: Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang

Link: [https://neurips.cc/virtual/2023/poster/70625](https://neurips.cc/virtual/2023/poster/70625)

Abstract:

 A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: â€œkeeping critical thinkingâ€ and â€œletting everyone do their jobsâ€ in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.

æ‘˜è¦:

äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¸€ä¸ªé•¿æœŸç›®æ ‡æ˜¯åƒäººç±»ä¸€æ ·æ‰§è¡Œå¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»…é€šè¿‡åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥æ¨¡ä»¿äººç±»æ€ç»´ï¼Œå°±åœ¨è¯­è¨€æ¨¡æ€çš„å¤šæ­¥æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾ç€çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›æ­¥è½¬ç§»åˆ°å¤šæ¨¡æ€ç¯å¢ƒä¸­å¸¦æ¥äº†æ›´å¤§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¯¹åŠ³åŠ¨å¯†é›†å‹æ³¨é‡Šçš„ä¸åˆ‡å®é™…çš„éœ€æ±‚ä»¥åŠçµæ´»æ€§ã€æ™®éæ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„é™åˆ¶ã€‚ä¸ºäº†å”¤èµ·å¤šæ¨¡æ€ä¸­çš„ CoT æ¨ç†ï¼Œè¿™é¡¹å·¥ä½œé¦–å…ˆå¯¹å¤šæ¨¡æ€å¸¦æ¥çš„è¿™äº›æŒ‘æˆ˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶æå‡ºäº†ä¸¤ä¸ªå…³é”®è§è§£ï¼šå¤šæ¨¡æ€ CoT æ¨ç†ä¸­çš„â€œä¿æŒæ‰¹åˆ¤æ€§æ€ç»´â€å’Œâ€œè®©æ¯ä¸ªäººå„å¸å…¶èŒâ€ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„DDCoTæç¤ºï¼Œé€šè¿‡è´Ÿç©ºé—´æç¤ºä¿æŒæ‰¹åˆ¤æ€åº¦ï¼Œå¹¶å°†å¤šæ¨¡æ€èå…¥æ¨ç†ï¼Œé¦–å…ˆå°†æ³•å­¦ç¡•å£«çš„æ¨ç†è´£ä»»åˆ’åˆ†ä¸ºæ¨ç†å’Œè¯†åˆ«ï¼Œç„¶åå°†è§†è§‰æ¨¡å‹çš„è§†è§‰è¯†åˆ«èƒ½åŠ›èå…¥è”åˆæ¨ç†ã€‚è¿‡ç¨‹ã€‚ DDCoT ç”Ÿæˆçš„åŸºæœ¬åŸç†ä¸ä»…æé«˜äº†å¤§å°è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æç¤ºå’Œå¾®è°ƒå­¦ä¹ ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾ç€ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè€Œä¸”è¿˜è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¦‚æ‹¬æ€§å’Œå¯è§£é‡Šæ€§ã€‚

## Thrust: Adaptively Propels Large Language Models with External Knowledge<sup>poster<sup>

Authors: Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Jianshu Chen

Link: [https://neurips.cc/virtual/2023/poster/70026](https://neurips.cc/virtual/2023/poster/70026)

Abstract:

 Although large-scale pre-trained language models (PTLMs) are shown to encode rich knowledge in their model parameters, the inherent knowledge in PTLMs can be opaque or static, making external knowledge necessary. However, the existing information retrieval techniques could be costly and may even introduce noisy and sometimes misleading knowledge. To address these challenges, we propose the instance-level adaptive propulsion of external knowledge (IAPEK), where we only conduct the retrieval when necessary. To achieve this goal, we propose to model whether a PTLM contains enough knowledge to solve an instance with a novel metric, Thrust, which leverages the representation distribution of a small amount of seen instances. Extensive experiments demonstrate that Thrust is a good measurement of models' instance-level knowledgeability. Moreover, we can achieve higher cost-efficiency with the Thrust score as the retrieval indicator than the naive usage of external knowledge on 88% of the evaluated tasks with 26% average performance improvement. Such findings shed light on the real-world practice of knowledge-enhanced LMs with a limited budget for knowledge seeking due to computation latency or costs.

æ‘˜è¦:

è™½ç„¶å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPTLMï¼‰è¢«è¯æ˜å¯ä»¥åœ¨å…¶æ¨¡å‹å‚æ•°ä¸­ç¼–ç ä¸°å¯Œçš„çŸ¥è¯†ï¼Œä½† PTLM ä¸­çš„å›ºæœ‰çŸ¥è¯†å¯èƒ½æ˜¯ä¸é€æ˜çš„æˆ–é™æ€çš„ï¼Œä½¿å¾—å¤–éƒ¨çŸ¥è¯†å˜å¾—å¿…è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¿¡æ¯æ£€ç´¢æŠ€æœ¯å¯èƒ½æˆæœ¬é«˜æ˜‚ï¼Œç”šè‡³å¯èƒ½å¼•å…¥å˜ˆæ‚çš„ã€æœ‰æ—¶ç”šè‡³æ˜¯è¯¯å¯¼æ€§çš„çŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤–éƒ¨çŸ¥è¯†çš„å®ä¾‹çº§è‡ªé€‚åº”æ¨è¿›ï¼ˆIAPEKï¼‰ï¼Œæˆ‘ä»¬ä»…åœ¨å¿…è¦æ—¶è¿›è¡Œæ£€ç´¢ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å»ºè®®å¯¹ PTLM æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„çŸ¥è¯†æ¥ä½¿ç”¨æ–°é¢–çš„åº¦é‡ Thrust æ¥è§£å†³å®ä¾‹è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥åº¦é‡åˆ©ç”¨äº†å°‘é‡å·²è§å®ä¾‹çš„è¡¨ç¤ºåˆ†å¸ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒThrust å¯ä»¥å¾ˆå¥½åœ°è¡¡é‡æ¨¡å‹çš„å®ä¾‹çº§çŸ¥è¯†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨ 88% çš„è¯„ä¼°ä»»åŠ¡ä¸Šï¼Œä»¥ Thrust åˆ†æ•°ä½œä¸ºæ£€ç´¢æŒ‡æ ‡ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°æ¯”å•çº¯ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†æ›´é«˜çš„æˆæœ¬æ•ˆç‡ï¼Œå¹³å‡æ€§èƒ½æé«˜ 26%ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†çŸ¥è¯†å¢å¼ºå‹è¯­è¨€æ¨¡å‹çš„ç°å®ä¸–ç•Œå®è·µï¼Œç”±äºè®¡ç®—å»¶è¿Ÿæˆ–æˆæœ¬ï¼ŒçŸ¥è¯†æœç´¢çš„é¢„ç®—æœ‰é™ã€‚

## Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting<sup>poster<sup>

Authors: Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman

Link: [https://neurips.cc/virtual/2023/poster/71118](https://neurips.cc/virtual/2023/poster/71118)

Abstract:

 Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputsâ€”e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always "(A)"â€”which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡åœ¨ç»™å‡ºæœ€ç»ˆè¾“å‡ºä¹‹å‰è¿›è¡Œé€æ­¥æ¨ç†ï¼ˆé€šå¸¸ç§°ä¸ºæ€æƒ³é“¾æ¨ç†ï¼ˆCoTï¼‰ï¼‰æ¥åœ¨è®¸å¤šä»»åŠ¡ä¸Šå–å¾—å‡ºè‰²çš„æ€§èƒ½ã€‚äººä»¬å¾ˆå®¹æ˜“å°†è¿™äº› CoT è§£é‡Šè§£é‡Šä¸ºæ³•å­¦ç¡•å£«è§£å†³ä»»åŠ¡çš„è¿‡ç¨‹ã€‚æ³•å­¦ç¡•å£«é¢„æµ‹çš„è¿™ç§é€æ˜åº¦å°†å¸¦æ¥æ˜¾ç€çš„å®‰å…¨æ•ˆç›Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç° CoT è§£é‡Šå¯èƒ½ä¼šç³»ç»Ÿåœ°æ­ªæ›²æ¨¡å‹é¢„æµ‹çš„çœŸå®åŸå› ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨æ¨¡å‹è¾“å…¥ä¸­æ·»åŠ åå·®ç‰¹å¾ä¼šä¸¥é‡å½±å“ CoT è§£é‡Šâ€”â€”ä¾‹å¦‚ï¼Œé€šè¿‡åœ¨å‡ æ¬¡æç¤ºä¸­é‡æ–°æ’åºå¤šé¡¹é€‰æ‹©é€‰é¡¹ï¼Œä½¿ç­”æ¡ˆå§‹ç»ˆä¸ºâ€œ(A)â€â€”â€”æ¨¡å‹ç³»ç»Ÿåœ°åœ¨ä»–ä»¬çš„è§£é‡Šã€‚å½“æˆ‘ä»¬å°†æ¨¡å‹åå‘äºé”™è¯¯ç­”æ¡ˆæ—¶ï¼Œå®ƒä»¬ç»å¸¸ä¼šç”Ÿæˆ CoT è§£é‡Šï¼Œä½¿è¿™äº›ç­”æ¡ˆåˆç†åŒ–ã€‚å½“ä½¿ç”¨ OpenAI çš„ GPT-3.5 å’Œ Anthropic çš„ Claude 1.0 è¿›è¡Œæµ‹è¯•æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ BIG-Bench Hard çš„ä¸€ç»„ 13 é¡¹ä»»åŠ¡çš„å‡†ç¡®æ€§ä¸‹é™å¤šè¾¾ 36%ã€‚åœ¨ç¤¾ä¼šåè§ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹è§£é‡Šè¯æ˜ç»™å‡ºç¬¦åˆåˆ»æ¿å°è±¡çš„ç­”æ¡ˆæ˜¯åˆç†çš„ï¼Œè€Œæ²¡æœ‰æåŠè¿™äº›ç¤¾ä¼šåè§çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCoT çš„è§£é‡Šå¯èƒ½çœ‹ä¼¼åˆç†ï¼Œä½†å´å…·æœ‰è¯¯å¯¼æ€§ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æˆ‘ä»¬å¯¹æ³•å­¦ç¡•å£«çš„ä¿¡ä»»ï¼Œä½†æ— æ³•ä¿è¯å…¶å®‰å…¨ã€‚å»ºç«‹æ›´åŠ é€æ˜å’Œå¯è§£é‡Šçš„ç³»ç»Ÿéœ€è¦é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„åŠªåŠ›æ¥æé«˜ CoT çš„å¿ è¯šåº¦ï¼Œæˆ–è€…æ”¾å¼ƒ CoT è€Œè½¬è€Œé‡‡ç”¨æ›¿ä»£æ–¹æ³•ã€‚

## ZipLM: Inference-Aware Structured Pruning of Language Models<sup>poster<sup>

Authors: Eldar KurtiÄ‡, Elias Frantar, Dan Alistarh

Link: [https://neurips.cc/virtual/2023/poster/71044](https://neurips.cc/virtual/2023/poster/71044)

Abstract:

 The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and pruning techniques, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications. In particular, ZipLM outperforms all prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and TinyBERT. Moreover, it matches the performance of the heavily optimized MobileBERT model, obtained via extensive architecture search, by simply pruning the baseline BERT-large model. When compressing GPT2, ZipLM outperforms DistilGPT2 while being 60\% smaller and 30\% faster. Our code is available at: https://github.com/IST-DASLab/ZipLM.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªç ´æ€§æ€§èƒ½ä¼´éšç€å¤§é‡çš„è®¡ç®—å ç”¨å’Œé«˜æ˜‚çš„éƒ¨ç½²æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ–°å‹çš„ LLM ç»“æ„åŒ–å‹ç¼©æ–¹æ³•ï¼ˆç§°ä¸º ZipLMï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ ZipLM å®ç°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ä¸é€Ÿåº¦æ¯”ï¼ŒåŒæ—¶åœ¨ä»»ä½•ç»™å®šçš„æ¨ç†ç¯å¢ƒä¸­åŒ¹é…ä¸€ç»„æ‰€éœ€çš„ç›®æ ‡è¿è¡Œæ—¶åŠ é€Ÿæ¯”ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæ¨¡å‹ã€ä¸€ä¸ªæ•°æ®é›†ã€ä¸€ä¸ªæ¨ç†ç¯å¢ƒä»¥åŠä¸€ç»„åŠ é€Ÿç›®æ ‡ï¼ŒZipLM ä¼šè¿­ä»£åœ°è¯†åˆ«å¹¶åˆ é™¤æŸå¤±ä¸è¿è¡Œæ—¶é—´æƒè¡¡æœ€å·®çš„ç»„ä»¶ã€‚ä¸ä¸“æ³¨äºåè®­ç»ƒ/ä¸€æ¬¡æ€§æˆ–æ¸è¿›å‹ç¼©è®¾ç½®å¹¶ä¸”ä»…é’ˆå¯¹ç‰¹å®šæ¨¡å‹ç³»åˆ—ï¼ˆä¾‹å¦‚ BERTï¼ˆç¼–ç å™¨ï¼‰æˆ– GPTï¼ˆè§£ç å™¨ï¼‰ï¼‰çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒZipLM ç”Ÿæˆæœ€å…ˆè¿›çš„å‹ç¼©è·¨æ‰€æœ‰è¿™äº›è®¾ç½®çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ä¹‹å‰çš„è’¸é¦å’Œå‰ªææŠ€æœ¯ç›¸æ¯”ï¼ŒZipLM åªéœ€ä¸€å°éƒ¨åˆ†è®¡ç®—æˆæœ¬å°±èƒ½è·å¾—ä¼˜å¼‚çš„ç»“æœï¼Œä½¿å…¶æˆä¸ºç”Ÿæˆæ•´ä¸ªç³»åˆ—æ›´å°ã€æ›´å¿«ä¸”é«˜ç²¾åº¦æ¨¡å‹çš„ç»æµæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¹¶ä¿è¯æ»¡è¶³æ‰€éœ€çš„æ¨ç†è§„èŒƒã€‚ç‰¹åˆ«æ˜¯ï¼ŒZipLM ä¼˜äºæ‰€æœ‰ç°æœ‰çš„åŸºäº BERT çš„è’¸é¦å’Œå‰ªææŠ€æœ¯ï¼Œä¾‹å¦‚ CoFiã€MiniLM å’Œ TinyBERTã€‚æ­¤å¤–ï¼Œå®ƒä¸ç»è¿‡å¹¿æ³›æ¶æ„æœç´¢è€Œè·å¾—çš„é«˜åº¦ä¼˜åŒ–çš„ MobileBERT æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œåªéœ€ä¿®å‰ªåŸºçº¿ BERT-large æ¨¡å‹å³å¯ã€‚å‹ç¼© GPT2 æ—¶ï¼ŒZipLM çš„æ€§èƒ½ä¼˜äº DistilGPT2ï¼ŒåŒæ—¶ä½“ç§¯å° 60%ï¼Œé€Ÿåº¦å¿« 30%ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼šhttps://github.com/IST-DASLab/ZipLMã€‚

## Inference-Time Intervention: Eliciting Truthful Answers from a Language Model<sup>poster<sup>

Authors: Kenneth Li, Oam Patel, Fernanda ViÃ©gas, Hanspeter Pfister, Martin Wattenberg

Link: [https://neurips.cc/virtual/2023/poster/71200](https://neurips.cc/virtual/2023/poster/71200)

Abstract:

 We introduce Inference-Time Intervention (ITI), a technique designed to enhance the "truthfulness" of large language models (LLMs). ITI operates by shifting model activations during inference, following a learned set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from $32.5\%$ to $65.1\%$. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.

æ‘˜è¦:

æˆ‘ä»¬ä»‹ç»äº†æ¨ç†æ—¶é—´å¹²é¢„ï¼ˆITIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰â€œçœŸå®æ€§â€çš„æŠ€æœ¯ã€‚ ITI çš„è¿ä½œæ–¹å¼æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ”¹å˜æ¨¡å‹æ¿€æ´»ï¼Œéµå¾ªæœ‰é™æ•°é‡çš„æ³¨æ„åŠ›å¤´å­¦ä¹ åˆ°çš„ä¸€ç»„æ–¹å‘ã€‚è¿™ç§å¹²é¢„æ˜¾ç€æé«˜äº† LLaMA æ¨¡å‹åœ¨ TruthfulQA åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚åœ¨åä¸º Alpaca çš„æŒ‡ä»¤å¾®è°ƒ LLaMA ä¸Šï¼ŒITI å°†å…¶çœŸå®æ€§ä» $32.5\%$ æé«˜åˆ° $65.1\%$ã€‚æˆ‘ä»¬ç¡®å®šäº†çœŸå®æ€§å’Œä¹äºåŠ©äººä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡è°ƒæ•´å¹²é¢„å¼ºåº¦æ¥å¹³è¡¡å®ƒã€‚ ITI æ˜¯å¾®åˆ›ä¸”è®¡ç®—æˆæœ¬ä½å»‰çš„ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯çš„æ•°æ®æ•ˆç‡å¾ˆé«˜ï¼šè™½ç„¶ RLHF ç­‰æ–¹æ³•éœ€è¦å¤§é‡æ³¨é‡Šï¼Œä½† ITI ä»…ä½¿ç”¨æ•°ç™¾ä¸ªç¤ºä¾‹å³å¯æ‰¾åˆ°çœŸå®çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ³•å­¦ç¡•å£«å¯èƒ½å¯¹æŸäº›äº‹æƒ…ä¸ºçœŸçš„å¯èƒ½æ€§æœ‰ä¸€ç§å†…åœ¨çš„è¡¨å¾ï¼Œå³ä½¿ä»–ä»¬åœ¨è¡¨é¢ä¸Šäº§ç”Ÿäº†è°è¨€ã€‚

## Language Models are Weak Learners<sup>poster<sup>

Authors: Hariharan Manikandan, Yiding Jiang, J. Zico Kolter

Link: [https://neurips.cc/virtual/2023/poster/72822](https://neurips.cc/virtual/2023/poster/72822)

Abstract:

 A central notion in practical and theoretical machine learning is that of a weak learner, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin.  Such weak learners form the practical basis for canonical machine learning methods such as boosting.  In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners.  Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data.  We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification, and achieves the aim of acting as a weak learner on this task.  We incorporate these models into a boosting approach, which in many settings can leverage the knowledge within the LLM to outperform traditional tree-based boosting.  The model outperforms both few-shot learning and occasionally even more involved fine-tuning procedures, particularly for some tasks involving small numbers of data points.  The results illustrate the potential for prompt-based LLMs to function not just as few-shot learners themselves, but as components of larger machine learning models.

æ‘˜è¦:

æœºå™¨å­¦ä¹ å®è·µå’Œç†è®ºä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µæ˜¯å¼±å­¦ä¹ å™¨ï¼Œåˆ†ç±»å™¨èƒ½å¤Ÿå®ç°ä¼˜äºéšæœºçš„æ€§èƒ½ï¼ˆåœ¨ä»»ä½•ç»™å®šçš„æ•°æ®åˆ†å¸ƒä¸Šï¼‰ï¼Œå³ä½¿æ˜¯å¾ˆå°çš„å·®è·ã€‚è¿™ç§å¼±å­¦ä¹ å™¨æ„æˆäº†è¯¸å¦‚ boosting ä¹‹ç±»çš„è§„èŒƒæœºå™¨å­¦ä¹ æ–¹æ³•çš„å®è·µåŸºç¡€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜åŸºäºæç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åƒå¼±å­¦ä¹ è€…ä¸€æ ·æœ‰æ•ˆåœ°è¿è¡Œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯´æ˜äº†å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºåº”ç”¨äºè¡¨æ ¼æ•°æ®çš„å¢å¼ºç®—æ³•ä¸­çš„å¼±å­¦ä¹ å™¨ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡æä¾›ï¼ˆæ ¹æ®å…´è¶£åˆ†å¸ƒé€‚å½“é‡‡æ ·ï¼‰è¡¨æ ¼æ•°æ®æ ·æœ¬çš„æ–‡æœ¬æè¿°ï¼Œæ³•å­¦ç¡•å£«å¯ä»¥ç”Ÿæˆæ ·æœ¬æ‘˜è¦ï¼Œä½œä¸ºåˆ†ç±»æ¨¡æ¿ï¼Œå¹¶è¾¾åˆ°å……å½“å¼±å­¦ä¹ è€…çš„ç›®çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å°†è¿™äº›æ¨¡å‹èå…¥åˆ°ä¸€ç§æå‡æ–¹æ³•ä¸­ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥åˆ©ç”¨æ³•å­¦ç¡•å£«ä¸­çš„çŸ¥è¯†æ¥è¶…è¶Šä¼ ç»Ÿçš„åŸºäºæ ‘çš„æå‡ã€‚è¯¥æ¨¡å‹çš„æ€§èƒ½ä¼˜äºå°æ ·æœ¬å­¦ä¹ ï¼Œæœ‰æ—¶ç”šè‡³æ›´å¤šåœ°æ¶‰åŠå¾®è°ƒè¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¶‰åŠå°‘é‡æ•°æ®ç‚¹çš„æŸäº›ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºæç¤ºçš„æ³•å­¦ç¡•å£«ä¸ä»…å¯ä»¥ä½œä¸ºå°æ ·æœ¬å­¦ä¹ è€…æœ¬èº«ï¼Œè€Œä¸”å¯ä»¥ä½œä¸ºæ›´å¤§çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç»„æˆéƒ¨åˆ†ã€‚

## ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation<sup>poster<sup>

Authors: Chenyang Le, Yao Qian, Long Zhou, Shujie LIU, Yanmin Qian, Michael Zeng, Xuedong Huang

Link: [https://neurips.cc/virtual/2023/poster/72758](https://neurips.cc/virtual/2023/poster/72758)

Abstract:

 Joint speech-language training is challenging due to the large demand for training data and GPU consumption, as well as the modality gap between speech and language. We present ComSL, a speech-language model built atop a composite architecture of public pre-trained speech-only and language-only models and optimized data-efficiently for spoken language tasks. Particularly, we propose to incorporate cross-modality learning into transfer learning and conduct them simultaneously for downstream tasks in a multi-task learning manner. Our approach has demonstrated effectiveness in end-to-end speech-to-text translation tasks, achieving a new state-of-the-art average BLEU score of 31.5 on the multilingual speech to English text translation task for 21 languages, as measured on the public CoVoST2 evaluation set.

æ‘˜è¦:

ç”±äºå¯¹è®­ç»ƒæ•°æ®å’Œ GPU æ¶ˆè€—çš„å¤§é‡éœ€æ±‚ï¼Œä»¥åŠè¯­éŸ³å’Œè¯­è¨€ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œè”åˆè¯­éŸ³è¯­è¨€è®­ç»ƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº† ComSLï¼Œè¿™æ˜¯ä¸€ç§è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œæ„å»ºåœ¨å…¬å…±é¢„è®­ç»ƒçº¯è¯­éŸ³å’Œçº¯è¯­è¨€æ¨¡å‹çš„å¤åˆæ¶æ„ä¹‹ä¸Šï¼Œå¹¶é’ˆå¯¹å£è¯­ä»»åŠ¡é«˜æ•ˆåœ°ä¼˜åŒ–äº†æ•°æ®ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å»ºè®®å°†è·¨æ¨¡æ€å­¦ä¹ çº³å…¥è¿ç§»å­¦ä¹ ä¸­ï¼Œå¹¶ä»¥å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹å¼å¯¹ä¸‹æ¸¸ä»»åŠ¡åŒæ—¶è¿›è¡Œã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç«¯åˆ°ç«¯è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ä¸­å±•ç°äº†æœ‰æ•ˆæ€§ï¼Œåœ¨ 21 ç§è¯­è¨€çš„å¤šè¯­è¨€è¯­éŸ³åˆ°è‹±è¯­æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ä¸­å–å¾—äº† 31.5 åˆ†çš„æœ€æ–°å¹³å‡ BLEU åˆ†æ•°ï¼ˆæ ¹æ®ä»¥ä¸‹æŒ‡æ ‡è¡¡é‡ï¼‰å…¬å…± CoVoST2 è¯„ä¼°é›†ã€‚

## Toolformer: Language Models Can Teach Themselves to Use Tools<sup>oral<sup>

Authors: Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom

Link: [https://neurips.cc/virtual/2023/poster/71288](https://neurips.cc/virtual/2023/poster/71288)

Abstract:

 Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller specialized models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.

æ‘˜è¦:

è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è¡¨ç°å‡ºéå‡¡çš„èƒ½åŠ›ï¼Œå¯ä»¥ä»…é€šè¿‡å‡ ä¸ªç¤ºä¾‹æˆ–æ–‡æœ¬æŒ‡ä»¤æ¥è§£å†³æ–°ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯å¤§è§„æ¨¡ä»»åŠ¡ã€‚çŸ›ç›¾çš„æ˜¯ï¼Œå®ƒä»¬è¿˜éš¾ä»¥å®ç°åŸºæœ¬åŠŸèƒ½ï¼Œä¾‹å¦‚ç®—æœ¯æˆ–äº‹å®æŸ¥æ‰¾ï¼Œè€Œæ›´ç®€å•å’Œæ›´å°çš„ä¸“ç”¨æ¨¡å‹åœ¨è¿™äº›åŠŸèƒ½ä¸Šè¡¨ç°å‡ºè‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº† LM å¯ä»¥é€šè¿‡ç®€å•çš„ API è‡ªå­¦ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå¹¶å®ç°ä¸¤å…¨å…¶ç¾ã€‚æˆ‘ä»¬å¼•å…¥äº† Toolformerï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œç”¨äºå†³å®šè°ƒç”¨å“ªäº› APIã€ä½•æ—¶è°ƒç”¨å®ƒä»¬ã€ä¼ é€’å“ªäº›å‚æ•°ä»¥åŠå¦‚ä½•æœ€å¥½åœ°å°†ç»“æœåˆå¹¶åˆ°æœªæ¥çš„ä»¤ç‰Œé¢„æµ‹ä¸­ã€‚è¿™æ˜¯é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼å®Œæˆçš„ï¼Œåªéœ€è¦ä¸ºæ¯ä¸ª API è¿›è¡Œå°‘é‡æ¼”ç¤ºå³å¯ã€‚æˆ‘ä»¬æ•´åˆäº†ä¸€ç³»åˆ—å·¥å…·ï¼ŒåŒ…æ‹¬è®¡ç®—å™¨ã€é—®ç­”ç³»ç»Ÿã€æœç´¢å¼•æ“ã€ç¿»è¯‘ç³»ç»Ÿå’Œæ—¥å†ã€‚ Toolformer åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾ç€æé«˜äº†é›¶æ ·æœ¬æ€§èƒ½ï¼Œé€šå¸¸å¯ä»¥ä¸æ›´å¤§çš„æ¨¡å‹ç«äº‰ï¼Œè€Œæ— éœ€ç‰ºç‰²å…¶æ ¸å¿ƒè¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚

## GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph<sup>poster<sup>

Authors: Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang

Link: [https://neurips.cc/virtual/2023/poster/71275](https://neurips.cc/virtual/2023/poster/71275)

Abstract:

 Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms the previous adapter-based methods.

æ‘˜è¦:

é€‚é…å™¨å¼é«˜æ•ˆè¿ç§»å­¦ä¹ ï¼ˆETLï¼‰åœ¨ä½æ•°æ®æ¡ä»¶ä¸‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è°ƒä¼˜ä¸­è¡¨ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå…¶ä¸­ä»…å¼•å…¥ä¸€äº›é¢å¤–å‚æ•°æ¥æŒ–æ˜åŸºäºVLM çš„é€šç”¨ä¸”å¼ºå¤§çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°é€‚é…å™¨é£æ ¼çš„ä½œå“é¢ä¸´ä¸¤ä¸ªé™åˆ¶ï¼šï¼ˆiï¼‰ä»…ä½¿ç”¨å•ä¸€æ¨¡æ€å¯¹ç‰¹å®šäºä»»åŠ¡çš„çŸ¥è¯†è¿›è¡Œå»ºæ¨¡ï¼› (ii) å¿½è§†ä¸‹æ¸¸ä»»åŠ¡ä¸­ç±»é—´å…³ç³»çš„åˆ©ç”¨ï¼Œä»è€Œå¯¼è‡´æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é€‚é…å™¨å¼è°ƒæ•´ç­–ç•¥ï¼Œç§°ä¸º GraphAdapterï¼Œå®ƒé€šè¿‡ä½¿ç”¨åŒæ¨¡æ€ç»“æ„çŸ¥è¯†ï¼ˆå³æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸­ä¸åŒè¯­ä¹‰/ç±»çš„ç›¸å…³æ€§ï¼‰æ˜¾å¼å»ºæ¨¡æ¥æ‰§è¡Œæ–‡æœ¬é€‚é…å™¨ã€‚çŸ¥è¯†å›¾ã€‚å…·ä½“åœ°ï¼ŒåŒçŸ¥è¯†å›¾ç”±ä¸¤ä¸ªå­å›¾å»ºç«‹ï¼Œå³æ–‡æœ¬çŸ¥è¯†å­å›¾å’Œè§†è§‰çŸ¥è¯†å­å›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹å’Œè¾¹åˆ†åˆ«è¡¨ç¤ºä¸¤ç§æ¨¡æ€çš„è¯­ä¹‰/ç±»åŠå…¶ç›¸å…³æ€§ã€‚è¿™ä½¿å¾—æ¯ä¸ªæç¤ºçš„æ–‡æœ¬ç‰¹å¾èƒ½å¤Ÿåˆ©ç”¨æ¥è‡ªæ–‡æœ¬å’Œè§†è§‰æ¨¡å¼çš„ç‰¹å®šäºä»»åŠ¡çš„ç»“æ„çŸ¥è¯†ï¼Œä»è€Œä¸ºä¸‹æ¸¸ä»»åŠ¡äº§ç”Ÿæ›´æœ‰æ•ˆçš„åˆ†ç±»å™¨ã€‚å¯¹ 11 ä¸ªåŸºå‡†æ•°æ®é›†çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ GraphAdapter æ˜¾ç€ä¼˜äºä¹‹å‰åŸºäºé€‚é…å™¨çš„æ–¹æ³•ã€‚

## SatLM: Satisfiability-Aided Language Models Using Declarative Prompting<sup>poster<sup>

Authors: Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett

Link: [https://neurips.cc/virtual/2023/poster/71537](https://neurips.cc/virtual/2023/poster/71537)

Abstract:

 Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling (SatLM) approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it out of the description more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness of the answer with respect to the parsed specification and avoid planning errors in the solving process. We evaluate SATLM on 8 different datasets and show that it consistently outperforms program-aided LMs in the imperative paradigm. In particular, SATLM outperforms program-aided LMs by 23% on a challenging subset of the GSM arithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and BoardgameQA, surpassing previous models that are trained on the respective training sets.

æ‘˜è¦:

å…ˆå‰çš„å·¥ä½œå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾æç¤ºä¸ç¨‹åºåŒ–è¡¨ç¤ºç›¸ç»“åˆï¼Œä»¥æ‰§è¡Œæœ‰æ•ˆä¸”é€æ˜çš„æ¨ç†ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯¹äºåªéœ€è¦å‰å‘æ¨ç†ï¼ˆä¾‹å¦‚ï¼Œç®€å•ç®—æœ¯ï¼‰çš„ä»»åŠ¡æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äºéœ€è¦æ›´å¤æ‚çš„è§„åˆ’å’Œæœç´¢çš„çº¦æŸæ±‚è§£é—®é¢˜æ¥è¯´æ•ˆæœè¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯æ»¡è¶³æ€§è¾…åŠ©è¯­è¨€å»ºæ¨¡ï¼ˆSatLMï¼‰æ–¹æ³•ï¼Œç”¨äºæé«˜æ³•å­¦ç¡•å£«çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨æ³•å­¦ç¡•å£«æ¥ç”Ÿæˆå£°æ˜æ€§ä»»åŠ¡è§„èŒƒè€Œä¸æ˜¯å‘½ä»¤å¼ç¨‹åºï¼Œå¹¶åˆ©ç”¨ç°æˆçš„è‡ªåŠ¨å®šç†è¯æ˜å™¨æ¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹ã€‚å£°æ˜æ€§è§„èŒƒæ¯”æ¨ç†æ­¥éª¤æ›´æ¥è¿‘é—®é¢˜æè¿°ï¼Œå› æ­¤æ³•å­¦ç¡•å£«å¯ä»¥æ›´å‡†ç¡®åœ°ä»æè¿°ä¸­è§£æå‡ºæ¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†å®é™…æ¨ç†ä»»åŠ¡å¸è½½ç»™è‡ªåŠ¨å®šç†è¯æ˜å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¿è¯ç­”æ¡ˆç›¸å¯¹äºè§£æè§„èŒƒçš„æ­£ç¡®æ€§ï¼Œå¹¶é¿å…æ±‚è§£è¿‡ç¨‹ä¸­çš„è®¡åˆ’é”™è¯¯ã€‚æˆ‘ä»¬åœ¨ 8 ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼° SATLMï¼Œå¹¶è¡¨æ˜å®ƒåœ¨å‘½ä»¤å¼èŒƒå¼ä¸­å§‹ç»ˆä¼˜äºç¨‹åºè¾…åŠ©çš„ LMã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨ GSM ç®—æœ¯æ¨ç†æ•°æ®é›†çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†ä¸Šï¼ŒSATLM çš„æ€§èƒ½æ¯”ç¨‹åºè¾…åŠ©çš„ LM æé«˜äº† 23%ï¼› SATLM è¿˜åœ¨ LSAT å’Œ BoardgameQA ä¸Šå®ç°äº†æ–°çš„ SoTAï¼Œè¶…è¶Šäº†ä¹‹å‰åœ¨å„è‡ªè®­ç»ƒé›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚

## Passive learning of active causal strategies in agents and language models<sup>poster<sup>

Authors: Andrew Lampinen, Stephanie Chan, Ishita Dasgupta, Andrew Nam, Jane Wang

Link: [https://neurips.cc/virtual/2023/poster/72481](https://neurips.cc/virtual/2023/poster/72481)

Abstract:

 What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training.We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and have implications for understanding the behaviors and capabilities of language models.

æ‘˜è¦:

ä»è¢«åŠ¨æ•°æ®ä¸­å¯ä»¥äº†è§£å› æœå…³ç³»å’Œå®éªŒä»€ä¹ˆï¼Ÿé‰´äºæœ€è¿‘è¢«åŠ¨è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ç­‰äº¤äº’é¢†åŸŸå–å¾—çš„æˆåŠŸï¼Œè¿™ä¸ªé—®é¢˜å°±æ˜¾å¾—å°¤ä¸ºçªå‡ºã€‚è¢«åŠ¨å­¦ä¹ æœ¬è´¨ä¸Šæ˜¯æœ‰é™çš„ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œçº¯ç²¹çš„è¢«åŠ¨å­¦ä¹ å®é™…ä¸Šå¯ä»¥è®©ä»£ç†å­¦ä¹ ç”¨äºç¡®å®šå’Œä½¿ç”¨å› æœç»“æ„çš„é€šç”¨ç­–ç•¥ï¼Œåªè¦ä»£ç†å¯ä»¥åœ¨æµ‹è¯•æ—¶è¿›è¡Œå¹²é¢„ã€‚æˆ‘ä»¬æ­£å¼è¯´æ˜ï¼Œå­¦ä¹ å…ˆè¿›è¡Œå®éªŒï¼Œç„¶åå¯»æ±‚ç›®æ ‡çš„ç­–ç•¥åŸåˆ™ä¸Šå¯ä»¥ä»è¢«åŠ¨å­¦ä¹ ä¸­è¿›è¡Œæ¦‚æ‹¬ã€‚ç„¶åï¼Œæˆ‘ä»¬å‡­ç»éªŒè¯æ˜ï¼Œé€šè¿‡æ¨¡ä»¿ä¸“å®¶æ•°æ®è®­ç»ƒçš„æ™ºèƒ½ä½“ç¡®å®å¯ä»¥åœ¨æµ‹è¯•æ—¶è¿›è¡Œæ¦‚æ‹¬ï¼Œä»¥æ¨æ–­å’Œä½¿ç”¨è®­ç»ƒæ•°æ®ä¸­ä»æœªå­˜åœ¨çš„å› æœå…³ç³»ï¼›è¿™äº›ä»£ç†è¿˜å¯ä»¥å°†å®éªŒç­–ç•¥æ¨å¹¿åˆ°è®­ç»ƒä¸­ä»æœªè§‚å¯Ÿåˆ°çš„æ–°å˜é‡é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿åœ¨å…·æœ‰é«˜ç»´è§‚å¯Ÿçš„æ›´å¤æ‚çš„ç¯å¢ƒä¸­ï¼Œåœ¨è‡ªç„¶è¯­è¨€çš„æ”¯æŒä¸‹ï¼Œä¹Ÿå¯ä»¥ä»è¢«åŠ¨æ•°æ®ä¸­æ¨å¹¿å› æœå¹²é¢„å’Œåˆ©ç”¨çš„ç­–ç•¥è§£é‡Šã€‚è§£é‡Šç”šè‡³å¯ä»¥è®©è¢«åŠ¨å­¦ä¹ è€…ä»å®Œå…¨æ··æ‚çš„è®­ç»ƒæ•°æ®ä¸­æ¦‚æ‹¬å‡ºåˆ†å¸ƒä¸å‡çš„æƒ…å†µã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä»…æ¥å—è¢«åŠ¨ä¸‹ä¸€ä¸ªå•è¯é¢„æµ‹è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥ä»åŒ…å«è§£é‡Šå’Œæ¨ç†çš„å‡ æ¬¡æç¤ºä¸­æ¦‚æ‹¬å‡ºå› æœå¹²é¢„ç­–ç•¥ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ä¸»åŠ¨å› æœç­–ç•¥çš„è¢«åŠ¨å­¦ä¹ çš„æƒŠäººåŠ›é‡ï¼Œå¹¶ä¸”å¯¹äºç†è§£è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’Œèƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚

## Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering<sup>poster<sup>

Authors: Noah Hollmann, Samuel MÃ¼ller, Frank Hutter

Link: [https://neurips.cc/virtual/2023/poster/72592](https://neurips.cc/virtual/2023/poster/72592)

Abstract:

 As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems.We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features.Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets -- boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature.CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML. We release our code, a simple demo and a python package.

æ‘˜è¦:

éšç€è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹  (AutoML) é¢†åŸŸçš„è¿›æ­¥ï¼Œå°†é¢†åŸŸçŸ¥è¯†çº³å…¥è¿™äº›ç³»ç»Ÿå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„åŠ›é‡æ¥å®ç°è¿™ä¸€ç›®æ ‡çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹ï¼ˆCAAFEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è¡¨æ ¼æ•°æ®é›†çš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ LLM æ ¹æ®æ•°æ®é›†çš„æè¿°è¿­ä»£åœ°ä¸ºè¡¨æ ¼æ•°æ®é›†ç”Ÿæˆé™„åŠ çš„è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç‰¹å¾ã€‚è¯¥æ–¹æ³•ç”Ÿæˆç”¨äºåˆ›å»ºæ–°ç‰¹å¾çš„ Python ä»£ç ä»¥åŠå¯¹æ‰€ç”Ÿæˆç‰¹å¾çš„å®ç”¨æ€§çš„è§£é‡Šã€‚å°½ç®¡æ–¹æ³•ç®€å•ï¼Œä½† CAAFE æé«˜äº† 14 ä¸ªæ•°æ®é›†ä¸­çš„ 11 ä¸ªæ•°æ®é›†çš„æ€§èƒ½ - å°†æ‰€æœ‰æ•°æ®é›†çš„å¹³å‡ ROC AUC æ€§èƒ½ä» 0.798 æé«˜åˆ° 0.822 - ç±»ä¼¼é€šè¿‡åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šä½¿ç”¨éšæœºæ£®æ—è€Œä¸æ˜¯é€»è¾‘å›å½’æ‰€å®ç°çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒCAAFE å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªç”Ÿæˆçš„ç‰¹å¾æä¾›æ–‡æœ¬è§£é‡Šæ¥è¿›è¡Œè§£é‡Šã€‚CAAFE ä¸ºæ•°æ®ç§‘å­¦ä»»åŠ¡ä¸­æ›´å¹¿æ³›çš„åŠè‡ªåŠ¨åŒ–é“ºå¹³äº†é“è·¯ï¼Œå¹¶å¼ºè°ƒäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§£å†³æ–¹æ¡ˆçš„é‡è¦æ€§ï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆå¯ä»¥å°† AutoML ç³»ç»Ÿçš„èŒƒå›´æ‰©å±•åˆ°è¯­ä¹‰ AutoMLã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€ä¸€ä¸ªç®€å•çš„æ¼”ç¤ºå’Œä¸€ä¸ª python åŒ…ã€‚

## Post Hoc Explanations of Language Models Can Improve Language Models<sup>poster<sup>

Authors: Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, Himabindu Lakkaraju

Link: [https://neurips.cc/virtual/2023/poster/72907](https://neurips.cc/virtual/2023/poster/72907)

Abstract:

 Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, lead to critical insights for refining in context learning.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡ç¨‹ä¸­çº³å…¥äººå·¥æ³¨é‡Šçš„åŸºæœ¬åŸç†ï¼ˆä¾‹å¦‚ï¼Œæ€ç»´é“¾æç¤ºï¼‰å¯ä»¥æ˜¾ç€æé«˜è¿™äº›æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ä¸Šã€‚ç„¶è€Œï¼Œçº³å…¥è¿™äº›åŸºæœ¬åŸç†åœ¨å¯æ‰©å±•æ€§æ–¹é¢æå‡ºäº†æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™éœ€è¦é«˜åº¦çš„äººç±»å‚ä¸ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œäº‹åè§£é‡Šæ¥æ”¾å¤§æ¨¡å‹æ€§èƒ½ï¼ˆAMPLIFYï¼‰ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨åŒ–åŸç†ç”Ÿæˆè¿‡ç¨‹æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ©ç”¨äº‹åè§£é‡Šæ–¹æ³•è¾“å‡ºå½’å› åˆ†æ•°ï¼ˆè§£é‡Šï¼‰ï¼Œæ•è·æ¯ä¸ªè¾“å…¥ç‰¹å¾å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†è‡ªåŠ¨åŒ–çš„è‡ªç„¶è¯­è¨€åŸç†ï¼Œå…¶ä¸­åµŒå…¥äº†äº‹åè§£é‡Šçš„è§è§£ï¼Œä¸ºæ³•å­¦ç¡•å£«æä¾›çº æ­£ä¿¡å·ã€‚å¯¹ç°å®ä¸–ç•Œæ•°æ®é›†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ AMPLIFY åœ¨å„ç§ä»»åŠ¡ä¸­ä½¿é¢„æµ‹ç²¾åº¦æé«˜äº†çº¦ 10-25%ï¼ŒåŒ…æ‹¬é‚£äº›ä¾èµ–äºäººå·¥æ³¨é‡ŠåŸç†çš„å…ˆå‰æ–¹æ³•ï¼ˆä¾‹å¦‚ Chain-of-ï¼‰æ€è€ƒæç¤ºä¸è¶³ã€‚æˆ‘ä»¬çš„å·¥ä½œæ˜¯å¼ºè°ƒäº‹åè§£é‡Šä½œä¸ºæé«˜æ³•å­¦ç¡•å£«æœ‰æ•ˆæ€§çš„å®è´µå·¥å…·çš„æ½œåŠ›çš„é¦–æ¬¡å°è¯•ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†é¢å¤–çš„å®è¯åˆ†æå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥è¯æ˜ AMPLIFY æ¯ä¸ªç»„ä»¶çš„å½±å“ï¼Œè¿™åè¿‡æ¥åˆä¸ºä¸Šä¸‹æ–‡å­¦ä¹ çš„æ”¹è¿›æä¾›äº†é‡è¦çš„è§è§£ã€‚

## Textually Pretrained Speech Language Models<sup>poster<sup>

Authors: Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis CONNEAU, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, Yossi Adi

Link: [https://neurips.cc/virtual/2023/poster/71490](https://neurips.cc/virtual/2023/poster/71490)

Abstract:

 Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available.

æ‘˜è¦:

è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMï¼‰ä»…å¤„ç†å’Œç”Ÿæˆå£°å­¦æ•°æ®ï¼Œæ²¡æœ‰æ–‡æœ¬ç›‘ç£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† TWISTï¼Œä¸€ç§ä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬è¯­è¨€æ¨¡å‹çš„çƒ­å¯åŠ¨æ¥è®­ç»ƒ SpeechLM çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°è¡¨æ˜ TWIST å…¨é¢ä¼˜äºå†·å¯åŠ¨ SpeechLMã€‚æˆ‘ä»¬æ ¹æ®ç»éªŒåˆ†æäº†ä¸åŒæ¨¡å‹è®¾è®¡é€‰æ‹©ï¼ˆä¾‹å¦‚è¯­éŸ³æ ‡è®°å™¨ã€é¢„è®­ç»ƒæ–‡æœ¬æ¨¡å‹å’Œæ•°æ®é›†å¤§å°ï¼‰çš„æ•ˆæœã€‚æˆ‘ä»¬å‘ç°æ¨¡å‹å’Œæ•°æ®é›†è§„æ¨¡åœ¨æ„å»ºæ€§èƒ½æ›´å¥½çš„ SpeechLM æ–¹é¢éƒ½å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æ ¹æ®æˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ï¼ˆæ®æˆ‘ä»¬æ‰€çŸ¥ï¼‰åœ¨å‚æ•°æ•°é‡å’Œè®­ç»ƒæ•°æ®æ–¹é¢æœ€å¤§çš„ SpeechLMã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº† StoryCloze æ–‡æœ¬åŸºå‡†çš„ä¸¤ä¸ªå£è¯­ç‰ˆæœ¬ï¼Œä»¥è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹è¯„ä¼°å¹¶æ¨è¿›è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚æˆ‘ä»¬å…¬å¼€æä¾›è¯­éŸ³æ ·æœ¬ã€ä»£ç å’Œæ¨¡å‹ã€‚

## CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society<sup>poster<sup>

Authors: Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem

Link: [https://neurips.cc/virtual/2023/poster/72905](https://neurips.cc/virtual/2023/poster/72905)

Abstract:

 The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their â€œcognitiveâ€ processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing . Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.

æ‘˜è¦:

åŸºäºèŠå¤©çš„è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´å¤æ‚ä»»åŠ¡è§£å†³æ–¹é¢å–å¾—äº†æ˜¾ç€è¿›å±•ã€‚ç„¶è€Œï¼Œä»–ä»¬çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºäººç±»è¾“å…¥æ¥æŒ‡å¯¼å¯¹è¯ï¼Œè¿™å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ä¸”è€—æ—¶ã€‚æœ¬æ–‡æ¢è®¨äº†æ„å»ºå¯æ‰©å±•æŠ€æœ¯ä»¥ä¿ƒè¿›é€šä¿¡ä»£ç†ä¹‹é—´è‡ªä¸»åˆä½œçš„æ½œåŠ›ï¼Œå¹¶æ·±å…¥äº†è§£å…¶â€œè®¤çŸ¥â€è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³å®ç°è‡ªä¸»åˆä½œçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºè§’è‰²æ‰®æ¼”çš„æ–°å‹é€šä¿¡ä»£ç†æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠä½¿ç”¨åˆå§‹æç¤ºæ¥å¼•å¯¼èŠå¤©ä»£ç†å®Œæˆä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒä¸äººç±»æ„å›¾çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è§’è‰²æ‰®æ¼”æ¥ç”Ÿæˆå¯¹è¯æ•°æ®ï¼Œä»¥ç ”ç©¶ä»£ç†ç¤¾ä¼šçš„è¡Œä¸ºå’Œèƒ½åŠ›ï¼Œä¸ºç ”ç©¶å¯¹è¯è¯­è¨€æ¨¡å‹æä¾›å®è´µçš„èµ„æºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æŒ‡ä»¤éµå¾ªåˆä½œè¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬å¼•å…¥ä¸€ç§æ–°é¢–çš„é€šä¿¡ä»£ç†æ¡†æ¶ï¼Œæä¾›ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•æ¥ç ”ç©¶å¤šä»£ç†ç³»ç»Ÿçš„åˆä½œè¡Œä¸ºå’ŒåŠŸèƒ½ï¼Œä»¥åŠå¼€æºæˆ‘ä»¬çš„åº“ä»¥æ”¯æŒé€šä¿¡ä»£ç†åŠå…¶ä»–æ–¹é¢çš„ç ”ç©¶ï¼šhttps://github.com/éª†é©¼-è‰¾/éª†é©¼ã€‚

## Large Language Models Are Semi-Parametric Reinforcement Learning Agents<sup>poster<sup>

Authors: Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, Kai Yu

Link: [https://neurips.cc/virtual/2023/poster/71228](https://neurips.cc/virtual/2023/poster/71228)

Abstract:

 Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as Rememberer. By equipping the LLM with a long-term experience memory, Rememberer is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed Rememberer constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of Rememberer.

æ‘˜è¦:

å—è®¤çŸ¥ç§‘å­¦å¯¹äººç±»è®°å¿†å’Œæ¨ç†æœºåˆ¶çš„è§è§£çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹å¯è¿›åŒ–çš„åŸºäº LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰çš„ä»£ç†æ¡†æ¶ï¼šRemembererã€‚é€šè¿‡ä¸º LLM é…å¤‡é•¿æœŸç»éªŒè®°å¿†ï¼ŒRememberer èƒ½å¤Ÿåˆ©ç”¨è¿‡å»çš„ç»éªŒæ¥å®ç°ä¸åŒçš„ä»»åŠ¡ç›®æ ‡ï¼Œè¿™ä¼˜äºå…·æœ‰å›ºå®šæ ·æœ¬æˆ–é…å¤‡ç¬æ—¶å·¥ä½œè®°å¿†çš„åŸºäº LLM çš„ä»£ç†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥å¸¦æœ‰ç»éªŒè®°å¿†çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLEMï¼‰æ¥æ›´æ–°è®°å¿†ã€‚å› æ­¤ï¼Œæ•´ä¸ªç³»ç»Ÿå¯ä»¥ä»æˆåŠŸå’Œå¤±è´¥çš„ç»éªŒä¸­å­¦ä¹ ï¼Œå¹¶åœ¨ä¸å¾®è°ƒLLMå‚æ•°çš„æƒ…å†µä¸‹å‘å±•å…¶èƒ½åŠ›ã€‚è¿™æ ·ï¼Œæ‰€æå‡ºçš„ Rememberer å°±æ„æˆäº†ä¸€ä¸ªåŠå‚æ•° RL ä»£ç†ã€‚åœ¨ä¸¤ä¸ª RL ä»»åŠ¡é›†ä¸Šè¿›è¡Œäº†å¤§é‡çš„å®éªŒæ¥è¯„ä¼°æ‰€æå‡ºçš„æ¡†æ¶ã€‚ä¸åŒåˆå§‹åŒ–å’Œè®­ç»ƒé›†çš„å¹³å‡ç»“æœåœ¨ä¸¤ä¸ªä»»åŠ¡é›†ä¸Šçš„æˆåŠŸç‡åˆ†åˆ«æ¯”ä¹‹å‰çš„ SOTA æé«˜äº† 4% å’Œ 2%ï¼Œè¯æ˜äº† Rememberer çš„ä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚

## Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models<sup>poster<sup>

Authors: Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, Philip Torr, Volker Tresp

Link: [https://neurips.cc/virtual/2023/poster/73702](https://neurips.cc/virtual/2023/poster/73702)

Abstract:

 Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. As test samples in real-world applications usually differ from adaptation data, the robustness of these adaptation methods against distribution shifts are essential. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead, it results in even lower robustness. We hope this study could benefit future research in the development of robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at https://adarobustness.github.io.

æ‘˜è¦:

äººä»¬æå‡ºäº†å¤šç§é€‚åº”æ–¹æ³•ï¼Œä¾‹å¦‚ LoRAã€æç¤ºå’Œé€‚é…å™¨ï¼Œä»¥å¢å¼ºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ã€‚ç”±äºç°å®åº”ç”¨ä¸­çš„æµ‹è¯•æ ·æœ¬é€šå¸¸ä¸é€‚åº”æ•°æ®ä¸åŒï¼Œå› æ­¤è¿™äº›é€‚åº”æ–¹æ³•é’ˆå¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº† 4 ä¸ªè§†è§‰è¯­è¨€æ•°æ®é›†ä¸Š 11 ç§å¹¿æ³›ä½¿ç”¨çš„é€‚åº”æ–¹æ³•åœ¨å¤šæ¨¡æ€æŸåä¸‹çš„ç¨³å¥æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº† 7 ä¸ªåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…æ‹¬ 96 ä¸ªè§†è§‰æŸåå’Œ 87 ä¸ªæ–‡æœ¬æŸåï¼Œä»¥ç ”ç©¶ä¸åŒé€‚åº”æ–¹æ³•çš„é²æ£’æ€§ã€å¯ç”¨é€‚åº”ç¤ºä¾‹çš„å½±å“ä»¥åŠé€‚åº”è¿‡ç¨‹ä¸­å¯è®­ç»ƒå‚æ•°å¤§å°çš„å½±å“ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼š1ï¼‰é€‚åº”æ–¹æ³•å¯¹æ–‡æœ¬æŸåæ¯”è§†è§‰æŸåæ›´æ•æ„Ÿã€‚ 2) å®Œå…¨å¾®è°ƒå¹¶ä¸èƒ½å§‹ç»ˆæä¾›æœ€é«˜çš„é²æ£’æ€§ï¼›ç›¸åï¼Œé€‚é…å™¨å¯ä»¥å®ç°æ›´å¥½çš„é²æ£’æ€§å’Œç›¸å½“çš„æ¸…æ´æ€§èƒ½ã€‚ 3ï¼‰ä¸é¢„æœŸç›¸åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¢åŠ é€‚åº”æ•°æ®å’Œå‚æ•°çš„æ•°é‡å¹¶ä¸èƒ½ä¿è¯å¢å¼ºçš„ç¨³å¥æ€§ï¼›ç›¸åï¼Œå®ƒä¼šå¯¼è‡´é²æ£’æ€§æ›´ä½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½å¤Ÿæœ‰ç›Šäºæœªæ¥å¼€å‘ç¨³å¥çš„å¤šæ¨¡å¼é€‚åº”æ–¹æ³•çš„ç ”ç©¶ã€‚æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„åŸºå‡†ã€ä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨ https://adarobustness.github.io è®¿é—®ã€‚

## On Evaluating Adversarial Robustness of Large Vision-Language Models<sup>poster<sup>

Authors: Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan LI, Ngai-Man (Man) Cheung, Min Lin

Link: [https://neurips.cc/virtual/2023/poster/69997](https://neurips.cc/virtual/2023/poster/69997)

Abstract:

 Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Our project page: https://yunqing-me.github.io/AttackVLM/.

æ‘˜è¦:

GPT-4 ç­‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) åœ¨å“åº”ç”Ÿæˆæ–¹é¢å–å¾—äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰è¾“å…¥æ–¹é¢ï¼Œä¸ ChatGPT ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œèƒ½å¤Ÿå®ç°æ›´å…·åˆ›é€ æ€§å’Œé€‚åº”æ€§çš„äº¤äº’ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¤šæ¨¡æ€ç”ŸæˆåŠ å‰§äº†å®‰å…¨é—®é¢˜ï¼Œå› ä¸ºå¯¹æ‰‹å¯ä»¥é€šè¿‡å·§å¦™åœ°æ“çºµæœ€è„†å¼±çš„æ¨¡æ€ï¼ˆä¾‹å¦‚è§†è§‰ï¼‰æ¥æˆåŠŸé€ƒé¿æ•´ä¸ªç³»ç»Ÿã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºè®®åœ¨æœ€ç°å®å’Œé«˜é£é™©çš„ç¯å¢ƒä¸­è¯„ä¼°å¼€æºå¤§å‹ VLM çš„ç¨³å¥æ€§ï¼Œåœ¨è¿™ç§ç¯å¢ƒä¸­ï¼Œå¯¹æ‰‹åªæœ‰é»‘ç›’ç³»ç»Ÿè®¿é—®æƒé™ï¼Œå¹¶è¯•å›¾æ¬ºéª—æ¨¡å‹è¿”å›ç›®æ ‡å“åº”ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é¦–å…ˆé’ˆå¯¹ CLIP å’Œ BLIP ç­‰é¢„è®­ç»ƒæ¨¡å‹åˆ¶ä½œæœ‰é’ˆå¯¹æ€§çš„å¯¹æŠ—æ€§ç¤ºä¾‹ï¼Œç„¶åå°†è¿™äº›å¯¹æŠ—æ€§ç¤ºä¾‹è½¬ç§»åˆ°å…¶ä»– VLMï¼Œå¦‚ MiniGPT-4ã€LLaVAã€UniDiffuserã€BLIP-2 å’Œ Img2Promptã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¯¹è¿™äº› VLM çš„é»‘ç›’æŸ¥è¯¢å¯ä»¥è¿›ä¸€æ­¥æé«˜æœ‰é’ˆå¯¹æ€§çš„è§„é¿çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆæœ‰é’ˆå¯¹æ€§çš„å“åº”çš„æˆåŠŸç‡é«˜å¾—æƒŠäººã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæä¾›äº†å¯¹å¤§å‹ VLM çš„å¯¹æŠ—æ€§æ¼æ´çš„å®šé‡ç†è§£ï¼Œå¹¶å‘¼ååœ¨å®é™…éƒ¨ç½²ä¹‹å‰å¯¹å…¶æ½œåœ¨çš„å®‰å…¨ç¼ºé™·è¿›è¡Œæ›´å½»åº•çš„æ£€æŸ¥ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼šhttps://yunqing-me.github.io/AttackVLM/ã€‚

## What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks<sup>poster<sup>

Authors: Taicheng Guo, kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang

Link: [https://neurips.cc/virtual/2023/poster/73716](https://neurips.cc/virtual/2023/poster/73716)

Abstract:

 Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4,GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMsâ€™ performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.

æ‘˜è¦:

 å…·æœ‰å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å‡ºç°ï¼Œå¹¶å·²åº”ç”¨äºç§‘å­¦ã€é‡‘èå’Œè½¯ä»¶å·¥ç¨‹ç­‰å„ä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œæ³•å­¦ç¡•å£«æ¨åŠ¨åŒ–å­¦é¢†åŸŸå‘å±•çš„èƒ½åŠ›ä»ä¸æ¸…æ¥šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡ä¸æ˜¯è¿½æ±‚æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œè€Œæ˜¯è¯„ä¼°æ³•å­¦ç¡•å£«åœ¨åŒ–å­¦é¢†åŸŸå„ç§ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç¡®å®šäº†æ³•å­¦ç¡•å£«ä¸­éœ€è¦æ¢ç´¢çš„ä¸‰ç§å…³é”®çš„åŒ–å­¦ç›¸å…³èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç†è§£ã€æ¨ç†å’Œè§£é‡Šï¼Œå¹¶å»ºç«‹äº†åŒ…å«å…«é¡¹åŒ–å­¦ä»»åŠ¡çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„åˆ†æåˆ©ç”¨äº†å¹¿æ³›è®¤å¯çš„æ•°æ®é›†ï¼Œä¿ƒè¿›äº†æ³•å­¦ç¡•å£«åœ¨å®ç”¨åŒ–å­¦èƒŒæ™¯ä¸‹èƒ½åŠ›çš„å¹¿æ³›æ¢ç´¢ã€‚äº”ä¸ªæ³•å­¦ç¡•å£«ï¼ˆGPT-4ã€GPT-3.5ã€Davinci-003ã€Llama å’Œ Gacticaï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ç¯å¢ƒä¸­çš„æ¯é¡¹åŒ–å­¦ä»»åŠ¡ä¸­è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä½¿ç”¨ç²¾å¿ƒæŒ‘é€‰çš„æ¼”ç¤ºç¤ºä¾‹å’Œä¸“é—¨åˆ¶ä½œçš„æç¤ºã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼ŒGPT-4 ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œæ³•å­¦ç¡•å£«åœ¨å…«é¡¹åŒ–å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸åŒçš„ç«äº‰æ°´å¹³ã€‚é™¤äº†ç»¼åˆåŸºå‡†åˆ†æçš„ä¸»è¦å‘ç°ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å·¥ä½œè¿˜æ·±å…¥äº†è§£äº†å½“å‰æ³•å­¦ç¡•å£«çš„å±€é™æ€§ä»¥åŠæƒ…å¢ƒå­¦ä¹ è®¾ç½®å¯¹æ³•å­¦ç¡•å£«åœ¨å„ç§åŒ–å­¦ä»»åŠ¡ä¸­è¡¨ç°çš„å½±å“ã€‚æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„ä»£ç å’Œæ•°æ®é›†å¯ä» https://github.com/ChemFoundationModels/ChemmLLMBench è·å–ã€‚

## CoLLAT: On Adding Fine-grained Audio Understanding to Language Models using Token-Level Locked-Language Tuning<sup>poster<sup>

Authors: Dadallage A R Silva, Spencer Whitehead, Christopher Lengerich, Hugh Leather

Link: [https://neurips.cc/virtual/2023/poster/72956](https://neurips.cc/virtual/2023/poster/72956)

Abstract:

 Humans can easily understand various audio concepts, but conventional audio classification models fail due to their inability to predict unseen classes during training. To address this challenge, recent literature has explored contrastive language-audio pretraining to learn an audio understanding model using natural language supervision from a pretrained language model. However, despite their reasonable zero-shot performance in audio understanding, these models typically fail to achieve optimal performance while preserving the text understanding capabilities of the pretrained language model. They also perform poorly when comprehending audio clips with multiple audio concepts. To bridge these gaps, we propose $CoLLAT$: $Co$ntrastive $L$ocked $L$anguage and $A$udio $T$uning. This is a framework to effectively learn an audio understanding model with a locked language model, which is learned using a novel pretraining objective for audio-to-text grounding to yield fine-grained audio understanding. Our extensive experiments, which include several downstream applications such as audio classification, cross-modal retrieval, and audio-guided image generation, demonstrate that $CoLLAT$ yields state-of-the-art performance for audio understanding. Additionally, it unlocks audio guidance to applications built on top of pretrained language models.

æ‘˜è¦:

äººç±»å¯ä»¥è½»æ¾ç†è§£å„ç§éŸ³é¢‘æ¦‚å¿µï¼Œä½†ä¼ ç»Ÿçš„éŸ³é¢‘åˆ†ç±»æ¨¡å‹ç”±äºæ— æ³•é¢„æµ‹è®­ç»ƒè¿‡ç¨‹ä¸­çœ‹ä¸è§çš„ç±»åˆ«è€Œå¤±è´¥ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ€è¿‘çš„æ–‡çŒ®æ¢ç´¢äº†å¯¹æ¯”è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒï¼Œä»¥ä½¿ç”¨æ¥è‡ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç›‘ç£æ¥å­¦ä¹ éŸ³é¢‘ç†è§£æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å…·æœ‰åˆç†çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†è¿™äº›æ¨¡å‹é€šå¸¸æ— æ³•åœ¨ä¿ç•™é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç†è§£èƒ½åŠ›çš„åŒæ—¶å®ç°æœ€ä½³æ€§èƒ½ã€‚ä»–ä»¬åœ¨ç†è§£å…·æœ‰å¤šä¸ªéŸ³é¢‘æ¦‚å¿µçš„éŸ³é¢‘å‰ªè¾‘æ—¶ä¹Ÿè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å»ºè®® $CoLLAT$ï¼š$Co$ntrastive $L$ocked $L$anguage å’Œ $A$udio $T$uningã€‚è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨é”å®šè¯­è¨€æ¨¡å‹æœ‰æ•ˆå­¦ä¹ éŸ³é¢‘ç†è§£æ¨¡å‹çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¯ä½¿ç”¨ä¸€ç§æ–°é¢–çš„éŸ³é¢‘åˆ°æ–‡æœ¬åŸºç¡€çš„é¢„è®­ç»ƒç›®æ ‡æ¥å­¦ä¹ çš„ï¼Œä»¥äº§ç”Ÿç»†ç²’åº¦çš„éŸ³é¢‘ç†è§£ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒï¼ˆåŒ…æ‹¬éŸ³é¢‘åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢å’ŒéŸ³é¢‘å¼•å¯¼å›¾åƒç”Ÿæˆç­‰å¤šä¸ªä¸‹æ¸¸åº”ç”¨ï¼‰è¡¨æ˜ï¼Œ$CoLLAT$ åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä¸ºåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ„å»ºçš„åº”ç”¨ç¨‹åºè§£é”äº†éŸ³é¢‘æŒ‡å¯¼ã€‚

## VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks<sup>poster<sup>

Authors: Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai

Link: [https://neurips.cc/virtual/2023/poster/71428](https://neurips.cc/virtual/2023/poster/71428)

Abstract:

 Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The code shall be released.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç€åŠ é€Ÿäº†é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿›æ­¥ï¼Œå…¶å¯¹ç”¨æˆ·å®šåˆ¶ä»»åŠ¡å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œèµ‹äºˆå®ƒä»¬åœ¨ä¸€ç³»åˆ—åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå°½ç®¡æœ‰ä¼—å¤šå¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å¯ç”¨ï¼Œä½†å®ƒä»¬ä»ç„¶ä»…é™äºé¢„å®šä¹‰å½¢å¼çš„ä»»åŠ¡ï¼Œéš¾ä»¥ä¸æ³•å­¦ç¡•å£«çš„å¼€æ”¾å¼ä»»åŠ¡èƒ½åŠ›ç›¸åŒ¹é…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäº LLM çš„æ¡†æ¶ï¼Œç”¨äºä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼Œç§°ä¸º VisionLLMã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å›¾åƒè§†ä¸ºå¤–è¯­ï¼Œå¹¶å°†ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸å¯ä»¥ä½¿ç”¨è¯­è¨€æŒ‡ä»¤çµæ´»å®šä¹‰å’Œç®¡ç†çš„è¯­è¨€ä»»åŠ¡ç»“åˆèµ·æ¥ï¼Œä¸ºè§†è§‰å’Œè¯­è¨€ä»»åŠ¡æä¾›äº†ç»Ÿä¸€çš„è§†è§’ã€‚ç„¶åï¼ŒåŸºäº LLM çš„è§£ç å™¨å¯ä»¥æ ¹æ®è¿™äº›æŒ‡ä»¤å¯¹å¼€æ”¾å¼ä»»åŠ¡åšå‡ºé€‚å½“çš„é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„VisionLLMå¯ä»¥é€šè¿‡è¯­è¨€æŒ‡ä»¤å®ç°ä¸åŒçº§åˆ«çš„ä»»åŠ¡å®šåˆ¶ï¼Œä»ç»†ç²’åº¦çš„å¯¹è±¡çº§åˆ°ç²—ç²’åº¦çš„ä»»åŠ¡çº§å®šåˆ¶ï¼Œå‡å–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡åŸºäº LLM çš„é€šç”¨æ¡†æ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨ COCO ä¸Šå®ç°è¶…è¿‡ 60% çš„ mAPï¼Œä¸ç‰¹å®šäºæ£€æµ‹çš„æ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿä¸ºé€šæ‰è§†è§‰å’Œè¯­è¨€æ¨¡å‹è®¾å®šæ–°çš„åŸºçº¿ã€‚ä»£ç å°†è¢«å‘å¸ƒã€‚

## Large language models transition from integrating across position-yoked, exponential windows to structure-yoked, power-law windows<sup>poster<sup>

Authors: David Skrill, Samuel Norman-Haignere

Link: [https://neurips.cc/virtual/2023/poster/73028](https://neurips.cc/virtual/2023/poster/73028)

Abstract:

 Modern language models excel at integrating across long temporal scales needed to encode linguistic meaning and show non-trivial similarities to biological neural systems. Prior work suggests that human brain responses to language exhibit hierarchically organized "integration windows" that substantially constrain the overall influence of an input token (e.g., a word) on the neural response. However, little prior work has attempted to use integration windows to characterize computations in large language models (LLMs). We developed a simple word-swap procedure for estimating integration windows from black-box language models that does not depend on access to gradients or knowledge of the model architecture (e.g., attention weights). Using this method, we show that trained LLMs exhibit stereotyped integration windows that are well-fit by a convex combination of an exponential and a power-law function, with a partial transition from exponential to power-law dynamics across network layers. We then introduce a metric for quantifying the extent to which these integration windows vary with structural boundaries (e.g., sentence boundaries), and using this metric, we show that integration windows become increasingly yoked to structure at later network layers. None of these findings were observed in an untrained model, which as expected integrated uniformly across its input. These results suggest that LLMs learn to integrate information in natural language using a stereotyped pattern: integrating across position-yoked, exponential windows at early layers, followed by structure-yoked, power-law windows at later layers. The methods we describe in this paper provide a general-purpose toolkit for understanding temporal integration in language models, facilitating cross-disciplinary research at the intersection of biological and artificial intelligence.

æ‘˜è¦:

ç°ä»£è¯­è¨€æ¨¡å‹æ“…é•¿æ•´åˆç¼–ç è¯­è¨€æ„ä¹‰æ‰€éœ€çš„é•¿æ—¶é—´å°ºåº¦ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¸ç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„éå¹³å‡¡ç›¸ä¼¼æ€§ã€‚å…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼Œäººè„‘å¯¹è¯­è¨€çš„ååº”è¡¨ç°å‡ºåˆ†å±‚ç»„ç»‡çš„â€œæ•´åˆçª—å£â€ï¼Œè¯¥çª—å£å®è´¨ä¸Šé™åˆ¶äº†è¾“å…¥æ ‡è®°ï¼ˆä¾‹å¦‚ï¼Œå•è¯ï¼‰å¯¹ç¥ç»ååº”çš„æ€»ä½“å½±å“ã€‚ç„¶è€Œï¼Œä¹‹å‰å¾ˆå°‘æœ‰å·¥ä½œå°è¯•ä½¿ç”¨é›†æˆçª—å£æ¥è¡¨å¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è®¡ç®—ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç®€å•çš„å•è¯äº¤æ¢è¿‡ç¨‹ï¼Œç”¨äºä¼°è®¡é»‘ç›’è¯­è¨€æ¨¡å‹çš„é›†æˆçª—å£ï¼Œè¯¥è¿‡ç¨‹ä¸ä¾èµ–äºå¯¹æ¢¯åº¦çš„è®¿é—®æˆ–æ¨¡å‹æ¶æ„çš„çŸ¥è¯†ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›æƒé‡ï¼‰ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è¡¨æ˜è®­ç»ƒæœ‰ç´ çš„ LLM è¡¨ç°å‡ºåˆ»æ¿çš„ç§¯åˆ†çª—å£ï¼Œè¯¥çª—å£éå¸¸é€‚åˆæŒ‡æ•°å‡½æ•°å’Œå¹‚å¾‹å‡½æ•°çš„å‡¸ç»„åˆï¼Œå¹¶ä¸”è·¨ç½‘ç»œå±‚ä»æŒ‡æ•°åˆ°å¹‚å¾‹åŠ¨æ€çš„éƒ¨åˆ†è¿‡æ¸¡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªåº¦é‡æ¥é‡åŒ–è¿™äº›é›†æˆçª—å£éšç»“æ„è¾¹ç•Œï¼ˆä¾‹å¦‚ï¼Œå¥å­è¾¹ç•Œï¼‰å˜åŒ–çš„ç¨‹åº¦ï¼Œå¹¶ä½¿ç”¨è¯¥åº¦é‡ï¼Œæˆ‘ä»¬è¡¨æ˜é›†æˆçª—å£è¶Šæ¥è¶Šä¸åé¢çš„ç½‘ç»œå±‚çš„ç»“æ„ç›¸å…³è”ã€‚åœ¨æœªç»è®­ç»ƒçš„æ¨¡å‹ä¸­æ²¡æœ‰è§‚å¯Ÿåˆ°è¿™äº›å‘ç°ï¼Œæ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œè¯¥æ¨¡å‹åœ¨å…¶è¾“å…¥ä¸­ç»Ÿä¸€æ•´åˆã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ³•å­¦ç¡•å£«å­¦ä¹ ä½¿ç”¨åˆ»æ¿æ¨¡å¼æ•´åˆè‡ªç„¶è¯­è¨€ä¸­çš„ä¿¡æ¯ï¼šåœ¨æ—©æœŸå±‚è·¨ä½ç½®å…³è”çš„æŒ‡æ•°çª—å£è¿›è¡Œæ•´åˆï¼Œç„¶ååœ¨åé¢çš„å±‚è¿›è¡Œç»“æ„å…³è”çš„å¹‚å¾‹çª—å£æ•´åˆã€‚æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æè¿°çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªé€šç”¨å·¥å…·åŒ…ï¼Œç”¨äºç†è§£è¯­è¨€æ¨¡å‹ä¸­çš„æ—¶é—´æ•´åˆï¼Œä¿ƒè¿›ç”Ÿç‰©å’Œäººå·¥æ™ºèƒ½äº¤å‰é¢†åŸŸçš„è·¨å­¦ç§‘ç ”ç©¶ã€‚

## Text Promptable Surgical Instrument Segmentation with Vision-Language Models<sup>poster<sup>

Authors: Zijian Zhou, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Miaojing Shi

Link: [https://neurips.cc/virtual/2023/poster/71267](https://neurips.cc/virtual/2023/poster/71267)

Abstract:

 In this paper, we propose a novel text promptable surgical instrument segmentation approach to overcome challenges associated with diversity and differentiation of surgical instruments in minimally invasive surgeries. We redefine the task as text promptable, thereby enabling a more nuanced comprehension of surgical instruments and adaptability to new instrument types. Inspired by recent advancements in vision-language models, we leverage pretrained image and text encoders as our model backbone and design a text promptable mask decoder consisting of attention- and convolution-based prompting schemes for surgical instrument segmentation prediction. Our model leverages multiple text prompts for each surgical instrument through a new mixture of prompts mechanism, resulting in enhanced segmentation performance. Additionally, we introduce a hard instrument area reinforcement module to improve image feature comprehension and segmentation precision. Extensive experiments on several surgical instrument segmentation datasets demonstrate our model's superior performance and promising generalization capability. To our knowledge, this is the first implementation of a promptable approach to surgical instrument segmentation, offering significant potential for practical application in the field of robotic-assisted surgery.

æ‘˜è¦:

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬æç¤ºæ‰‹æœ¯å™¨æ¢°åˆ†å‰²æ–¹æ³•ï¼Œä»¥å…‹æœå¾®åˆ›æ‰‹æœ¯ä¸­æ‰‹æœ¯å™¨æ¢°å¤šæ ·æ€§å’Œå·®å¼‚åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºæ–‡æœ¬æç¤ºï¼Œä»è€Œèƒ½å¤Ÿæ›´ç»†è‡´åœ°ç†è§£æ‰‹æœ¯å™¨æ¢°å¹¶é€‚åº”æ–°çš„å™¨æ¢°ç±»å‹ã€‚å—è§†è§‰è¯­è¨€æ¨¡å‹æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ä½œä¸ºæˆ‘ä»¬çš„æ¨¡å‹ä¸»å¹²ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–‡æœ¬æç¤ºæ©æ¨¡è§£ç å™¨ï¼Œå…¶ä¸­åŒ…æ‹¬ç”¨äºæ‰‹æœ¯å™¨æ¢°åˆ†å‰²é¢„æµ‹çš„åŸºäºæ³¨æ„åŠ›å’Œå·ç§¯çš„æç¤ºæ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ–°çš„æ··åˆæç¤ºæœºåˆ¶åˆ©ç”¨æ¯ä¸ªæ‰‹æœ¯å™¨æ¢°çš„å¤šä¸ªæ–‡æœ¬æç¤ºï¼Œä»è€Œå¢å¼ºäº†åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¡¬ä»ªå™¨åŒºåŸŸå¢å¼ºæ¨¡å—æ¥æé«˜å›¾åƒç‰¹å¾ç†è§£å’Œåˆ†å‰²ç²¾åº¦ã€‚å¯¹å¤šä¸ªæ‰‹æœ¯å™¨æ¢°åˆ†å‰²æ•°æ®é›†çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹çš„å“è¶Šæ€§èƒ½å’Œæœ‰å‰é€”çš„æ³›åŒ–èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯æ‰‹æœ¯å™¨æ¢°åˆ†å‰²çš„å¿«é€Ÿæ–¹æ³•çš„é¦–æ¬¡å®æ–½ï¼Œä¸ºæœºå™¨äººè¾…åŠ©æ‰‹æœ¯é¢†åŸŸçš„å®é™…åº”ç”¨æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚

## Data Selection for Language Models via Importance Resampling<sup>poster<sup>

Authors: Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang

Link: [https://neurips.cc/virtual/2023/poster/70154](https://neurips.cc/virtual/2023/poster/70154)

Abstract:

 Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlation with average downstream accuracy (r=0.89) when computed with simple n-gram features. This motivates our instantiation of DSIR using n-gram features. When performing continued pretraining towards a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia + books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on the GLUE benchmark.

æ‘˜è¦:

é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ•°æ®é›†å¯¹äºé€šç”¨é¢†åŸŸï¼ˆä¾‹å¦‚ GPT-3ï¼‰å’Œç‰¹å®šé¢†åŸŸï¼ˆä¾‹å¦‚ Codexï¼‰è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰éƒ½è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å°†è¿™ä¸ªé—®é¢˜å½¢å¼åŒ–ä¸ºé€‰æ‹©ä¸€ä¸ªå¤§å‹åŸå§‹æœªæ ‡è®°æ•°æ®é›†çš„å­é›†æ¥åŒ¹é…ç»™å®šä¸€äº›æœªæ ‡è®°ç›®æ ‡æ ·æœ¬çš„æ‰€éœ€ç›®æ ‡åˆ†å¸ƒã€‚ç”±äºåŸå§‹æ–‡æœ¬æ•°æ®è§„æ¨¡å¤§ã€ç»´åº¦å¤šï¼Œç°æœ‰æ–¹æ³•ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•æˆ–ä½¿ç”¨ä¸“å®¶æ‰‹åŠ¨æ•´ç†æ•°æ®ã€‚ç›¸åï¼Œæˆ‘ä»¬æ‰©å±•äº†ä½ç»´ LM æ•°æ®é€‰æ‹©ä¸­ä½¿ç”¨çš„ç»å…¸é‡è¦æ€§é‡é‡‡æ ·æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†å…·æœ‰é‡è¦æ€§é‡é‡‡æ ·çš„æ•°æ®é€‰æ‹©ï¼ˆDSIRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯ä»¥ä¼°è®¡ç¼©å°çš„ç‰¹å¾ç©ºé—´ä¸­çš„é‡è¦æ€§æƒé‡ä»¥å®ç°æ˜“å¤„ç†ï¼Œå¹¶æ ¹æ®è¿™äº›æƒé‡é€‰æ‹©å…·æœ‰é‡è¦æ€§é‡é‡‡æ ·çš„æ•°æ®ã€‚ä¸ºäº†ç¡®å®šåˆé€‚çš„ç‰¹å¾ç©ºé—´ï¼Œæˆ‘ä»¬è¯æ˜äº† KL ç¼©å‡ï¼ˆä¸€ç§è¡¡é‡ç‰¹å¾ç©ºé—´ä¸­é€‰å®šçš„é¢„è®­ç»ƒæ•°æ®ä¸ç›®æ ‡ä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦çš„æ•°æ®åº¦é‡ï¼‰åœ¨ä½¿ç”¨ç®€å•çš„ n- è®¡ç®—æ—¶ä¸å¹³å‡ä¸‹æ¸¸ç²¾åº¦ (r=0.89) å…·æœ‰é«˜åº¦ç›¸å…³æ€§å…‹ç‰¹å¾ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬ä½¿ç”¨ n-gram ç‰¹å¾æ¥å®ä¾‹åŒ– DSIRã€‚å½“é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡ŒæŒç»­é¢„è®­ç»ƒæ—¶ï¼ŒDSIR åœ¨ 8 ä¸ªç›®æ ‡åˆ†å¸ƒä¸Šçš„è¡¨ç°ä¸ä¸“å®¶ç®¡ç†ç›¸å½“ã€‚åœ¨é¢„è®­ç»ƒé€šç”¨é¢†åŸŸæ¨¡å‹ï¼ˆç›®æ ‡æ˜¯ç»´åŸºç™¾ç§‘ + ä¹¦ç±ï¼‰æ—¶ï¼ŒDSIR åœ¨ GLUE åŸºå‡†ä¸Šæ¯”éšæœºé€‰æ‹©å’Œå¯å‘å¼è¿‡æ»¤åŸºçº¿æé«˜äº† 2-2.5%ã€‚

## Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language.<sup>poster<sup>

Authors: Eghbal Hosseini, Evelina Fedorenko

Link: [https://neurips.cc/virtual/2023/poster/70807](https://neurips.cc/virtual/2023/poster/70807)

Abstract:

 Predicting upcoming events is critical to our ability to effectively interact with ourenvironment and conspecifics. In natural language processing, transformer models,which are trained on next-word prediction, appear to construct a general-purposerepresentation of language that can support diverse downstream tasks. However, westill lack an understanding of how a predictive objective shapes such representations.Inspired by recent work in vision neuroscience HÃ©naff et al. (2019), here we test ahypothesis about predictive representations of autoregressive transformer models.In particular, we test whether the neural trajectory of a sequence of words in asentence becomes progressively more straight as it passes through the layers of thenetwork. The key insight behind this hypothesis is that straighter trajectories shouldfacilitate prediction via linear extrapolation. We quantify straightness using a 1-dimensional curvature metric, and present four findings in support of the trajectorystraightening hypothesis: i) In trained models, the curvature progressively decreasesfrom the first to the middle layers of the network. ii) Models that perform better onthe next-word prediction objective, including larger models and models trained onlarger datasets, exhibit greater decreases in curvature, suggesting that this improvedability to straighten sentence neural trajectories may be the underlying driver ofbetter language modeling performance. iii) Given the same linguistic context, thesequences that are generated by the model have lower curvature than the groundtruth (the actual continuations observed in a language corpus), suggesting thatthe model favors straighter trajectories for making predictions. iv) A consistentrelationship holds between the average curvature and the average surprisal ofsentences in the middle layers of models, such that sentences with straighter neuraltrajectories also have lower surprisal. Importantly, untrained models donâ€™t exhibitthese behaviors. In tandem, these results support the trajectory straighteninghypothesis and provide a possible mechanism for how the geometry of the internalrepresentations of autoregressive models supports next word prediction.

æ‘˜è¦:

é¢„æµ‹å³å°†å‘ç”Ÿçš„äº‹ä»¶å¯¹äºæˆ‘ä»¬ä¸ç¯å¢ƒå’ŒåŒç±»æœ‰æ•ˆäº’åŠ¨çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œç»è¿‡ä¸‹ä¸€ä¸ªå•è¯é¢„æµ‹è®­ç»ƒçš„ Transformer æ¨¡å‹ä¼¼ä¹æ„å»ºäº†å¯ä»¥æ”¯æŒå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„é€šç”¨è¯­è¨€è¡¨ç¤ºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç„¶ç¼ºä¹å¯¹é¢„æµ‹ç›®æ ‡å¦‚ä½•å½¢æˆè¿™ç§è¡¨å¾çš„ç†è§£ã€‚å—åˆ°è§†è§‰ç¥ç»ç§‘å­¦ HÃ©naff ç­‰äººæœ€è¿‘å·¥ä½œçš„å¯å‘ã€‚ ï¼ˆ2019ï¼‰ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æµ‹è¯•äº†å…³äºè‡ªå›å½’å˜å‹å™¨æ¨¡å‹çš„é¢„æµ‹è¡¨ç¤ºçš„å‡è®¾ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æµ‹è¯•äº†å¥å­ä¸­å•è¯åºåˆ—çš„ç¥ç»è½¨è¿¹åœ¨ç©¿è¿‡ç½‘ç»œå±‚æ—¶æ˜¯å¦é€æ¸å˜å¾—æ›´åŠ ç›´ã€‚è¿™ä¸€å‡è®¾èƒŒåçš„å…³é”®è§è§£æ˜¯ï¼Œæ›´ç›´çš„è½¨è¿¹åº”è¯¥æœ‰åŠ©äºé€šè¿‡çº¿æ€§å¤–æ¨è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç»´æ›²ç‡åº¦é‡æ¥é‡åŒ–ç›´çº¿åº¦ï¼Œå¹¶æå‡ºå››ä¸ªå‘ç°æ¥æ”¯æŒè½¨è¿¹æ‹‰ç›´å‡è®¾ï¼šiï¼‰åœ¨è®­ç»ƒæ¨¡å‹ä¸­ï¼Œæ›²ç‡ä»ç½‘ç»œçš„ç¬¬ä¸€å±‚åˆ°ä¸­é—´å±‚é€æ¸å‡å°ã€‚ iiï¼‰åœ¨ä¸‹ä¸€ä¸ªå•è¯é¢„æµ‹ç›®æ ‡ä¸Šè¡¨ç°æ›´å¥½çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬æ›´å¤§çš„æ¨¡å‹å’Œåœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œè¡¨ç°å‡ºæ›´å¤§çš„æ›²ç‡ä¸‹é™ï¼Œè¿™è¡¨æ˜è¿™ç§æ‹‰ç›´å¥å­ç¥ç»è½¨è¿¹çš„æ”¹è¿›èƒ½åŠ›å¯èƒ½æ˜¯æ›´å¥½çš„è¯­è¨€å»ºæ¨¡æ€§èƒ½çš„æ½œåœ¨é©±åŠ¨åŠ›ã€‚ iiiï¼‰åœ¨ç›¸åŒçš„è¯­è¨€ç¯å¢ƒä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆçš„åºåˆ—çš„æ›²ç‡ä½äºçœŸå®å€¼ï¼ˆåœ¨è¯­è¨€è¯­æ–™åº“ä¸­è§‚å¯Ÿåˆ°çš„å®é™…è¿ç»­ï¼‰ï¼Œè¿™è¡¨æ˜è¯¥æ¨¡å‹å€¾å‘äºä½¿ç”¨æ›´ç›´çš„è½¨è¿¹è¿›è¡Œé¢„æµ‹ã€‚ iv) æ¨¡å‹ä¸­é—´å±‚å¥å­çš„å¹³å‡æ›²ç‡å’Œå¹³å‡æƒŠè®¶åº¦ä¹‹é—´å­˜åœ¨ä¸€è‡´çš„å…³ç³»ï¼Œä½¿å¾—ç¥ç»è½¨è¿¹è¾ƒç›´çš„å¥å­ä¹Ÿå…·æœ‰è¾ƒä½çš„æƒŠè®¶åº¦ã€‚é‡è¦çš„æ˜¯ï¼Œæœªç»è®­ç»ƒçš„æ¨¡å‹ä¸ä¼šè¡¨ç°å‡ºè¿™äº›è¡Œä¸ºã€‚åŒæ—¶ï¼Œè¿™äº›ç»“æœæ”¯æŒè½¨è¿¹çŸ«ç›´å‡è®¾ï¼Œå¹¶ä¸ºè‡ªå›å½’æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„å‡ ä½•å¦‚ä½•æ”¯æŒä¸‹ä¸€ä¸ªå•è¯é¢„æµ‹æä¾›äº†å¯èƒ½çš„æœºåˆ¶ã€‚

## Grammar Prompting for Domain-Specific Language Generation with  Large Language Models<sup>poster<sup>

Authors: Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous, Yoon Kim

Link: [https://neurips.cc/virtual/2023/poster/72512](https://neurips.cc/virtual/2023/poster/72512)

Abstract:

 Large language models (LLMs)  can learn to perform a wide range of natural language tasks from just a  handful of in-context examples. However, for generating strings from highly structured  languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We propose \emph{grammar prompting}, a simple approach to enable LLMs to use external knowledge and domain-specific constraints, expressed through a grammar in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar.  For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡å°‘é‡çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ¥å­¦ä¹ æ‰§è¡Œå„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¸ºäº†ä»é«˜åº¦ç»“æ„åŒ–çš„è¯­è¨€ï¼ˆä¾‹å¦‚ï¼Œè¯­ä¹‰è§£æåˆ°å¤æ‚çš„ç‰¹å®šé¢†åŸŸè¯­è¨€ï¼‰ç”Ÿæˆå­—ç¬¦ä¸²ï¼Œæ³•å­¦ç¡•å£«å¾ˆéš¾ä»å‡ ä¸ªèŒƒä¾‹ä¸­è¿›è¡Œæ¦‚æ‹¬ã€‚æˆ‘ä»¬æå‡º\emph{è¯­æ³•æç¤º}ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œä½¿æ³•å­¦ç¡•å£«èƒ½å¤Ÿåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æœŸé—´ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å’Œç‰¹å®šé¢†åŸŸçš„çº¦æŸï¼Œé€šè¿‡å·´ç§‘æ–¯-è¯ºå°”èŒƒå¼ï¼ˆBNFï¼‰è¯­æ³•æ¥è¡¨è¾¾ã€‚è¯­æ³•æç¤ºé€šè¿‡ä¸“é—¨çš„è¯­æ³•å¢å¼ºäº†æ¯ä¸ªæ¼”ç¤ºç¤ºä¾‹ï¼Œè¯¥è¯­æ³•è‡³å°‘è¶³ä»¥ç”Ÿæˆç‰¹å®šçš„è¾“å‡ºç¤ºä¾‹ï¼Œå…¶ä¸­ä¸“é—¨çš„è¯­æ³•æ˜¯å®Œæ•´ DSL è¯­æ³•çš„å­é›†ã€‚å¯¹äºæ¨ç†ï¼ŒLLM é¦–å…ˆåœ¨ç»™å®šæµ‹è¯•è¾“å…¥çš„æƒ…å†µä¸‹é¢„æµ‹ BNF è¯­æ³•ï¼Œç„¶åæ ¹æ®è¯­æ³•è§„åˆ™ç”Ÿæˆè¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯­æ³•æç¤ºå¯ä»¥ä½¿æ³•å­¦ç¡•å£«èƒ½å¤Ÿåœ¨å„ç§ DSL ç”Ÿæˆä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬è¯­ä¹‰è§£æï¼ˆSMCalFlowã€Overnightã€GeoQueryï¼‰ã€PDDL è§„åˆ’å’ŒåŸºäº SMILES çš„åˆ†å­ç”Ÿæˆã€‚

## Large Language Models are Visual Reasoning Coordinators<sup>poster<sup>

Authors: Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu

Link: [https://neurips.cc/virtual/2023/poster/72992](https://neurips.cc/virtual/2023/poster/72992)

Abstract:

 Visual reasoning requires multimodal perception and commonsense cognition of the world. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains. However, how to harness the collective power of these complementary VLMs is rarely explored. Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.

æ‘˜è¦:

è§†è§‰æ¨ç†éœ€è¦å¤šæ¨¡æ€æ„ŸçŸ¥å’Œå¯¹ä¸–ç•Œçš„å¸¸è¯†è®¤çŸ¥ã€‚æœ€è¿‘ï¼Œäººä»¬æå‡ºäº†å¤šç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œåœ¨å„ä¸ªé¢†åŸŸéƒ½å…·æœ‰å‡ºè‰²çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¦‚ä½•åˆ©ç”¨è¿™äº›äº’è¡¥çš„ VLM çš„é›†ä½“åŠ›é‡å´å¾ˆå°‘è¢«æ¢è®¨ã€‚åƒé›†æˆè¿™æ ·çš„ç°æœ‰æ–¹æ³•ä»ç„¶éš¾ä»¥å°†è¿™äº›æ¨¡å‹ä¸æ‰€éœ€çš„é«˜é˜¶é€šä¿¡èšåˆèµ·æ¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Colaï¼Œè¿™æ˜¯ä¸€ç§åè°ƒå¤šä¸ª VLM è¿›è¡Œè§†è§‰æ¨ç†çš„æ–°é¢–èŒƒå¼ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å¯ä»¥é€šè¿‡ä¿ƒè¿›åˆ©ç”¨å…¶ç‹¬ç‰¹ä¸”äº’è¡¥çš„åŠŸèƒ½çš„è‡ªç„¶è¯­è¨€é€šä¿¡æ¥æœ‰æ•ˆåœ°åè°ƒå¤šä¸ª VLMã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æŒ‡ä»¤è°ƒæ•´å˜ä½“ Cola-FT åœ¨è§†è§‰é—®ç­” (VQA)ã€å¤–éƒ¨çŸ¥è¯† VQAã€è§†è§‰è•´æ¶µå’Œè§†è§‰ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä¸Šä¸‹æ–‡å­¦ä¹ å˜ä½“ Cola-Zero åœ¨é›¶æ¬¡å’Œå°‘æ¬¡è®¾ç½®ä¸­è¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶å’Œå¯è§†åŒ–ï¼Œæˆ‘ä»¬éªŒè¯äº†æ³•å­¦ç¡•å£«åè°ƒå‘˜ç¡®å®ç†è§£äº†æŒ‡ä»¤æç¤ºä»¥åŠ VLM çš„å•ç‹¬åŠŸèƒ½ï¼›ç„¶åå®ƒåè°ƒå®ƒä»¬ä»¥å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚

## Can Language Models Teach? Teacher Explanations Improve Student Performance via Personalization<sup>poster<sup>

Authors: Swarnadeep Saha, Peter Hase, Mohit Bansal

Link: [https://neurips.cc/virtual/2023/poster/72111](https://neurips.cc/virtual/2023/poster/72111)

Abstract:

 A hallmark property of explainable AI models is the ability to teach other agents, communicating knowledge of how to perform a task. While Large Language Models (LLMs) perform complex reasoning by generating explanations for their predictions, it is unclear whether they also make good teachers for weaker agents. To address this, we consider a student-teacher framework between two LLM agents and study if, when, and how the teacher should intervene with natural language explanations to improve the studentâ€™s performance. Since communication is expensive, we define a budget such that the teacher only communicates explanations for a fraction of the data, after which the student should perform well on its own. We decompose the teaching problem along four axes: (1) if teacherâ€™s test time in- tervention improve student predictions, (2) when it is worth explaining a data point, (3) how the teacher should personalize explanations to better teach the student, and (4) if teacher explanations also improve student performance on future unexplained data. We first show that teacher LLMs can indeed intervene on student reasoning to improve their performance. Next, inspired by the Theory of Mind abilities of effective teachers, we propose building two few-shot mental models of the student. The first model defines an Intervention Function that simulates the utility of an intervention, allowing the teacher to intervene when this utility is the highest and improving student performance at lower budgets. The second model enables the teacher to personalize explanations for a particular student and outperform unpersonalized teachers. We also demonstrate that in multi-turn interactions, teacher explanations generalize and learning from explained data improves student performance on future unexplained data. Finally, we also verify that misaligned teachers can lower student performance to random chance by intentionally misleading them.

æ‘˜è¦:

å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æ¨¡å‹çš„ä¸€ä¸ªæ ‡å¿—æ€§ç‰¹æ€§æ˜¯èƒ½å¤Ÿæ•™å¯¼å…¶ä»–æ™ºèƒ½ä½“ï¼Œäº¤æµå¦‚ä½•æ‰§è¡Œä»»åŠ¡çš„çŸ¥è¯†ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸ºå…¶é¢„æµ‹ç”Ÿæˆè§£é‡Šæ¥æ‰§è¡Œå¤æ‚çš„æ¨ç†ï¼Œä½†å°šä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦ä¹Ÿèƒ½æˆä¸ºè¾ƒå¼±æ™ºèƒ½ä½“çš„å¥½è€å¸ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸¤ä¸ªæ³•å­¦ç¡•å£«ä»£ç†äººä¹‹é—´çš„å­¦ç”Ÿ-æ•™å¸ˆæ¡†æ¶ï¼Œå¹¶ç ”ç©¶æ•™å¸ˆæ˜¯å¦ã€ä½•æ—¶ä»¥åŠå¦‚ä½•å¹²é¢„è‡ªç„¶è¯­è¨€è§£é‡Šä»¥æé«˜å­¦ç”Ÿçš„è¡¨ç°ã€‚ç”±äºæ²Ÿé€šçš„æˆæœ¬å¾ˆé«˜ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªé¢„ç®—ï¼Œè®©è€å¸ˆåªä¼ è¾¾å¯¹ä¸€å°éƒ¨åˆ†æ•°æ®çš„è§£é‡Šï¼Œä¹‹åå­¦ç”Ÿåº”è¯¥è‡ªå·±è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬æ²¿ç€å››ä¸ªè½´åˆ†è§£æ•™å­¦é—®é¢˜ï¼šï¼ˆ1ï¼‰æ•™å¸ˆçš„æµ‹è¯•æ—¶é—´å¹²é¢„æ˜¯å¦æ”¹å–„äº†å­¦ç”Ÿçš„é¢„æµ‹ï¼Œï¼ˆ2ï¼‰ä½•æ—¶å€¼å¾—è§£é‡Šæ•°æ®ç‚¹ï¼Œï¼ˆ3ï¼‰æ•™å¸ˆåº”å¦‚ä½•ä¸ªæ€§åŒ–è§£é‡Šä»¥æ›´å¥½åœ°æ•™å¯¼å­¦ç”Ÿï¼Œ (4) æ•™å¸ˆçš„è§£é‡Šæ˜¯å¦ä¹Ÿèƒ½æé«˜å­¦ç”Ÿåœ¨æœªæ¥æœªè§£é‡Šæ•°æ®ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜æ³•å­¦ç¡•å£«æ•™å¸ˆç¡®å®å¯ä»¥å¹²é¢„å­¦ç”Ÿçš„æ¨ç†ä»¥æé«˜ä»–ä»¬çš„è¡¨ç°ã€‚æ¥ä¸‹æ¥ï¼Œå—åˆ°é«˜æ•ˆæ•™å¸ˆå¿ƒæ™ºç†è®ºèƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬å»ºè®®å»ºç«‹ä¸¤ä¸ªå­¦ç”Ÿçš„å°æ ·æœ¬å¿ƒæ™ºæ¨¡å‹ã€‚ç¬¬ä¸€ä¸ªæ¨¡å‹å®šä¹‰äº†æ¨¡æ‹Ÿå¹²é¢„æ•ˆç”¨çš„å¹²é¢„å‡½æ•°ï¼Œå…è®¸æ•™å¸ˆåœ¨æ•ˆç”¨æœ€é«˜æ—¶è¿›è¡Œå¹²é¢„ï¼Œå¹¶ä»¥è¾ƒä½çš„é¢„ç®—æé«˜å­¦ç”Ÿçš„è¡¨ç°ã€‚ç¬¬äºŒç§æ¨¡å‹ä½¿æ•™å¸ˆèƒ½å¤Ÿå¯¹ç‰¹å®šå­¦ç”Ÿè¿›è¡Œä¸ªæ€§åŒ–è§£é‡Šï¼Œå¹¶è¶…è¶Šéä¸ªæ€§åŒ–çš„æ•™å¸ˆã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œåœ¨å¤šè½®äº¤äº’ä¸­ï¼Œæ•™å¸ˆçš„è§£é‡Šå¯ä»¥æ¦‚æ‹¬ï¼Œå¹¶ä¸”ä»è§£é‡Šçš„æ•°æ®ä¸­å­¦ä¹ å¯ä»¥æé«˜å­¦ç”Ÿåœ¨æœªæ¥æœªè§£é‡Šçš„æ•°æ®ä¸Šçš„è¡¨ç°ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜éªŒè¯äº†ä¸ä¸€è‡´çš„æ•™å¸ˆå¯ä»¥é€šè¿‡æ•…æ„è¯¯å¯¼å­¦ç”Ÿæ¥éšæœºé™ä½å­¦ç”Ÿçš„è¡¨ç°ã€‚

## Language Model Tokenizers Introduce Unfairness Between Languages<sup>poster<sup>

Authors: Aleksandar Petrov, Emanuele La Malfa, Philip Torr, Adel Bibi

Link: [https://neurips.cc/virtual/2023/poster/72721](https://neurips.cc/virtual/2023/poster/72721)

Abstract:

 Recent language models have shown impressive multilingual performance, even when not explicitly trained for it.Despite this, there are concerns about the quality of their outputs across different languages.In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked.The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases.These disparities persist even for tokenizers that are intentionally trained for multilingual support.Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs.This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the models.Therefore, we make the case that we should train future language models using multilingually fair subword tokenizers.

æ‘˜è¦:

å³ä½¿æ²¡æœ‰æ˜ç¡®è®­ç»ƒï¼Œæœ€è¿‘çš„è¯­è¨€æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å¤šè¯­è¨€æ€§èƒ½ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¸åŒè¯­è¨€çš„è¾“å‡ºè´¨é‡ä»ä»¤äººæ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸åŒè¯­è¨€çš„å¤„ç†å·®å¼‚æ˜¯å¦‚ä½•äº§ç”Ÿçš„åœ¨æ ‡è®°åŒ–é˜¶æ®µï¼Œç”šè‡³åœ¨æ¨¡å‹è¢«è°ƒç”¨ä¹‹å‰ã€‚ç¿»è¯‘æˆä¸åŒè¯­è¨€çš„ç›¸åŒæ–‡æœ¬å¯èƒ½å…·æœ‰æˆªç„¶ä¸åŒçš„æ ‡è®°åŒ–é•¿åº¦ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å·®å¼‚é«˜è¾¾ 15 å€ã€‚å³ä½¿å¯¹äºä¸“é—¨è®­ç»ƒå¤šè¯­è¨€æ”¯æŒçš„æ ‡è®°åŒ–å™¨ï¼Œè¿™äº›å·®å¼‚ä»ç„¶å­˜åœ¨å¯¹äºæŸäº›è¯­è¨€å¯¹ï¼Œå­—ç¬¦çº§å’Œå­—èŠ‚çº§æ¨¡å‹çš„ç¼–ç é•¿åº¦ä¹Ÿå­˜åœ¨è¶…è¿‡ 4 å€çš„å·®å¼‚ã€‚è¿™å¯¼è‡´æŸäº›è¯­è¨€ç¤¾åŒºåœ¨è®¿é—®å•†ä¸šè¯­è¨€æœåŠ¡çš„æˆæœ¬ã€å¤„ç†æ—¶é—´å’Œå»¶è¿Ÿæ–¹é¢å—åˆ°ä¸å…¬å¹³å¾…é‡ï¼Œä»¥åŠå¯ä»¥ä½œä¸ºæ¨¡å‹ä¸Šä¸‹æ–‡æä¾›çš„å†…å®¹é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºåº”è¯¥ä½¿ç”¨å¤šè¯­è¨€å…¬å¹³çš„å­è¯æ ‡è®°å™¨æ¥è®­ç»ƒæœªæ¥çš„è¯­è¨€æ¨¡å‹ã€‚

## LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation<sup>poster<sup>

Authors: Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, William Yang Wang

Link: [https://neurips.cc/virtual/2023/poster/71821](https://neurips.cc/virtual/2023/poster/71821)

Abstract:

 Existing automatic evaluation on text-to-image synthesis can only provide an image-text matching score, without considering the object-level compositionality, which results in poor correlation with human judgments. In this work, we propose LLMScore, a new framework that offers evaluation scores with multi-granularity compositionality. LLMScore leverages the large language models (LLMs) to evaluate text-to-image models. Initially, it transforms the image into image-level and object-level visual descriptions. Then an evaluation instruction is fed into the LLMs to measure the alignment between the synthesized image and the text, ultimately generating a score accompanied by a rationale. Our substantial analysis reveals the highest correlation of LLMScore with human judgments on a wide range of datasets (Attribute Binding Contrast, Concept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore achieves Kendall's tau correlation with human evaluations that is 58.8% and 31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP, respectively.

æ‘˜è¦:

 ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„è‡ªåŠ¨è¯„ä¼°åªèƒ½æä¾›å›¾åƒåˆ°æ–‡æœ¬çš„åŒ¹é…åˆ†æ•°ï¼Œè€Œæ²¡æœ‰è€ƒè™‘å¯¹è±¡çº§åˆ«çš„ç»„åˆæ€§ï¼Œè¿™å¯¼è‡´ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§è¾ƒå·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† LLMScoreï¼Œè¿™æ˜¯ä¸€ä¸ªæä¾›å¤šç²’åº¦ç»„åˆæ€§è¯„ä¼°åˆ†æ•°çš„æ–°æ¡†æ¶ã€‚ LLMScore åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ¥è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚æœ€åˆï¼Œå®ƒå°†å›¾åƒè½¬æ¢ä¸ºå›¾åƒçº§å’Œå¯¹è±¡çº§è§†è§‰æè¿°ã€‚ç„¶åï¼Œå°†è¯„ä¼°æŒ‡ä»¤è¾“å…¥åˆ°æ³•å­¦ç¡•å£«ä¸­ï¼Œä»¥æµ‹é‡åˆæˆå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹é½æƒ…å†µï¼Œæœ€ç»ˆç”Ÿæˆé™„æœ‰ç†ç”±çš„åˆ†æ•°ã€‚æˆ‘ä»¬çš„å®è´¨æ€§åˆ†ææ­ç¤ºäº† LLMScore ä¸äººç±»å¯¹å„ç§æ•°æ®é›†ï¼ˆå±æ€§ç»‘å®šå¯¹æ¯”åº¦ã€æ¦‚å¿µè¿æ¥ã€MSCOCOã€DrawBenchã€PaintSkillsï¼‰çš„åˆ¤æ–­å…·æœ‰æœ€é«˜çš„ç›¸å…³æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ LLMScore å®ç°äº† Kendall tau ä¸äººç±»è¯„ä¼°çš„ç›¸å…³æ€§ï¼Œåˆ†åˆ«æ¯”å¸¸ç”¨çš„æ–‡æœ¬å›¾åƒåŒ¹é…æŒ‡æ ‡ CLIP å’Œ BLIP é«˜ 58.8% å’Œ 31.2%ã€‚

## Joint processing of linguistic properties in brains and language models<sup>poster<sup>

Authors: SUBBAREDDY OOTA, Manish Gupta, Mariya Toneva

Link: [https://neurips.cc/virtual/2023/poster/72702](https://neurips.cc/virtual/2023/poster/72702)

Abstract:

 Language models have been shown to be very effective in predicting brain recordings of subjects experiencing complex language stimuli. For a deeper understanding of this alignment, it is important to understand the correspondence between the detailed processing of linguistic information by the human brain versus language models. We investigate this correspondence via a direct approach, in which we eliminate information related to specific linguistic properties in the language model representations and observe how this intervention affects the alignment with fMRI brain recordings obtained while participants listened to a story. We investigate a range of linguistic properties (surface, syntactic, and semantic) and find that the elimination of each one results in a significant decrease in brain alignment. Specifically, we find that syntactic properties (i.e. Top Constituents and Tree Depth) have the largest effect on the trend of brain alignment across model layers. These findings provide clear evidence for the role of specific linguistic information in the alignment between brain and language models, and open new avenues for mapping the joint information processing in both systems. We make the code publicly available [https://github.com/subbareddy248/linguistic-properties-brain-alignment].

æ‘˜è¦:

è¯­è¨€æ¨¡å‹å·²è¢«è¯æ˜åœ¨é¢„æµ‹ç»å†å¤æ‚è¯­è¨€åˆºæ¿€çš„å—è¯•è€…çš„å¤§è„‘è®°å½•æ–¹é¢éå¸¸æœ‰æ•ˆã€‚ä¸ºäº†æ›´æ·±å…¥åœ°ç†è§£è¿™ç§ä¸€è‡´æ€§ï¼Œäº†è§£äººè„‘ä¸è¯­è¨€æ¨¡å‹å¯¹è¯­è¨€ä¿¡æ¯çš„è¯¦ç»†å¤„ç†ä¹‹é—´çš„å¯¹åº”å…³ç³»éå¸¸é‡è¦ã€‚æˆ‘ä»¬é€šè¿‡ç›´æ¥æ–¹æ³•ç ”ç©¶è¿™ç§å¯¹åº”å…³ç³»ï¼Œåœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†è¯­è¨€æ¨¡å‹è¡¨ç¤ºä¸­ä¸ç‰¹å®šè¯­è¨€å±æ€§ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶è§‚å¯Ÿè¿™ç§å¹²é¢„å¦‚ä½•å½±å“ä¸å‚ä¸è€…å¬æ•…äº‹æ—¶è·å¾—çš„åŠŸèƒ½ç£å…±æŒ¯æˆåƒå¤§è„‘è®°å½•çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç³»åˆ—è¯­è¨€ç‰¹æ€§ï¼ˆè¡¨é¢ã€å¥æ³•å’Œè¯­ä¹‰ï¼‰ï¼Œå‘ç°æ¶ˆé™¤æ¯ä¸€ç§ç‰¹æ€§éƒ½ä¼šå¯¼è‡´å¤§è„‘æ’åˆ—æ˜¾ç€ä¸‹é™ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°å¥æ³•å±æ€§ï¼ˆå³é¡¶çº§æˆåˆ†å’Œæ ‘æ·±åº¦ï¼‰å¯¹è·¨æ¨¡å‹å±‚çš„å¤§è„‘å¯¹é½è¶‹åŠ¿å½±å“æœ€å¤§ã€‚è¿™äº›å‘ç°ä¸ºç‰¹å®šè¯­è¨€ä¿¡æ¯åœ¨å¤§è„‘å’Œè¯­è¨€æ¨¡å‹ä¹‹é—´çš„åè°ƒä¸­çš„ä½œç”¨æä¾›äº†æ˜ç¡®çš„è¯æ®ï¼Œå¹¶ä¸ºç»˜åˆ¶ä¸¤ä¸ªç³»ç»Ÿä¸­çš„è”åˆä¿¡æ¯å¤„ç†å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚æˆ‘ä»¬å…¬å¼€æä¾›ä»£ç  [https://github.com/subbareddy248/linguistic-properties-brain-alignment]ã€‚

## ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings<sup>oral<sup>

Authors: Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu

Link: [https://neurips.cc/virtual/2023/poster/72492](https://neurips.cc/virtual/2023/poster/72492)

Abstract:

 Integrating large language models (LLMs) with various tools has led to increased attention in the field. Existing approaches either involve fine-tuning the LLM, which is both computationally costly and limited to a fixed set of tools, or prompting LLMs by in-context tool demonstrations. Although the latter method offers adaptability to new tools, it struggles with the inherent context length constraint of LLMs when many new tools are presented, and mastering a new set of tools with few-shot examples remains challenging, resulting in suboptimal performance. To address these limitations, we propose a novel solution, named ToolkenGPT, wherein LLMs effectively learn to master tools as predicting tokens through tool embeddings for solving complex tasks. In this framework, each tool is transformed into vector embeddings and plugged into the language model head. Once the function is triggered during text generation, the LLM enters a special function mode to execute the tool calls. Our experiments show that function embeddings effectively help LLMs understand tool use and improve on several tasks, including numerical reasoning, knowledge-based question answering and embodied decision-making.

æ‘˜è¦:

å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å„ç§å·¥å…·é›†æˆå·²ç»å¼•èµ·äº†è¯¥é¢†åŸŸçš„è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆæ¶‰åŠå¯¹æ³•å­¦ç¡•å£«è¿›è¡Œå¾®è°ƒï¼Œè¿™æ—¢éœ€è¦è®¡ç®—æˆæœ¬é«˜ï¼Œåˆä»…é™äºä¸€ç»„å›ºå®šçš„å·¥å…·ï¼Œè¦ä¹ˆé€šè¿‡ä¸Šä¸‹æ–‡å·¥å…·æ¼”ç¤ºæ¥æç¤ºæ³•å­¦ç¡•å£«ã€‚å°½ç®¡åä¸€ç§æ–¹æ³•æä¾›äº†å¯¹æ–°å·¥å…·çš„é€‚åº”æ€§ï¼Œä½†å½“å‡ºç°è®¸å¤šæ–°å·¥å…·æ—¶ï¼Œå®ƒä¼šå—åˆ°æ³•å­¦ç¡•å£«å›ºæœ‰çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶çš„å›°æ‰°ï¼Œå¹¶ä¸”é€šè¿‡å°‘é‡ç¤ºä¾‹æŒæ¡ä¸€ç»„æ–°å·¥å…·ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º ToolkenGPT çš„æ–°é¢–è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­æ³•å­¦ç¡•å£«å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ æŒæ¡å·¥å…·ï¼Œé€šè¿‡å·¥å…·åµŒå…¥æ¥é¢„æµ‹ä»¤ç‰Œæ¥è§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæ¯ä¸ªå·¥å…·éƒ½è¢«è½¬æ¢ä¸ºå‘é‡åµŒå…¥å¹¶æ’å…¥åˆ°è¯­è¨€æ¨¡å‹å¤´ä¸­ã€‚ä¸€æ—¦åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­è§¦å‘è¯¥å‡½æ•°ï¼ŒLLM å°±ä¼šè¿›å…¥ç‰¹æ®Šå‡½æ•°æ¨¡å¼æ¥æ‰§è¡Œå·¥å…·è°ƒç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå‡½æ•°åµŒå…¥æœ‰æ•ˆåœ°å¸®åŠ©æ³•å­¦ç¡•å£«ç†è§£å·¥å…·çš„ä½¿ç”¨å¹¶æ”¹è¿›å¤šé¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ•°å­—æ¨ç†ã€åŸºäºçŸ¥è¯†çš„é—®ç­”å’Œå…·ä½“å†³ç­–ã€‚

## LeanDojo: Theorem Proving with Retrieval-Augmented Language Models<sup>oral<sup>

Authors: Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, Animashree Anandkumar

Link: [https://neurips.cc/virtual/2023/poster/73510](https://neurips.cc/virtual/2023/poster/73510)

Abstract:

 Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selectionâ€”a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨ä½¿ç”¨ Lean ç­‰è¯æ˜åŠ©æ‰‹è¯æ˜å½¢å¼å®šç†æ–¹é¢è¡¨ç°å‡ºäº†è‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œç”±äºç§æœ‰ä»£ç ã€æ•°æ®å’Œå¤§é‡è®¡ç®—éœ€æ±‚ï¼Œç°æœ‰æ–¹æ³•å¾ˆéš¾é‡ç°æˆ–æ„å»ºã€‚è¿™ç»™å®šç†è¯æ˜çš„æœºå™¨å­¦ä¹ æ–¹æ³•çš„ç ”ç©¶é€ æˆäº†å·¨å¤§çš„éšœç¢ã€‚æœ¬æ–‡é€šè¿‡ä»‹ç» LeanDojo æ¶ˆé™¤äº†è¿™äº›éšœç¢ï¼šä¸€ä¸ªç”±å·¥å…·åŒ…ã€æ•°æ®ã€æ¨¡å‹å’ŒåŸºå‡†ç»„æˆçš„å¼€æºç²¾ç›Šæ¸¸ä¹åœºã€‚ LeanDojo ä» Lean ä¸­æå–æ•°æ®ï¼Œå¹¶ä»¥ç¼–ç¨‹æ–¹å¼ä¸è¯æ˜ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚å®ƒåŒ…å«è¯æ˜ä¸­å‰æçš„ç»†ç²’åº¦æ³¨é‡Šï¼Œä¸ºå‰æé€‰æ‹©ï¼ˆå®šç†è¯æ˜ä¸­çš„å…³é”®ç“¶é¢ˆï¼‰æä¾›æœ‰ä»·å€¼çš„æ•°æ®ã€‚ä½¿ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº† ReProverï¼ˆæ£€ç´¢å¢å¼ºè¯æ˜å™¨ï¼‰ï¼šä¸€ç§åŸºäº LLM çš„è¯æ˜å™¨ï¼Œå¢å¼ºäº†æ£€ç´¢åŠŸèƒ½ï¼Œç”¨äºä»åºå¤§çš„æ•°å­¦åº“ä¸­é€‰æ‹©å‰æã€‚å®ƒä»·æ ¼ä½å»‰ï¼Œå¹¶ä¸”åªéœ€è¦ä¸€å‘¨çš„ GPU è®­ç»ƒã€‚æˆ‘ä»¬çš„æ£€ç´¢å™¨åˆ©ç”¨ LeanDojo çš„ç¨‹åºåˆ†æåŠŸèƒ½æ¥è¯†åˆ«å¯è®¿é—®çš„å‰æå’Œå›°éš¾çš„è´Ÿé¢ç¤ºä¾‹ï¼Œè¿™ä½¿å¾—æ£€ç´¢æ›´åŠ æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œå…¶ä¸­åŒ…å«ä» Lean æ•°å­¦åº“ä¸­æå–çš„ 98,734 ä¸ªå®šç†å’Œè¯æ˜ã€‚å®ƒå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®åˆ†å‰²ï¼Œè¦æ±‚è¯æ˜è€…ä¾èµ–äºè®­ç»ƒä¸­ä»æœªä½¿ç”¨è¿‡çš„æ–°é¢–å‰ææ¥æ¨å¹¿åˆ°å®šç†ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªåŸºå‡†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜äº† ReProver ç›¸å¯¹äºéæ£€ç´¢åŸºçº¿å’Œ GPT-4 çš„æœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€å¥—åŸºäºå¼€æº LLM çš„å®šç†è¯æ˜å™¨ï¼Œæ— éœ€ä»»ä½•ä¸“æœ‰æ•°æ®é›†ï¼Œå¹¶åœ¨ MIT è®¸å¯ä¸‹å‘å¸ƒï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚

## Language Is Not All You Need: Aligning Perception with Language Models<sup>poster<sup>

Authors: Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Nils Bjorck, Vishrav Chaudhary, Subhojit Som, XIA SONG, Furu Wei

Link: [https://neurips.cc/virtual/2023/poster/71488](https://neurips.cc/virtual/2023/poster/71488)

Abstract:

 A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multi-modal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.

æ‘˜è¦:

è¯­è¨€ã€å¤šæ¨¡æ€æ„ŸçŸ¥ã€è¡ŒåŠ¨å’Œä¸–ç•Œå»ºæ¨¡çš„å¤§èåˆæ˜¯è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®ä¸€æ­¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† KOSMOS-1ï¼Œä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œå®ƒå¯ä»¥æ„ŸçŸ¥ä¸€èˆ¬æ¨¡æ€ã€åœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ï¼ˆå³å°‘æ ·æœ¬ï¼‰å¹¶éµå¾ªæŒ‡ä»¤ï¼ˆå³é›¶æ ·æœ¬ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ç½‘ç»œè§„æ¨¡çš„å¤šæ¨¡æ€è¯­æ–™åº“ä¸Šä»å¤´å¼€å§‹è®­ç»ƒ KOSMOS-1ï¼ŒåŒ…æ‹¬ä»»æ„äº¤é”™çš„æ–‡æœ¬å’Œå›¾åƒã€å›¾åƒæ ‡é¢˜å¯¹å’Œæ–‡æœ¬æ•°æ®ã€‚æˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•æ¢¯åº¦æ›´æ–°æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­è¯„ä¼°å„ç§è®¾ç½®ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ¨¡å¼æ€ç»´é“¾æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKOSMOS-1 åœ¨ä»¥ä¸‹æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼š(i) è¯­è¨€ç†è§£ã€ç”Ÿæˆï¼Œç”šè‡³æ— éœ€ OCR çš„ NLPï¼ˆç›´æ¥è¾“å…¥æ–‡æ¡£å›¾åƒï¼‰ï¼Œ(ii) æ„ŸçŸ¥è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å¯¹è¯ã€å›¾åƒå­—å¹•ã€è§†è§‰é—®é¢˜å›ç­”ï¼Œä»¥åŠï¼ˆiiiï¼‰è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å¸¦æœ‰æè¿°çš„å›¾åƒè¯†åˆ«ï¼ˆé€šè¿‡æ–‡æœ¬æŒ‡ä»¤æŒ‡å®šåˆ†ç±»ï¼‰ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒMLLM å¯ä»¥ä»è·¨æ¨¡å¼è½¬ç§»ä¸­å—ç›Šï¼Œå³å°†çŸ¥è¯†ä»è¯­è¨€è½¬ç§»åˆ°å¤šæ¨¡å¼ï¼Œä»¥åŠä»å¤šæ¨¡å¼è½¬ç§»åˆ°è¯­è¨€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº† Raven IQ æµ‹è¯•æ•°æ®é›†ï¼Œç”¨äºè¯Šæ–­ MLLM çš„éè¯­è¨€æ¨ç†èƒ½åŠ›ã€‚

## Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation<sup>poster<sup>

Authors: Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, LINGMING ZHANG

Link: [https://neurips.cc/virtual/2023/poster/72990](https://neurips.cc/virtual/2023/poster/72990)

Abstract:

 Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus â€“ a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.

æ‘˜è¦:

ç¨‹åºç»¼åˆå·²ç»è¢«ç ”ç©¶äº†å¾ˆé•¿æ—¶é—´ï¼Œæœ€è¿‘çš„æ–¹æ³•ä¸»è¦é›†ä¸­äºç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½æ¥ç”Ÿæˆä»£ç ã€‚ç¼–ç¨‹åŸºå‡†ä»¥åŠç²¾é€‰çš„ç»¼åˆé—®é¢˜å’Œæµ‹è¯•ç”¨ä¾‹ç”¨äºè¡¡é‡å„ç§æ³•å­¦ç¡•å£«åœ¨ä»£ç ç»¼åˆæ–¹é¢çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œè¿™äº›æµ‹è¯•ç”¨ä¾‹åœ¨æ•°é‡å’Œè´¨é‡ä¸Šéƒ½å—åˆ°é™åˆ¶ï¼Œæ— æ³•å……åˆ†è¯„ä¼°ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„è¿™ç§é™åˆ¶å¼•å‡ºäº†ä»¥ä¸‹é—®é¢˜ï¼šåœ¨LLMæ—¶ä»£ï¼Œç”Ÿæˆçš„ä»£ç çœŸçš„æ­£ç¡®å—ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† EvalPlusâ€”â€”ä¸€ä¸ªä»£ç ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºä¸¥æ ¼åŸºå‡†æµ‹è¯• LLM ç»¼åˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚ EvalPlus ä½¿ç”¨è‡ªåŠ¨æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨æ–°ç”Ÿæˆçš„å¤§é‡æµ‹è¯•ç”¨ä¾‹æ¥å¢å¼ºç»™å®šçš„è¯„ä¼°æ•°æ®é›†ï¼Œå¹¶ç”±åŸºäº LLM å’ŒåŸºäºçªå˜çš„ç­–ç•¥æä¾›æ”¯æŒã€‚è™½ç„¶ EvalPlus å¾ˆé€šç”¨ï¼Œä½†æˆ‘ä»¬å°†æµè¡Œçš„ HumanEval åŸºå‡†æµ‹è¯•ç”¨ä¾‹æ‰©å±•äº† 80 å€æ¥æ„å»º HumanEval+ã€‚æˆ‘ä»¬å¯¹ 26 ä¸ªæµè¡Œçš„ LLMï¼ˆä¾‹å¦‚ GPT-4 å’Œ ChatGPTï¼‰è¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHumanEval+ èƒ½å¤Ÿæ•è· LLM åˆæˆçš„å¤§é‡ä»¥å‰æœªæ£€æµ‹åˆ°çš„é”™è¯¯ä»£ç ï¼Œå°† pass@k å‡å°‘é«˜è¾¾ 19.3-28.9%ã€‚æˆ‘ä»¬è¿˜ä»¤äººæƒŠè®¶åœ°å‘ç°æµ‹è¯•ä¸å……åˆ†å¯èƒ½å¯¼è‡´æ’åé”™è¯¯ã€‚ä¾‹å¦‚ï¼ŒWizardCoder-CodeLlama å’Œ Phind-CodeLlama ç°åœ¨åœ¨ HumanEval+ ä¸Šçš„æ€§èƒ½éƒ½ä¼˜äº ChatGPTï¼Œè€Œåœ¨ HumanEval ä¸Šå®ƒä»¬éƒ½æ— æ³•åšåˆ°ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä»…è¡¨æ˜ä¹‹å‰æµè¡Œçš„ä»£ç ç»¼åˆè¯„ä¼°ç»“æœå¹¶ä¸èƒ½å‡†ç¡®åæ˜ LLMåœ¨ä»£ç ç»¼åˆæ–¹é¢çš„çœŸå®æ€§èƒ½ï¼Œè€Œä¸”è¿˜ä¸ºé€šè¿‡è‡ªåŠ¨åŒ–æµ‹è¯•æ”¹è¿›æ­¤ç±»ç¼–ç¨‹åŸºå‡†å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚æˆ‘ä»¬åœ¨ https://github.com/evalplus/evalplus ä¸Šå¼€æºäº†æˆ‘ä»¬çš„å·¥å…·ã€å¢å¼ºçš„æ•°æ®é›†ä»¥åŠæ‰€æœ‰ LLM ç”Ÿæˆçš„ä»£ç ï¼Œä»¥ä¿ƒè¿›å’ŒåŠ é€Ÿæœªæ¥çš„ LLM-for-code ç ”ç©¶ã€‚

## Aligning Language Models with Human Preferences via a Bayesian Approach<sup>poster<sup>

Authors: Jiashuo WANG, Haozhao Wang, Shichao Sun, Wenjie Li

Link: [https://neurips.cc/virtual/2023/poster/72170](https://neurips.cc/virtual/2023/poster/72170)

Abstract:

 In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as $\textbf{d-PM}$. Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model. Extensive experiments on two human-centric NLG tasks, i.e., emotional support conversation and integrity ``Rule-of-Thumb'' generation, show that our method consistently exceeds previous SOTA models in both automatic and human evaluations.

æ‘˜è¦:

åœ¨å¯»æ±‚æ¨è¿›ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ç³»ç»Ÿæ—¶ï¼Œç¡®ä¿ NLG æ¨¡å‹ä¸äººç±»åå¥½ä¹‹é—´çš„ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†å®ç°è¿™ç§ä¸€è‡´æ€§ï¼Œå½“å‰æµè¡Œçš„æ–¹æ³•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å’Œæ ¹æ®äººç±»åé¦ˆè¿›è¡Œè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºäººç±»åå¥½çš„ä¸»è§‚æ€§è´¨è€Œäº§ç”Ÿçš„å›ºæœ‰åˆ†æ­§ç»™å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´ NLG æ€§èƒ½æ¶åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥å‰çš„æ–¹æ³•é€šå¸¸ä¾é å¤šæ•°æŠ•ç¥¨æˆ–å¹³å‡æ¥å°†å¤šä¸ªä¸ä¸€è‡´çš„åå¥½åˆå¹¶ä¸ºä¸€ä¸ªåˆå¹¶çš„åå¥½ã€‚å°½ç®¡æ˜“äºç†è§£å’Œæ‰§è¡Œï¼Œä½†æ­¤ç±»æ–¹æ³•æ— æ³•æ•æ‰äººç±»ä¹‹é—´çš„ç»†å¾®å·®åˆ«ï¼Œå¹¶ä¸”å¯èƒ½ä»…ä»£è¡¨ä¸ªä½“çš„ç‰¹å®šå­é›†ï¼Œå› æ­¤ç¼ºä¹å®šé‡æ­ç¤ºäººç±»åå¥½çš„æ™®éæ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé‡‡ç”¨è´å¶æ–¯æ¡†æ¶æ¥è§£é‡Šäººç±»åå¥½ä¹‹é—´çš„åˆ†æ­§åˆ†å¸ƒï¼Œå¹¶è®­ç»ƒåå¥½æ¨¡å‹ï¼Œå¹¶å°†å…¶å‘½åä¸º $\textbf{d-PM}$ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è®­ç»ƒè¿‡ç¨‹æ•ˆç‡ä½ä¸‹ä¸”å¤æ‚ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å»ºè®®åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ç­–ç•¥åˆ©ç”¨ä» d-PM æ¨¡å‹å¾—å‡ºçš„åå¥½åˆ†æ•°æ¥è®­ç»ƒ NLG æ¨¡å‹ã€‚å¯¹ä¸¤ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„ NLG ä»»åŠ¡ï¼ˆå³æƒ…æ„Ÿæ”¯æŒå¯¹è¯å’Œå®Œæ•´æ€§â€œç»éªŒæ³•åˆ™â€ç”Ÿæˆï¼‰è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººç±»è¯„ä¼°æ–¹é¢å§‹ç»ˆä¼˜äºä»¥å‰çš„ SOTA æ¨¡å‹ã€‚

## ChessGPT: Bridging Policy Learning and Language Modeling<sup>poster<sup>

Authors: Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang

Link: [https://neurips.cc/virtual/2023/poster/73461](https://neurips.cc/virtual/2023/poster/73461)

Abstract:

 When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and ChessGPT, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model's chess ability. Experimental results validate our model and dataset's effectiveness. We open source our code, model, and dataset at https://github.com/waterhorse1/ChessGPT.

æ‘˜è¦:

åœ¨è§£å†³å†³ç­–ä»»åŠ¡æ—¶ï¼Œäººç±»é€šå¸¸ä¾èµ–äºä¸¤ä¸ªå…³é”®æ¥æºçš„ä¿¡æ¯ï¼šï¼ˆ1ï¼‰å†å²æ”¿ç­–æ•°æ®ï¼Œæä¾›æ¥è‡ªç¯å¢ƒçš„äº¤äº’å›æ”¾ï¼›ï¼ˆ2ï¼‰è‡ªç„¶è¯­è¨€å½¢å¼çš„åˆ†æè§è§£ï¼Œæ­ç¤ºå®è´µçš„æ€ç»´è¿‡ç¨‹æˆ–æˆ˜ç•¥è€ƒè™‘ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶åªå…³æ³¨ä¸€ä¸ªæ¥æºï¼šä»–ä»¬è¦ä¹ˆä¸“é—¨ä½¿ç”¨å†å²é‡æ’­æ¥ç›´æ¥å­¦ä¹ æ”¿ç­–æˆ–ä»·å€¼å‡½æ•°ï¼Œè¦ä¹ˆä»…åˆ©ç”¨è¯­è¨€è¯­æ–™åº“è¿›è¡Œè¯­è¨€æ¨¡å‹è®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºå¼ºå¤§çš„è‡ªä¸»ä»£ç†åº”è¯¥æ¶µç›–è¿™ä¸¤ä¸ªæ¥æºã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† ChessGPTï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é›†æˆå›½é™…è±¡æ£‹æ¸¸æˆä¸­è¿™ä¸¤ä¸ªæ¥æºçš„æ•°æ®æ¥æ¡¥æ¥ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€å»ºæ¨¡çš„ GPT æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸å›½é™…è±¡æ£‹ç›¸å…³çš„å¤§å‹æ¸¸æˆå’Œè¯­è¨€æ•°æ®é›†ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸¤ä¸ªæ¨¡å‹ç¤ºä¾‹ ChessCLIP å’Œ ChessGPTï¼Œé›†æˆäº†ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€å»ºæ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹çš„å›½é™…è±¡æ£‹èƒ½åŠ›ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨ https://github.com/waterhorse1/ChessGPT å¼€æºæˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ã€‚

## Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models<sup>poster<sup>

Authors: Naoki Egami, Musashi Hinck, Brandon Stewart, Hanying Wei

Link: [https://neurips.cc/virtual/2023/poster/70979](https://neurips.cc/virtual/2023/poster/70979)

Abstract:

 In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. One increasingly common way to annotate documents cheaply at scale is through large language models (LLMs). However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased. We present a new algorithm for using imperfect annotation surrogates for downstream statistical analyses while guaranteeing statistical propertiesâ€”like asymptotic unbiasedness and proper uncertainty quantificationâ€”which are fundamental to CSS research. We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80-90\%. To address this, we build on debiased machine learning to propose the design-based supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without inferential guarantees.

æ‘˜è¦:

åœ¨è®¡ç®—ç¤¾ä¼šç§‘å­¦ï¼ˆCSSï¼‰ä¸­ï¼Œç ”ç©¶äººå‘˜åˆ†ææ–‡æ¡£æ¥è§£é‡Šç¤¾ä¼šå’Œæ”¿æ²»ç°è±¡ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒCSS ç ”ç©¶äººå‘˜é¦–å…ˆè·å–æ–‡æ¡£çš„æ ‡ç­¾ï¼Œç„¶ååœ¨ç¬¬äºŒæ­¥ä¸­ä½¿ç”¨å¯è§£é‡Šçš„å›å½’åˆ†ææ¥è§£é‡Šæ ‡ç­¾ã€‚ä¸€ç§æ—¥ç›Šå¸¸è§çš„å¤§è§„æ¨¡å»‰ä»·æ³¨é‡Šæ–‡æ¡£çš„æ–¹æ³•æ˜¯é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ã€‚ç„¶è€Œï¼Œä¸å…¶ä»–ç”Ÿæˆæ³¨é‡Šçš„å¯æ‰©å±•æ–¹å¼ä¸€æ ·ï¼Œæ­¤ç±»æ›¿ä»£æ ‡ç­¾é€šå¸¸æ˜¯ä¸å®Œç¾ä¸”æœ‰åè§çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œä½¿ç”¨ä¸å®Œç¾çš„æ³¨é‡Šæ›¿ä»£é¡¹è¿›è¡Œä¸‹æ¸¸ç»Ÿè®¡åˆ†æï¼ŒåŒæ—¶ä¿è¯ç»Ÿè®¡ç‰¹æ€§ï¼ˆä¾‹å¦‚æ¸è¿‘æ— åæ€§å’Œé€‚å½“çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼‰ï¼Œè¿™äº›ç‰¹æ€§æ˜¯ CSS ç ”ç©¶çš„åŸºç¡€ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨ä¸‹æ¸¸ç»Ÿè®¡åˆ†æä¸­ç›´æ¥ä½¿ç”¨æ›¿ä»£æ ‡ç­¾ä¼šå¯¼è‡´å¾ˆå¤§çš„åå·®å’Œæ— æ•ˆçš„ç½®ä¿¡åŒºé—´ï¼Œå³ä½¿æ›¿ä»£å‡†ç¡®åº¦é«˜è¾¾ 80-90%ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å»åæœºå™¨å­¦ä¹ çš„åŸºç¡€ä¸Šæå‡ºäº†åŸºäºè®¾è®¡çš„ç›‘ç£å­¦ä¹ ï¼ˆDSLï¼‰ä¼°è®¡å™¨ã€‚ DSL é‡‡ç”¨åŒé‡é²æ£’ç¨‹åºå°†ä»£ç†æ ‡ç­¾ä¸å°‘é‡é«˜è´¨é‡ã€é»„é‡‘æ ‡å‡†æ ‡ç­¾ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ§åˆ¶é»„é‡‘æ ‡å‡†æ ‡ç­¾çš„æŠ½æ ·æ–‡æ¡£çš„æ¦‚ç‡ï¼Œä¿è¯ä¸‹æ¸¸ç»Ÿè®¡åˆ†æçš„æœ‰æ•ˆæ¨æ–­ï¼Œå³ä½¿æ›¿ä»£è€…æœ‰ä»»æ„åè§å¹¶ä¸”ä¸éœ€è¦ä¸¥æ ¼çš„å‡è®¾ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æå’Œå®éªŒç»“æœéƒ½è¡¨æ˜ï¼ŒDSL æä¾›äº†æœ‰æ•ˆçš„ç»Ÿè®¡æ¨æ–­ï¼ŒåŒæ—¶å®ç°äº†ä¸ä»…æ³¨é‡é¢„æµ‹è€Œæ²¡æœ‰æ¨æ–­ä¿è¯çš„ç°æœ‰æ›¿ä»£æ–¹æ¡ˆç›¸å½“çš„å‡æ–¹æ ¹è¯¯å·®ã€‚

## Extensible Prompts for Language Models on Zero-shot Language Style Customization<sup>poster<sup>

Authors: Tao Ge, Hu Jing, Li Dong, Shaoguang Mao, Yan Xia, Xun Wang, Si-Qing Chen, Furu Wei

Link: [https://neurips.cc/virtual/2023/poster/70594](https://neurips.cc/virtual/2023/poster/70594)

Abstract:

 We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words. Registering new imaginary words allows us to instruct the LLM to comprehend concepts that are difficult to describe with NL words, thereby making a prompt more descriptive. Also, these imaginary words are designed to be out-of-distribution (OOD) robust so that they can be (re)used like NL words in various prompts, distinguishing X-Prompt from soft prompt that is for fitting in-distribution data. We propose context-augmented learning (CAL) to learn imaginary words for general usability, enabling them to work properly in OOD (unseen) prompts. We experiment X-Prompt for zero-shot language style customization as a case study. The promising results of X-Prompt demonstrate its potential to facilitate advanced interaction beyond the natural language interface, bridging the communication gap between humans and LLMs.

æ‘˜è¦:

æˆ‘ä»¬æå‡ºäº†å¯æ‰©å±•æç¤ºï¼ˆX-Promptï¼‰ï¼Œç”¨äºæç¤ºè¶…è¶Šè‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ X-Prompt ä¸ä»…æ•™æˆæ³•å­¦ç¡•å£« NLï¼Œè¿˜æ•™æˆå¯æ‰©å±•çš„æƒ³è±¡è¯æ±‡è¯æ±‡ã€‚æ³¨å†Œæ–°çš„è™šæ„å•è¯ä½¿æˆ‘ä»¬èƒ½å¤ŸæŒ‡å¯¼æ³•å­¦ç¡•å£«ç†è§£éš¾ä»¥ç”¨ NL å•è¯æè¿°çš„æ¦‚å¿µï¼Œä»è€Œä½¿æç¤ºæ›´å…·æè¿°æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›è™šæ„å•è¯è¢«è®¾è®¡ä¸ºåˆ†å¸ƒå¤– (OOD) é²æ£’æ€§ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥åœ¨å„ç§æç¤ºä¸­åƒ NL å•è¯ä¸€æ ·ï¼ˆé‡æ–°ï¼‰ä½¿ç”¨ï¼Œä»è€Œå°† X-Prompt ä¸ç”¨äºæ‹Ÿåˆåˆ†å¸ƒå†…æ•°æ®çš„è½¯æç¤ºåŒºåˆ†å¼€æ¥ã€‚æˆ‘ä»¬æå‡ºä¸Šä¸‹æ–‡å¢å¼ºå­¦ä¹ ï¼ˆCALï¼‰æ¥å­¦ä¹ è™šæ„å•è¯ä»¥å®ç°ä¸€èˆ¬å¯ç”¨æ€§ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨ OODï¼ˆçœ‹ä¸è§çš„ï¼‰æç¤ºä¸­æ­£å¸¸å·¥ä½œã€‚æˆ‘ä»¬å°† X-Prompt è¿›è¡Œé›¶æ ·æœ¬è¯­è¨€é£æ ¼å®šåˆ¶å®éªŒä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ã€‚ X-Prompt ä»¤äººé¼“èˆçš„ç»“æœè¯æ˜äº†å…¶ä¿ƒè¿›è¶…è¶Šè‡ªç„¶è¯­è¨€ç•Œé¢çš„é«˜çº§äº¤äº’çš„æ½œåŠ›ï¼Œå¼¥åˆäº†äººç±»å’Œæ³•å­¦ç¡•å£«ä¹‹é—´çš„æ²Ÿé€šå·®è·ã€‚

## LLM-Pruner: On the Structural Pruning of Large Language Models<sup>poster<sup>

Authors: Xinyin Ma, Gongfan Fang, Xinchao Wang

Link: [https://neurips.cc/virtual/2023/poster/72074](https://neurips.cc/virtual/2023/poster/72074)

Abstract:

 Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code will be made public.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¦‚æ­¤ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›é€šå¸¸ä¼´éšç€å·¨å¤§çš„æ¨¡å‹å¤§å°ï¼Œè¿™åœ¨éƒ¨ç½²ã€æ¨ç†å’Œè®­ç»ƒé˜¶æ®µéƒ½å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç”±äºLLMæ˜¯é€šç”¨ä»»åŠ¡æ±‚è§£å™¨ï¼Œæˆ‘ä»¬ä»¥ä»»åŠ¡æ— å…³çš„æ–¹å¼æ¢ç´¢å…¶å‹ç¼©ï¼Œæ—¨åœ¨ä¿ç•™åŸå§‹LLMçš„å¤šä»»åŠ¡æ±‚è§£å’Œè¯­è¨€ç”Ÿæˆèƒ½åŠ›ã€‚å®ç°è¿™ä¸€ç›®æ ‡çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯æ³•å­¦ç¡•å£«çš„è®­ç»ƒè¯­æ–™åº“è§„æ¨¡å·¨å¤§ï¼Œè¿™ä½¿å¾—æ•°æ®ä¼ è¾“å’Œæ¨¡å‹åè®­ç»ƒéƒ½å˜å¾—è¿‡äºç¹é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªçº¦æŸçš„èŒƒå›´å†…è§£å†³æ³•å­¦ç¡•å£«çš„å‹ç¼©é—®é¢˜ï¼šä¸ä»»åŠ¡æ— å…³å¹¶æœ€å¤§é™åº¦åœ°å‡å°‘å¯¹åŸå§‹è®­ç»ƒæ•°æ®é›†çš„ä¾èµ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åä¸ºLLM-prunerï¼Œé‡‡ç”¨ç»“æ„å‰ªæï¼Œæ ¹æ®æ¢¯åº¦ä¿¡æ¯é€‰æ‹©æ€§åœ°å»é™¤éå…³é”®è€¦åˆç»“æ„ï¼Œæœ€å¤§é™åº¦åœ°ä¿ç•™LLMçš„å¤§éƒ¨åˆ†åŠŸèƒ½ã€‚ä¸ºæ­¤ï¼Œé€šè¿‡è°ƒä¼˜æŠ€æœ¯LoRAï¼Œä»…éœ€3å°æ—¶ï¼Œä»…éœ€è¦50Kæ•°æ®ï¼Œå³å¯æœ‰æ•ˆæ¢å¤å‰ªææ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ LLaMAã€Vicuna å’Œ ChatGLM ç­‰ä¸‰ä¸ª LLM ä¸ŠéªŒè¯äº† LLM-Prunerï¼Œå¹¶è¯æ˜å‹ç¼©æ¨¡å‹åœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œç”Ÿæˆæ–¹é¢ä»ç„¶è¡¨ç°å‡ºä»¤äººæ»¡æ„çš„èƒ½åŠ›ã€‚è¯¥ä»£ç å°†è¢«å…¬å¼€ã€‚

## Large Language Models Are Zero-Shot Time Series Forecasters<sup>poster<sup>

Authors: Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Wilson

Link: [https://neurips.cc/virtual/2023/poster/70543](https://neurips.cc/virtual/2023/poster/70543)

Abstract:

 By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions.  While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.

æ‘˜è¦:

é€šè¿‡å°†æ—¶é—´åºåˆ—ç¼–ç ä¸ºä¸€ä¸²æ•°å­—ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ä¸ºæ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€‚é€šè¿‡å¼€å‘è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç° GPT-3 å’Œ LLaMA-2 ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å¯ä»¥ä»¤äººæƒŠè®¶åœ°ä»¥é›¶æ ·æœ¬æ¨æ–­æ—¶é—´åºåˆ—ï¼Œå…¶æ°´å¹³ç›¸å½“äºæˆ–è¶…è¿‡åœ¨ä¸‹æ¸¸è®­ç»ƒçš„ä¸“ç”¨æ—¶é—´åºåˆ—æ¨¡å‹çš„æ€§èƒ½ä»»åŠ¡ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ç§æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†æœ‰æ•ˆæ ‡è®°æ—¶é—´åºåˆ—æ•°æ®å¹¶å°†æ ‡è®°ä¸Šçš„ç¦»æ•£åˆ†å¸ƒè½¬æ¢ä¸ºè¿ç»­å€¼ä¸Šçš„é«˜åº¦çµæ´»çš„å¯†åº¦çš„ç¨‹åºã€‚æˆ‘ä»¬è®¤ä¸ºæ—¶é—´åºåˆ—æ³•å­¦ç¡•å£«çš„æˆåŠŸæºäºå®ƒä»¬è‡ªç„¶åœ°è¡¨ç¤ºå¤šæ¨¡æ€åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œå†åŠ ä¸Šç®€å•æ€§å’Œé‡å¤æ€§çš„åå·®ï¼Œè¿™ä¸è®¸å¤šæ—¶é—´åºåˆ—ä¸­çš„æ˜¾ç€ç‰¹å¾ç›¸ä¸€è‡´ï¼Œä¾‹å¦‚é‡å¤çš„å­£èŠ‚æ€§è¶‹åŠ¿ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ³•å­¦ç¡•å£«å¦‚ä½•è‡ªç„¶åœ°å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œè€Œæ— éœ€é€šè¿‡éæ•°å­—æ–‡æœ¬è¿›è¡Œæ’è¡¥ï¼Œå®¹çº³æ–‡æœ¬è¾…åŠ©ä¿¡æ¯ï¼Œå¹¶å›ç­”é—®é¢˜ä»¥å¸®åŠ©è§£é‡Šé¢„æµ‹ã€‚è™½ç„¶æˆ‘ä»¬å‘ç°å¢åŠ æ¨¡å‹å¤§å°é€šå¸¸ä¼šæé«˜æ—¶é—´åºåˆ—çš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬è¡¨æ˜ GPT-4 çš„æ€§èƒ½å¯èƒ½æ¯” GPT-3 å·®ï¼Œå› ä¸ºå®ƒå¯¹æ•°å­—è¿›è¡Œæ ‡è®°çš„æ–¹å¼ä»¥åŠè¾ƒå·®çš„ä¸ç¡®å®šæ€§æ ¡å‡†ï¼Œè¿™å¯èƒ½æ˜¯ RLHF ç­‰å¯¹é½å¹²é¢„çš„ç»“æœã€‚

## Meta-Adapter: An Online Few-shot Learner for Vision-Language Model<sup>poster<sup>

Authors: cheng cheng, Lin Song, Ruoyi Xue, Hang Wang, Hongbin Sun, Yixiao Ge, Ying Shan

Link: [https://neurips.cc/virtual/2023/poster/71536](https://neurips.cc/virtual/2023/poster/71536)

Abstract:

 The contrastive vision-language pre-training, known as CLIP, demonstrates remarkable potential in perceiving open-world visual concepts, enabling effective zero-shot image recognition.  Nevertheless, few-shot learning methods based on CLIP typically require offline fine-tuning of the parameters on few-shot samples, resulting in longer inference time and the risk of overfitting in certain domains.  To tackle these challenges, we propose the Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features guided by the few-shot samples in an online manner.  With a few training samples, our method can enable effective few-shot learning capabilities and generalize to unseen data or tasks without additional fine-tuning, achieving competitive performance and high efficiency.  Without bells and whistles, our approach outperforms the state-of-the-art online few-shot learning method by an average of 3.6\% on eight image classification datasets with higher inference speed.  Furthermore, our model is simple and flexible, serving as a plug-and-play module directly applicable to downstream tasks.  Without further fine-tuning, Meta-Adapter obtains notable performance improvements in open-vocabulary object detection and segmentation tasks.

æ‘˜è¦:

å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆç§°ä¸º CLIPï¼‰åœ¨æ„ŸçŸ¥å¼€æ”¾ä¸–ç•Œè§†è§‰æ¦‚å¿µã€å®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬å›¾åƒè¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŸºäºCLIPçš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¯¹å°‘æ ·æœ¬æ ·æœ¬ä¸Šçš„å‚æ•°è¿›è¡Œç¦»çº¿å¾®è°ƒï¼Œå¯¼è‡´æ¨ç†æ—¶é—´æ›´é•¿ï¼Œå¹¶ä¸”åœ¨æŸäº›é¢†åŸŸå­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† Meta-Adapterï¼Œä¸€ç§è½»é‡çº§æ®‹å·®å¼é€‚é…å™¨ï¼Œä»¥åœ¨çº¿æ–¹å¼ç»†åŒ–ç”±å°‘é‡æ ·æœ¬å¼•å¯¼çš„ CLIP ç‰¹å¾ã€‚é€šè¿‡ä¸€äº›è®­ç»ƒæ ·æœ¬ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°æœ‰æ•ˆçš„å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶æ³›åŒ–åˆ°æœªè§è¿‡çš„æ•°æ®æˆ–ä»»åŠ¡ï¼Œè€Œæ— éœ€é¢å¤–çš„å¾®è°ƒï¼Œä»è€Œå®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½å’Œé«˜æ•ˆç‡ã€‚æ²¡æœ‰èŠ±é‡Œèƒ¡å“¨çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…«ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šä»¥æ›´é«˜çš„æ¨ç†é€Ÿåº¦å¹³å‡ä¼˜äºæœ€å…ˆè¿›çš„åœ¨çº¿å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³• 3.6%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç®€å•çµæ´»ï¼Œä½œä¸ºå³æ’å³ç”¨æ¨¡å—ç›´æ¥é€‚ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚æ— éœ€è¿›ä¸€æ­¥å¾®è°ƒï¼ŒMeta-Adapter åœ¨å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸­è·å¾—æ˜¾ç€çš„æ€§èƒ½æ”¹è¿›ã€‚

## AdaPlanner: Adaptive Planning from Feedback with Language Models<sup>poster<sup>

Authors: Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang

Link: [https://neurips.cc/virtual/2023/poster/70298](https://neurips.cc/virtual/2023/poster/70298)

Abstract:

 Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘è¯æ˜äº†ä½œä¸ºé¡ºåºå†³ç­–ä»»åŠ¡çš„è‡ªä¸»ä»£ç†çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•è¦ä¹ˆåœ¨æ²¡æœ‰è®¡åˆ’çš„æƒ…å†µä¸‹è´ªå©ªåœ°é‡‡å–è¡ŒåŠ¨ï¼Œè¦ä¹ˆä¾èµ–äºä¸é€‚åº”ç¯å¢ƒåé¦ˆçš„é™æ€è®¡åˆ’ã€‚å› æ­¤ï¼ŒLLM ä»£ç†çš„é¡ºåºå†³ç­–æ€§èƒ½éšç€é—®é¢˜å¤æ‚æ€§å’Œè®¡åˆ’èŒƒå›´çš„å¢åŠ è€Œé€€åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é—­ç¯æ–¹æ³• AdaPlannerï¼Œå®ƒå…è®¸ LLM ä»£ç†æ ¹æ®ç¯å¢ƒåé¦ˆè‡ªé€‚åº”åœ°å®Œå–„å…¶è‡ªè¡Œç”Ÿæˆçš„è®¡åˆ’ã€‚åœ¨ AdaPlanner ä¸­ï¼ŒLLM ä»£ç†é€šè¿‡è®¡åˆ’å†…å’Œè®¡åˆ’å¤–ç»†åŒ–ç­–ç•¥æ ¹æ®åé¦ˆè‡ªé€‚åº”åœ°ç»†åŒ–å…¶è®¡åˆ’ã€‚ä¸ºäº†å‡è½»å¹»è§‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä»£ç å¼çš„ LLM æç¤ºç»“æ„ï¼Œæœ‰åŠ©äºè·¨å„ç§ä»»åŠ¡ã€ç¯å¢ƒå’Œä»£ç†åŠŸèƒ½ç”Ÿæˆè®¡åˆ’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŠ€èƒ½å‘ç°æœºåˆ¶ï¼Œåˆ©ç”¨æˆåŠŸçš„è®¡åˆ’ä½œä¸ºå°‘æ•°æ ·æœ¬ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡æ›´å°‘çš„ä»»åŠ¡æ¼”ç¤ºæ¥è®¡åˆ’å’Œå®Œå–„ã€‚æˆ‘ä»¬åœ¨ ALFWorld å’Œ MiniWoB++ ç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒAdaPlanner çš„æ€§èƒ½æ¯”æœ€å…ˆè¿›çš„åŸºçº¿é«˜å‡º 3.73% å’Œ 4.11%ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ·æœ¬æ•°é‡åˆ†åˆ«å‡å°‘äº† 2 å€å’Œ 600 å€ã€‚ AdaPlanner çš„å®ç°å¯ä»¥åœ¨ https://github.com/haotiansun14/AdaPlanner è·å–ã€‚

## Evaluating Cognitive Maps and Planning in Large Language Models with CogEval<sup>poster<sup>

Authors: Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi, Robert Ness, Jonathan Larson

Link: [https://neurips.cc/virtual/2023/poster/71431](https://neurips.cc/virtual/2023/poster/71431)

Abstract:

 Recently an influx of studies claims emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in LLMs. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and falling in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.

æ‘˜è¦:

æœ€è¿‘å¤§é‡ç ”ç©¶å£°ç§°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ–°å…´çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ä¾èµ–äºè½¶äº‹ï¼Œå¿½è§†äº†è®­ç»ƒé›†çš„æ±¡æŸ“ï¼Œæˆ–è€…ç¼ºä¹æ¶‰åŠå¤šä¸ªä»»åŠ¡ã€æ§åˆ¶æ¡ä»¶ã€å¤šæ¬¡è¿­ä»£å’Œç»Ÿè®¡ç¨³å¥æ€§æµ‹è¯•çš„ç³»ç»Ÿè¯„ä¼°ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬åšå‡ºäº†ä¸¤ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡º CogEvalï¼Œè¿™æ˜¯ä¸€ç§å—è®¤çŸ¥ç§‘å­¦å¯å‘çš„åè®®ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°æ³•å­¦ç¡•å£«çš„è®¤çŸ¥èƒ½åŠ›ã€‚å¯ä»¥éµå¾ªCogEvalåè®®æ¥è¯„ä¼°å„ç§èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æŒ‰ç…§ CogEval ç³»ç»Ÿåœ°è¯„ä¼°å…«ä¸ª LLM çš„è®¤çŸ¥å›¾å’Œè§„åˆ’èƒ½åŠ›ï¼ˆOpenAI GPT-4ã€GPT-3.5-turbo-175Bã€davinci-003-175Bã€Google Bardã€Cohere-xlarge-52.4Bã€Anthropic Claude- 1-52Bã€LLaMA-13B å’Œ Alpaca-7Bï¼‰ã€‚æˆ‘ä»¬çš„ä»»åŠ¡æç¤ºåŸºäºäººä½“å®éªŒï¼Œè¿™æ—¢æä¾›äº†è¯„ä¼°è®¡åˆ’çš„æ—¢å®šç»“æ„æœ‰æ•ˆæ€§ï¼Œåˆä¸å­˜åœ¨äºæ³•å­¦ç¡•å£«åŸ¹è®­é›†ä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶æ³•å­¦ç¡•å£«åœ¨ä¸€äº›ç»“æ„è¾ƒç®€å•çš„è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„èƒ½åŠ›ï¼Œä½†ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†è§„åˆ’ä»»åŠ¡ä¸­æƒŠäººçš„å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬æ— æ•ˆè½¨è¿¹çš„å¹»è§‰å’Œé™·å…¥å¾ªç¯ã€‚è¿™äº›å‘ç°å¹¶ä¸æ”¯æŒæ³•å­¦ç¡•å£«å…·æœ‰ç°æˆçš„è®¡åˆ’èƒ½åŠ›çš„æƒ³æ³•ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæ³•å­¦ç¡•å£«ä¸ç†è§£è§„åˆ’é—®é¢˜èƒŒåçš„æ½œåœ¨å…³ç³»ç»“æ„ï¼ˆç§°ä¸ºè®¤çŸ¥å›¾ï¼‰ï¼Œå¹¶ä¸”æ— æ³•æ ¹æ®åº•å±‚ç»“æ„å±•å¼€ç›®æ ‡å¯¼å‘çš„è½¨è¿¹ã€‚è®¨è®ºäº†åº”ç”¨çš„å½±å“å’Œæœªæ¥çš„æ–¹å‘ã€‚

## Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning<sup>poster<sup>

Authors: Xiaoqian Wu, Yong-Lu Li, Jianhua Sun, Cewu Lu

Link: [https://neurips.cc/virtual/2023/poster/71665](https://neurips.cc/virtual/2023/poster/71665)

Abstract:

 Human reasoning can be understood as a cooperation between the intuitive, associative "System-1'' and the deliberative, logical "System-2''. For existing System-1-like methods in visual activity understanding, it is crucial to integrate System-2 processing to improve explainability, generalization, and data efficiency. One possible path of activity reasoning is building a symbolic system composed of symbols and rules, where one rule connects multiple symbols, implying human knowledge and reasoning abilities.Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted andchecked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation.Our method shows superiority in extensive activity understanding tasks. Code and data are available at https://mvig-rhos.com/symbol_llm.

æ‘˜è¦:

äººç±»æ¨ç†å¯ä»¥ç†è§£ä¸ºç›´è§‰çš„ã€è”æƒ³çš„â€œç³»ç»Ÿ 1â€å’Œæ·±æ€ç†Ÿè™‘çš„ã€é€»è¾‘çš„â€œç³»ç»Ÿ 2â€ä¹‹é—´çš„åˆä½œã€‚å¯¹äºè§†è§‰æ´»åŠ¨ç†è§£ä¸­ç°æœ‰çš„ç±»ä¼¼ System-1 çš„æ–¹æ³•ï¼Œé›†æˆ System-2 å¤„ç†ä»¥æé«˜å¯è§£é‡Šæ€§ã€æ³›åŒ–æ€§å’Œæ•°æ®æ•ˆç‡è‡³å…³é‡è¦ã€‚æ´»åŠ¨æ¨ç†çš„ä¸€ç§å¯èƒ½è·¯å¾„æ˜¯å»ºç«‹ä¸€ä¸ªç”±ç¬¦å·å’Œè§„åˆ™ç»„æˆçš„ç¬¦å·ç³»ç»Ÿï¼Œä¸€ä¸ªè§„åˆ™è¿æ¥å¤šä¸ªç¬¦å·ï¼Œæš—ç¤ºç€äººç±»çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚ä»¥å‰çš„æ–¹æ³•å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†å­˜åœ¨ç¼ºé™·ï¼Œæ‰‹å·¥åˆ¶ä½œçš„ç¬¦å·æœ‰é™ï¼Œè§†è§‰çš„è§„åˆ™æœ‰é™ã€‚åŸºäºæ³¨é‡Šï¼Œæœªèƒ½æ¶µç›–å¤æ‚çš„æ´»åŠ¨æ¨¡å¼å¹¶ä¸”ç¼ºä¹ç»„åˆæ¦‚æ‹¬ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¬¦å·ç³»ç»Ÿï¼Œå®ƒå…·æœ‰ä¸¤ä¸ªç†æƒ³çš„é‡è¦å±æ€§ï¼šå¹¿æ³›è¦†ç›–çš„ç¬¦å·å’Œç†æ€§çš„è§„åˆ™ã€‚é€šè¿‡æ‰‹åŠ¨æ³¨é‡Šæ”¶é›†å¤§é‡äººç±»çŸ¥è¯†æ¥å®ä¾‹åŒ–è¿™ä¸ªç¬¦å·ç³»ç»Ÿçš„æˆæœ¬å¾ˆé«˜ã€‚ç›¸åï¼Œæˆ‘ä»¬åˆ©ç”¨ LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„æœ€æ–°è¿›å±•ä½œä¸ºä¸¤ä¸ªç†æƒ³å±æ€§çš„è¿‘ä¼¼ï¼Œå³æ¥è‡ªå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¬¦å·ï¼ˆSymbol-LLMï¼‰ã€‚ç„¶åï¼Œç»™å®šå›¾åƒï¼Œä»å›¾åƒä¸­æå–è§†è§‰å†…å®¹å¹¶è¿›è¡Œæ£€æŸ¥ï¼Œå¹¶é€šè¿‡æ¨¡ç³Šé€»è¾‘è®¡ç®—æ ¹æ®è§„åˆ™æ¨ç†å‡ºæ´»åŠ¨è¯­ä¹‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›çš„æ´»åŠ¨ç†è§£ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºä¼˜è¶Šæ€§ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ https://mvig-rhos.com/symbol_llm è·å–ã€‚

## Lexinvariant Language Models<sup>poster<sup>

Authors: Qian Huang, Eric Zelikman, Sarah Chen, Yuhuai Wu, Gregory Valiant, Percy Liang

Link: [https://neurips.cc/virtual/2023/poster/71847](https://neurips.cc/virtual/2023/poster/71847)

Abstract:

 Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \textit{a priori} identity of any token. To answer this, we study \textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gaussian vectors, such that each token maps to the same representation within each sequence but different representations across sequences. Empirically, we demonstrate that it can indeed attain perplexity comparable to that of a standard language model, given a sufficiently long context. We further explore two properties of the lexinvariant language models: First, given text generated from a substitution cipher of English, it implicitly implements Bayesian in-context deciphering and infers the mapping to the underlying real tokens with high accuracy. Second, it has on average 4X better accuracy over synthetic in-context reasoning tasks. Finally, we discuss regularizing standard language models towards lexinvariance and potential practical applications.

æ‘˜è¦:

æ ‡è®°åµŒå…¥æ˜¯ä»ç¦»æ•£è¯æ±‡ç¬¦å·åˆ°è¿ç»­å‘é‡çš„æ˜ å°„ï¼Œæ˜¯ä»»ä½•è¯­è¨€æ¨¡å‹ (LM) çš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œè¯æ±‡ç¬¦å·çš„å«ä¹‰ä¹Ÿå¯ä»¥é€šè¿‡å®ƒä»¬åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„ç»“æ„ä½œç”¨æ¥ç¡®å®šç”šè‡³é‡æ–°å®šä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é—®ï¼šè¯­è¨€æ¨¡å‹æ˜¯å¦æœ‰å¯èƒ½åœ¨æ²¡æœ‰ \emph{any} å›ºå®šæ ‡è®°åµŒå…¥çš„æƒ…å†µä¸‹ä¿æŒé«˜æ€§èƒ½ï¼Ÿè¿™æ ·çš„è¯­è¨€æ¨¡å‹å¿…é¡»å®Œå…¨ä¾èµ–äºä¸Šä¸‹æ–‡ä¸­æ ‡è®°çš„å…±ç°å’Œé‡å¤ï¼Œè€Œä¸æ˜¯ä»»ä½•æ ‡è®°çš„ \textit{å…ˆéªŒ} èº«ä»½ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº† \textit{lexinvariant} è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯¹äºè¯æ±‡ç¬¦å·æ˜¯ä¸å˜çš„ï¼Œå› æ­¤åœ¨å®è·µä¸­ä¸éœ€è¦å›ºå®šçš„æ ‡è®°åµŒå…¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬å¯ä»¥æ„é€ ä¸€ä¸ªè¯æ³•å˜é‡ LMï¼Œä»¥ä¸Šä¸‹æ–‡é•¿åº¦çš„å¤šé¡¹å¼ç»Ÿä¸€é€Ÿç‡æ”¶æ•›åˆ°çœŸå®çš„è¯­è¨€æ¨¡å‹ï¼Œå¸¸æ•°å› å­åœ¨è¯æ±‡å¤§å°ä¸Šå‘ˆæ¬¡çº¿æ€§ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æ„å»ºè¯æ³•å˜é‡ LMï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨éšæœºé«˜æ–¯å‘é‡å¯¹æ ‡è®°è¿›è¡Œç¼–ç ï¼Œä»¥ä¾¿æ¯ä¸ªæ ‡è®°æ˜ å°„åˆ°æ¯ä¸ªåºåˆ—å†…çš„ç›¸åŒè¡¨ç¤ºï¼Œä½†æ˜ å°„åˆ°åºåˆ—ä¹‹é—´çš„ä¸åŒè¡¨ç¤ºã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåœ¨è¶³å¤Ÿé•¿çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå®ƒç¡®å®å¯ä»¥è¾¾åˆ°ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ç›¸å½“çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢è¯æ³•å˜é‡è¯­è¨€æ¨¡å‹çš„ä¸¤ä¸ªå±æ€§ï¼šé¦–å…ˆï¼Œç»™å®šä»è‹±è¯­æ›¿æ¢å¯†ç ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå®ƒéšå¼å®ç°è´å¶æ–¯ä¸Šä¸‹æ–‡è§£å¯†ï¼Œå¹¶é«˜ç²¾åº¦åœ°æ¨æ–­åˆ°åº•å±‚çœŸå®æ ‡è®°çš„æ˜ å°„ã€‚å…¶æ¬¡ï¼Œå®ƒçš„å‡†ç¡®åº¦å¹³å‡æ¯”ç»¼åˆä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡é«˜å‡º 4 å€ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºé’ˆå¯¹è¯æ³•å˜åŒ–å’Œæ½œåœ¨çš„å®é™…åº”ç”¨æ­£åˆ™åŒ–æ ‡å‡†è¯­è¨€æ¨¡å‹ã€‚

## InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning<sup>poster<sup>

Authors: Wenliang Dai, Junnan Li, DONGXU LI, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, Steven Hoi

Link: [https://neurips.cc/virtual/2023/poster/70085](https://neurips.cc/virtual/2023/poster/70085)

Abstract:

 Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-source.

æ‘˜è¦:

å¤§è§„æ¨¡é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´å·²æˆåŠŸåˆ›å»ºå…·æœ‰å¹¿æ³›èƒ½åŠ›çš„é€šç”¨è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºé¢å¤–çš„è§†è§‰è¾“å…¥å¯¼è‡´ä¸°å¯Œçš„è¾“å…¥åˆ†å¸ƒå’Œä»»åŠ¡å¤šæ ·æ€§ï¼Œæ„å»ºé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒå·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´ä»å¾…æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åŸºäºé¢„è®­ç»ƒçš„ BLIP-2 æ¨¡å‹å¯¹è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒä¼˜è¿›è¡Œäº†ç³»ç»Ÿã€å…¨é¢çš„ç ”ç©¶ã€‚æˆ‘ä»¬æ”¶é›†äº† 26 ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ï¼Œæ¶µç›–å„ç§ä»»åŠ¡å’ŒåŠŸèƒ½ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæŒ‡ä»¤è°ƒä¼˜æ ¼å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæŒ‡ä»¤æ„ŸçŸ¥æŸ¥è¯¢è½¬æ¢å™¨ï¼Œå®ƒæå–é€‚åˆç»™å®šæŒ‡ä»¤çš„ä¿¡æ¯ç‰¹å¾ã€‚ InstructBLIP åœ¨ 13 ä¸ªä¿ç•™æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨æ‰€æœ‰ 13 ä¸ªä¿ç•™æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¤§å¤§ä¼˜äº BLIP-2 å’Œæ›´å¤§çš„ Flamingo æ¨¡å‹ã€‚å½“å¯¹å„ä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿèƒ½å¸¦æ¥æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œå¸¦æœ‰å›¾åƒä¸Šä¸‹æ–‡çš„ ScienceQA é—®é¢˜çš„å‡†ç¡®ç‡ä¸º 90.7%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®šæ€§åœ°è¯æ˜äº† InstructBLIP ç›¸å¯¹äºå¹¶å‘å¤šæ¨¡æ€æ¨¡å‹çš„ä¼˜åŠ¿ã€‚æ‰€æœ‰ InstructBLIP æ¨¡å‹éƒ½æ˜¯å¼€æºçš„ã€‚

## LayoutPrompter: Awaken the Design Ability of Large Language Models<sup>poster<sup>

Authors: Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang Yang, Jian-Guang Lou, Dongmei Zhang

Link: [https://neurips.cc/virtual/2023/poster/72326](https://neurips.cc/virtual/2023/poster/72326)

Abstract:

 Conditional graphic layout generation, which automatically maps user constraints to high-quality layouts, has attracted widespread attention today. Although recent works have achieved promising performance, the lack of versatility and data efficiency hinders their practical applications. In this work, we propose LayoutPrompter, which leverages large language models (LLMs) to address the above problems through in-context learning. LayoutPrompter is made up of three key components, namely input-output serialization, dynamic exemplar selection and layout ranking. Specifically, the input-output serialization component meticulously designs the input and output formats for each layout generation task. Dynamic exemplar selection is responsible for selecting the most helpful prompting exemplars for a given input. And a layout ranker is used to pick the highest quality layout from multiple outputs of LLMs. We conduct experiments on all existing layout generation tasks using four public datasets. Despite the simplicity of our approach, experimental results show that LayoutPrompter can compete with or even outperform state-of-the-art approaches on these tasks without any model training or fine-tuning. This demonstrates the effectiveness of this versatile and training-free approach. In addition, the ablation studies show that LayoutPrompter is significantly superior to the training-based baseline in a low-data regime, further indicating the data efficiency of LayoutPrompter. Our project is available at https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter.

æ‘˜è¦:

æ¡ä»¶å›¾å½¢å¸ƒå±€ç”Ÿæˆï¼Œå¯è‡ªåŠ¨å°†ç”¨æˆ·çº¦æŸæ˜ å°„åˆ°é«˜è´¨é‡å¸ƒå±€ï¼Œå¦‚ä»Šå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡æœ€è¿‘çš„å·¥ä½œå–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œä½†é€šç”¨æ€§å’Œæ•°æ®æ•ˆç‡çš„ç¼ºä¹é˜»ç¢äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† LayoutPrompterï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚ LayoutPrompterç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼Œå³è¾“å…¥è¾“å‡ºåºåˆ—åŒ–ã€åŠ¨æ€ç¤ºä¾‹é€‰æ‹©å’Œå¸ƒå±€æ’åºã€‚å…·ä½“æ¥è¯´ï¼Œè¾“å…¥è¾“å‡ºåºåˆ—åŒ–ç»„ä»¶ä¸ºæ¯ä¸ªå¸ƒå±€ç”Ÿæˆä»»åŠ¡ç²¾å¿ƒè®¾è®¡è¾“å…¥å’Œè¾“å‡ºæ ¼å¼ã€‚åŠ¨æ€æ ·æœ¬é€‰æ‹©è´Ÿè´£ä¸ºç»™å®šè¾“å…¥é€‰æ‹©æœ€æœ‰å¸®åŠ©çš„æç¤ºæ ·æœ¬ã€‚å¸ƒå±€æ’åºå™¨ç”¨äºä»æ³•å­¦ç¡•å£«çš„å¤šä¸ªè¾“å‡ºä¸­æŒ‘é€‰æœ€é«˜è´¨é‡çš„å¸ƒå±€ã€‚æˆ‘ä»¬ä½¿ç”¨å››ä¸ªå…¬å…±æ•°æ®é›†å¯¹æ‰€æœ‰ç°æœ‰å¸ƒå±€ç”Ÿæˆä»»åŠ¡è¿›è¡Œå®éªŒã€‚å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•å¾ˆç®€å•ï¼Œä½†å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸Šï¼ŒLayoutPrompter å¯ä»¥ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç«äº‰ç”šè‡³è¶…è¶Šï¼Œè€Œæ— éœ€ä»»ä½•æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒã€‚è¿™è¯æ˜äº†è¿™ç§å¤šåŠŸèƒ½ä¸”æ— éœ€åŸ¹è®­çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä½æ•°æ®æƒ…å†µä¸‹ï¼ŒLayoutPrompter æ˜¾ç€ä¼˜äºåŸºäºè®­ç»ƒçš„åŸºçº¿ï¼Œè¿›ä¸€æ­¥è¡¨æ˜ LayoutPrompter çš„æ•°æ®æ•ˆç‡ã€‚æˆ‘ä»¬çš„é¡¹ç›®ä½äº https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompterã€‚

## Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization<sup>poster<sup>

Authors: Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee

Link: [https://neurips.cc/virtual/2023/poster/72931](https://neurips.cc/virtual/2023/poster/72931)

Abstract:

 Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern.  Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase.To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) â€“ a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage.We employ PEQA-tuning for task-specific adaptation on LLMs with up to $65$ billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶é«˜å†…å­˜éœ€æ±‚å’Œè®¡ç®—æˆæœ¬è€Œé¢ä¸´ç€å¾®è°ƒå’Œéƒ¨ç½²çš„æŒ‘æˆ˜ã€‚è™½ç„¶å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ–¹æ³•æ—¨åœ¨å‡å°‘å¾®è°ƒæœŸé—´ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜ä½¿ç”¨é‡ï¼Œä½†é¢„è®­ç»ƒçš„ LLM æƒé‡çš„å›ºæœ‰å¤§å°ä»ç„¶æ˜¯ä¸€ä¸ªç´§è¿«çš„é—®é¢˜ã€‚å°½ç®¡é‡åŒ–æŠ€æœ¯è¢«å¹¿æ³›æå‡ºæ¥ç¼“è§£å†…å­˜éœ€æ±‚å¹¶åŠ é€Ÿ LLM æ¨ç†ï¼Œä½†è¿™äº›æŠ€æœ¯å¤§å¤šæ•°éƒ½æ˜¯é’ˆå¯¹éƒ¨ç½²é˜¶æ®µçš„ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†å‚æ•°é«˜æ•ˆå’Œé‡åŒ–æ„ŸçŸ¥é€‚åº”ï¼ˆPEQAï¼‰â€”â€”ä¸€ç§ç®€å•ä½†ç»“åˆäº† PEFT ä¸é‡åŒ– LLM ä¼˜ç‚¹çš„æœ‰æ•ˆæ–¹æ³•ã€‚é€šè¿‡ä»…æ›´æ–°é‡åŒ–å°ºåº¦ï¼ŒPEQA å¯ä»¥ç›´æ¥åº”ç”¨äºé‡åŒ–çš„ LLMï¼Œç¡®ä¿æ— ç¼ä»»åŠ¡è½¬æ¢ã€‚ä¸ç°æœ‰ PEFT æ–¹æ³•å¹¶è¡Œï¼ŒPEQA æ˜¾ç€å‡å°‘äº†ä¸ä¼˜åŒ–å™¨çŠ¶æ€ç›¸å…³çš„å†…å­˜å¼€é”€ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨é‡åŒ–çš„ä¼˜åŠ¿æ¥å¤§å¹…å‡å°æ¨¡å‹å¤§å°ã€‚å³ä½¿åœ¨å¾®è°ƒä¹‹åï¼ŒPEQA è°ƒæ•´çš„ LLM çš„é‡åŒ–ç»“æ„ä»ç„¶å®Œå¥½æ— æŸï¼Œä»è€Œå¯ä»¥åœ¨éƒ¨ç½²é˜¶æ®µåŠ é€Ÿæ¨ç†ã€‚æˆ‘ä»¬é‡‡ç”¨ PEQA è°ƒæ•´å¯¹ LLM è¿›è¡Œç‰¹å®šäºä»»åŠ¡çš„é€‚åº”ï¼Œå‚æ•°é«˜è¾¾ 65 äº¿ç¾å…ƒã€‚ä¸ºäº†è¯„ä¼° PEQA è°ƒæ•´çš„ LLM çš„é€»è¾‘æ¨ç†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬ä½¿ç”¨æŒ‡ä»¤æ•°æ®é›†å¾®è°ƒä½ä½é‡åŒ– LLMã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿ LLM è¢«é‡åŒ–åˆ°ä½äº 4 ä½ç²¾åº¦ï¼Œå®ƒä»¬åœ¨è¯­è¨€å»ºæ¨¡ã€å°æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç†è§£æ–¹é¢çš„èƒ½åŠ›ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¼¹æ€§æ¢å¤ï¼ˆç”šè‡³æ”¹è¿›ï¼‰å…¶å…¨ç²¾åº¦åŸå§‹æ€§èƒ½ï¼š PEQAã€‚

## Fairness-guided Few-shot Prompting for Large Language Models<sup>poster<sup>

Authors: Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu

Link: [https://neurips.cc/virtual/2023/poster/72392](https://neurips.cc/virtual/2023/poster/72392)

Abstract:

 Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper,  we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»è¡¨ç°å‡ºä»¤äººæƒŠè®¶çš„æ‰§è¡Œä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ï¼Œå³ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥é€šè¿‡ä»¥å‡ ä¸ªè¾“å…¥è¾“å‡ºç¤ºä¾‹æ„å»ºçš„æç¤ºä¸ºæ¡ä»¶ï¼Œç›´æ¥åº”ç”¨äºè§£å†³å¤§é‡ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç”±äºè®­ç»ƒç¤ºä¾‹ã€ç¤ºä¾‹é¡ºåºå’Œæç¤ºæ ¼å¼çš„å˜åŒ–ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ å¯èƒ½ä¼šé­å—é«˜åº¦ä¸ç¨³å®šçš„å½±å“ã€‚å› æ­¤ï¼Œæ„å»ºé€‚å½“çš„æç¤ºå¯¹äºæé«˜æƒ…å¢ƒå­¦ä¹ çš„ç»©æ•ˆè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»é¢„æµ‹åå·®çš„è§’åº¦é‡æ–°å®¡è§†è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæŒ‡æ ‡æ¥è¯„ä¼°å›ºå®šæç¤ºç›¸å¯¹äºæ ‡ç­¾æˆ–ç»™å®šå±æ€§çš„é¢„æµ‹åå·®ã€‚ç„¶åæˆ‘ä»¬å‡­ç»éªŒè¡¨æ˜ï¼Œå…·æœ‰è¾ƒé«˜åå·®çš„æç¤ºæ€»æ˜¯ä¼šå¯¼è‡´é¢„æµ‹è´¨é‡ä¸ä»¤äººæ»¡æ„ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè´ªå©ªæœç´¢çš„æ–°é¢–æœç´¢ç­–ç•¥ï¼Œä»¥è¯†åˆ«è¿‘ä¹æœ€ä½³çš„æç¤ºï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨ GPT-3 ç­‰æœ€å…ˆè¿›çš„ä¸»æµæ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå…¨é¢çš„å®éªŒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä»¥æœ‰æ•ˆä¸”å¯è§£é‡Šçš„æ–¹å¼å¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ã€‚

## Self-Chained Image-Language Model for Video Localization and Question Answering<sup>poster<sup>

Authors: Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal

Link: [https://neurips.cc/virtual/2023/poster/71124](https://neurips.cc/virtual/2023/poster/71124)

Abstract:

 Recent studies have shown promising results on utilizing large pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP- 2) to tackle both temporal keyframe localization and question answering on videos. SeViLA framework consists of two modules: Localizer and Answerer, where both are parameter-efficiently fine-tuned from BLIP-2. We propose two ways of chaining these modules for cascaded inference and self-refinement. First, in the forward chain, the Localizer finds multiple language-aware keyframes in a video, which the Answerer uses to predict the answer. Second, in the reverse chain, the Answerer generates keyframe pseudo-labels to refine the Localizer, alleviating the need for expensive video moment localization annotations. Our SeViLA framework outperforms several strong baselines/previous works on five challenging video question answering and event prediction benchmarks, and achieves the state-of-the-art in both fine-tuning (NExT-QA and STAR) and zero-shot (NExT-QA, STAR, How2QA, and VLEP) settings. We show a comprehensive analysis of our framework, including the impact of Localizer, comparisons of Localizer with other temporal localization models, pre-training/self-refinement of Localizer, and varying the number of keyframes.

æ‘˜è¦:

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨å¤§å‹é¢„è®­ç»ƒå›¾åƒè¯­è¨€æ¨¡å‹è¿›è¡Œè§†é¢‘é—®ç­”å…·æœ‰è‰¯å¥½çš„æ•ˆæœã€‚è™½ç„¶è¿™äº›å›¾åƒè¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼è§†é¢‘è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ ï¼Œä½†å®ƒä»¬é€šå¸¸å°†ç»Ÿä¸€é‡‡æ ·çš„è§†é¢‘å¸§è¿æ¥ä¸ºè§†è§‰è¾“å…¥ï¼Œè€Œæ— éœ€æ˜¾å¼çš„è¯­è¨€æ„ŸçŸ¥ã€æ—¶é—´å»ºæ¨¡ã€‚å½“è§†é¢‘è¾“å…¥ä¸­åªæœ‰ä¸€éƒ¨åˆ†ä¸è¯­è¨€æŸ¥è¯¢ç›¸å…³æ—¶ï¼Œè¿™ç§ç»Ÿä¸€çš„å¸§é‡‡æ ·é€šå¸¸ä¼šå¯¼è‡´ä¸¢å¤±é‡è¦çš„è§†è§‰çº¿ç´¢ã€‚å°½ç®¡äººç±»ç»å¸¸æ‰¾åˆ°ä¸€ä¸ªè§†é¢‘æ—¶åˆ»æ¥å…³æ³¨å¹¶å€’å›è¯¥æ—¶åˆ»æ¥å›ç­”é—®é¢˜ï¼Œä½†è®­ç»ƒå…·æœ‰æŸ¥è¯¢æ„ŸçŸ¥çš„è§†é¢‘æ—¶åˆ»å®šä½å™¨é€šå¸¸éœ€è¦æ˜‚è´µçš„æ³¨é‡Šå’Œé«˜è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé“¾è§†é¢‘å®šä½å›ç­”ï¼ˆSeViLAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å•ä¸€å›¾åƒè¯­è¨€æ¨¡å‹ï¼ˆBLIP-2ï¼‰æ¥è§£å†³è§†é¢‘ä¸Šçš„æ—¶é—´å…³é”®å¸§å®šä½å’Œé—®é¢˜å›ç­”çš„æ–°é¢–æ¡†æ¶ã€‚ SeViLA æ¡†æ¶ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šLocalizer å’Œ Answererï¼Œè¿™ä¸¤ä¸ªæ¨¡å—éƒ½æ˜¯ä» BLIP-2 è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒçš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§é“¾æ¥è¿™äº›æ¨¡å—ä»¥è¿›è¡Œçº§è”æ¨ç†å’Œè‡ªæˆ‘ä¼˜åŒ–çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨å‰å‘é“¾ä¸­ï¼Œå®šä½å™¨åœ¨è§†é¢‘ä¸­æ‰¾åˆ°å¤šä¸ªè¯­è¨€æ„ŸçŸ¥å…³é”®å¸§ï¼Œåº”ç­”å™¨ä½¿ç”¨è¿™äº›å…³é”®å¸§æ¥é¢„æµ‹ç­”æ¡ˆã€‚å…¶æ¬¡ï¼Œåœ¨åå‘é“¾ä¸­ï¼Œåº”ç­”å™¨ç”Ÿæˆå…³é”®å¸§ä¼ªæ ‡ç­¾æ¥ç»†åŒ–å®šä½å™¨ï¼Œä»è€Œå‡è½»å¯¹æ˜‚è´µçš„è§†é¢‘æ—¶åˆ»å®šä½æ³¨é‡Šçš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„ SeViLA æ¡†æ¶åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘é—®ç­”å’Œäº‹ä»¶é¢„æµ‹åŸºå‡†ä¸Šä¼˜äºå‡ ä¸ªå¼ºå¤§çš„åŸºçº¿/ä¹‹å‰çš„å·¥ä½œï¼Œå¹¶åœ¨å¾®è°ƒï¼ˆNExT-QA å’Œ STARï¼‰å’Œé›¶æ ·æœ¬ï¼ˆNExT- QAã€STARã€How2QA å’Œ VLEPï¼‰è®¾ç½®ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¯¹æˆ‘ä»¬çš„æ¡†æ¶çš„å…¨é¢åˆ†æï¼ŒåŒ…æ‹¬ Localizer çš„å½±å“ã€Localizer ä¸å…¶ä»–æ—¶é—´å®šä½æ¨¡å‹çš„æ¯”è¾ƒã€Localizer çš„é¢„è®­ç»ƒ/è‡ªæˆ‘ç»†åŒ–ä»¥åŠæ”¹å˜å…³é”®å¸§çš„æ•°é‡ã€‚

## Kiki or Bouba? Sound Symbolism in Vision-and-Language Models<sup>poster<sup>

Authors: Morris Alper, Hadar Averbuch-Elor

Link: [https://neurips.cc/virtual/2023/poster/71134](https://neurips.cc/virtual/2023/poster/71134)

Abstract:

 Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.

æ‘˜è¦:

è™½ç„¶äººç±»è¯­è¨€ä¸­å£°éŸ³å’Œæ„ä¹‰ä¹‹é—´çš„æ˜ å°„è¢«è®¤ä¸ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä»»æ„çš„ï¼Œä½†è®¤çŸ¥ç§‘å­¦ç ”ç©¶è¡¨æ˜ï¼Œè·¨è¯­è¨€å’Œäººå£ç¾¤ä½“çš„ç‰¹å®šå£°éŸ³å’Œæ„ä¹‰ä¹‹é—´å­˜åœ¨ç€é‡è¦çš„ç›¸å…³æ€§ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºå£°éŸ³è±¡å¾ä¸»ä¹‰ã€‚åœ¨æ„ä¹‰çš„è®¸å¤šç»´åº¦ä¸­ï¼Œå£°éŸ³è±¡å¾å°¤å…¶çªå‡ºï¼Œå¹¶ä¸”åœ¨è¯­è¨€å’Œè§†è§‰é¢†åŸŸä¹‹é—´çš„è·¨æ¨¡å¼å…³è”æ–¹é¢å¾—åˆ°äº†å……åˆ†è¯æ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†å£°éŸ³è±¡å¾ä¸»ä¹‰æ˜¯å¦åæ˜ åœ¨è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ CLIP å’Œç¨³å®šæ‰©æ•£ï¼‰ä¸­çš„é—®é¢˜ã€‚ä½¿ç”¨é›¶æ ·æœ¬çŸ¥è¯†æ¢æµ‹æ¥ç ”ç©¶è¿™äº›æ¨¡å‹çš„å›ºæœ‰çŸ¥è¯†ï¼Œæˆ‘ä»¬å‘ç°å¼ºæœ‰åŠ›çš„è¯æ®è¡¨æ˜å®ƒä»¬ç¡®å®è¡¨ç°å‡ºè¿™ç§æ¨¡å¼ï¼Œä¸å¿ƒç†è¯­è¨€å­¦ä¸­ä¼—æ‰€å‘¨çŸ¥çš„ kiki-bouba æ•ˆåº”ç›¸ä¼¼ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ç§åˆ©ç”¨è®¡ç®—å·¥å…·å±•ç¤ºå£°éŸ³è±¡å¾æ„ä¹‰å¹¶ç†è§£å…¶æœ¬è´¨çš„æ–°é¢–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€ã€‚

## EvoPrompting: Language Models for Code-Level Neural Architecture Search<sup>poster<sup>

Authors: Angelica Chen, David Dohan, David So

Link: [https://neurips.cc/virtual/2023/poster/70729](https://neurips.cc/virtual/2023/poster/70729)

Abstract:

 Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as general adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm.While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.

æ‘˜è¦:

é‰´äºè¯­è¨€æ¨¡å‹ (LM) æœ€è¿‘åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆå°±ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨ LM ä½œä¸ºè¿›åŒ–ç¥ç»æ¶æ„æœç´¢ (NAS) ç®—æ³•çš„é€šç”¨è‡ªé€‚åº”å˜å¼‚å’Œäº¤å‰ç®—å­ã€‚ä»…é€šè¿‡æç¤ºå°±èƒ½æˆåŠŸçš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°è¿›åŒ–æç¤ºå·¥ç¨‹ä¸è½¯æç¤ºè°ƒæ•´ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º EvoPrompting çš„æ–¹æ³•ï¼‰çš„ç»“åˆï¼Œå§‹ç»ˆèƒ½æ‰¾åˆ°å¤šæ ·åŒ–ä¸”é«˜æ€§èƒ½çš„æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜ EvoPrompting åœ¨è®¡ç®—æ•ˆç‡é«˜çš„ MNIST-1D æ•°æ®é›†ä¸Šæ˜¯æœ‰æ•ˆçš„ï¼Œå…¶ä¸­ EvoPrompting äº§ç”Ÿçš„å·ç§¯æ¶æ„å˜ä½“åœ¨å‡†ç¡®æ€§å’Œæ¨¡å‹å¤§å°æ–¹é¢ä¼˜äºäººç±»ä¸“å®¶è®¾è®¡çš„æ¶æ„å’Œç®€å•çš„å°‘æ ·æœ¬æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨æˆ‘ä»¬çš„æ–¹æ³•åœ¨ CLRS ç®—æ³•æ¨ç†åŸºå‡†ä¸Šæœç´¢å›¾ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­ EvoPrompting èƒ½å¤Ÿè®¾è®¡æ–°é¢–çš„æ¶æ„ï¼Œåœ¨ 30 ä¸ªç®—æ³•æ¨ç†ä»»åŠ¡ä¸­çš„ 21 ä¸ªä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„æ¨¡å‹å¤§å°ã€‚ EvoPrompting æˆåŠŸåœ°åœ¨å„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­è®¾è®¡äº†å‡†ç¡®ã€é«˜æ•ˆçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒæ—¶ä¹Ÿå…·æœ‰è¶³å¤Ÿçš„é€šç”¨æ€§ï¼Œå¯ä»¥è½»æ¾é€‚åº”ç¥ç»ç½‘ç»œè®¾è®¡ä¹‹å¤–çš„å…¶ä»–ä»»åŠ¡ã€‚

## Stable and low-precision training for large-scale vision-language models<sup>poster<sup>

Authors: Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, Ludwig Schmidt

Link: [https://neurips.cc/virtual/2023/poster/70245](https://neurips.cc/virtual/2023/poster/70245)

Abstract:

 We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge---the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.

æ‘˜è¦:

æˆ‘ä»¬å¼•å…¥äº† 1ï¼‰åŠ é€Ÿå’Œ 2ï¼‰ç¨³å®šå¤§å‹è¯­è¨€è§†è§‰æ¨¡å‹è®­ç»ƒçš„æ–°æ–¹æ³•ã€‚ 1) ä¸ºäº†åŠ é€Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº† SwitchBackï¼Œä¸€ä¸ªç”¨äº int8 é‡åŒ–è®­ç»ƒçš„çº¿æ€§å±‚ï¼Œå®ƒæä¾›äº† 13-25% çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨ 1B å‚æ•° CLIP ViT-Huge ä¸Šä¸ bfloat16 è®­ç»ƒçš„æ€§èƒ½åŒ¹é…åœ¨ 0.1 ä¸ªç™¾åˆ†ç‚¹ä»¥å†…â€”â€”è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ int8 è®­ç»ƒã€‚æˆ‘ä»¬çš„ä¸»è¦å…³æ³¨ç‚¹æ˜¯ int8ï¼Œå› ä¸º GPU å¯¹ float8 çš„æ”¯æŒå¾ˆå°‘ï¼Œå°½ç®¡æˆ‘ä»¬ä¹Ÿé€šè¿‡æ¨¡æ‹Ÿåˆ†æ float8 çš„è®­ç»ƒã€‚è™½ç„¶ SwitchBack è¯æ˜å¯¹ float8 æœ‰æ•ˆï¼Œä½†æˆ‘ä»¬è¡¨æ˜ï¼Œå¦‚æœç½‘ç»œç»è¿‡è®­ç»ƒå’Œåˆå§‹åŒ–ï¼Œä¸é¼“åŠ±ä½¿ç”¨å¤§çš„ç‰¹å¾é‡ï¼Œé‚£ä¹ˆæ ‡å‡†æŠ€æœ¯ä¹Ÿæ˜¯æˆåŠŸçš„ï¼Œæˆ‘ä»¬é€šè¿‡ç”¨é›¶åˆå§‹åŒ–çš„å±‚è§„æ¨¡æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ 2) ä¸ºäº†ç¨³å®šæ€§ï¼Œæˆ‘ä»¬åˆ†æäº†æŸå¤±å³°å€¼ï¼Œå‘ç°åœ¨ AdamW äºŒé˜¶çŸ©ä¼°è®¡å™¨ä½ä¼°å¹³æ–¹æ¢¯åº¦åï¼Œå®ƒä»¬å§‹ç»ˆå‡ºç° 1-8 æ¬¡è¿­ä»£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨è AdamW-Adafactor æ··åˆä½“ï¼Œå®ƒå¯ä»¥åœ¨è®­ç»ƒ CLIP ViT-Huge æ¨¡å‹æ—¶é¿å…æŸå¤±å°–å³°ï¼Œå¹¶ä¸”åœ¨æˆ‘ä»¬æµ‹è¯•çš„å°ºåº¦ä¸Šä¼˜äºæ¢¯åº¦è£å‰ªã€‚

## Preference-grounded Token-level Guidance for Language Model Fine-tuning<sup>poster<sup>

Authors: Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, Mingyuan Zhou

Link: [https://neurips.cc/virtual/2023/poster/72756](https://neurips.cc/virtual/2023/poster/72756)

Abstract:

 Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and the utilization of the preference among multiple generations. For LM training, based on the amount of supervised data, we present two minimalist learning objectives that utilize the learned guidance. In experiments, our method performs competitively on two distinct representative LM tasks --- discrete-prompt generation and text summarization.

æ‘˜è¦:

å°†è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¸åå¥½ä¿æŒä¸€è‡´æ˜¯è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­çš„ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜æ˜¯åå¥½é€šå¸¸æ˜¯åœ¨åºåˆ—çº§åˆ«æä¾›çš„ï¼Œè€Œ LM è®­ç»ƒå’Œç”Ÿæˆéƒ½å‘ç”Ÿåœ¨ä»¤ç‰Œçº§åˆ«ã€‚å› æ­¤ï¼Œåå¥½å’Œ LM è®­ç»ƒæŸå¤±ä¹‹é—´å­˜åœ¨ç²’åº¦ä¸åŒ¹é…ï¼Œè¿™å¯èƒ½ä¼šä½¿å­¦ä¹ é—®é¢˜å˜å¾—å¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å‘æ›¿ä»£è®­ç»ƒè¿‡ç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨è¯¥è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨å°†åºåˆ—çº§åå¥½èå…¥åˆ°æ ‡è®°çº§è®­ç»ƒæŒ‡å¯¼ä¸­ï¼Œå¹¶åˆ©ç”¨å­¦ä¹ åˆ°çš„æŒ‡å¯¼æ¥æ”¹è¿› LM ä¹‹é—´è¿›è¡Œè¿­ä»£ã€‚å¯¹äºå¼•å¯¼å­¦ä¹ ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå°†æ¨¡ä»¿å­¦ä¹ ä¸­çš„æˆå¯¹åå¥½å­¦ä¹ æ‰©å±•åˆ°å¯å˜é•¿åº¦çš„ LM ç”Ÿæˆå’Œå¤šä»£ä¹‹é—´åå¥½çš„åˆ©ç”¨ã€‚å¯¹äº LM è®­ç»ƒï¼Œæ ¹æ®ç›‘ç£æ•°æ®é‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªåˆ©ç”¨å­¦ä¹ æŒ‡å¯¼çš„æç®€å­¦ä¹ ç›®æ ‡ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªä¸åŒçš„ä»£è¡¨æ€§ LM ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›â€”â€”ç¦»æ•£æç¤ºç”Ÿæˆå’Œæ–‡æœ¬æ‘˜è¦ã€‚

## MarioGPT: Open-Ended Text2Level Generation through Large Language Models<sup>poster<sup>

Authors: Shyam Sudhakaran, Miguel GonzÃ¡lez-Duque, Matthias Freiberger, Claire Glanois, Elias Najarro, Sebastian Risi

Link: [https://neurips.cc/virtual/2023/poster/71191](https://neurips.cc/virtual/2023/poster/71191)

Abstract:

 Procedural Content Generation (PCG) is a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific  intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have  shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. Here, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. MarioGPT can not only generate diverse levels, but can be  text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques.  As far as we know, MarioGPT is the first text-to-level model and combined with novelty search it enables the  generation of diverse levels with varying play-style dynamics (i.e. player paths) and the open-ended discovery of an increasingly diverse range of content. Code available at https://github.com/shyamsn97/mario-gpt.

æ‘˜è¦:

ç¨‹åºå†…å®¹ç”Ÿæˆï¼ˆPCGï¼‰æ˜¯ä¸€ç§ä»¥è‡ªåŠ¨åŒ–æ–¹å¼ç”Ÿæˆå¤æ‚å¤šæ ·ç¯å¢ƒçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè™½ç„¶ä½¿ç”¨ PCG æ–¹æ³•ç”Ÿæˆå†…å®¹é€šå¸¸å¾ˆç®€å•ï¼Œä½†ç”Ÿæˆåæ˜ ç‰¹å®šæ„å›¾å’Œçº¦æŸçš„æœ‰æ„ä¹‰çš„å†…å®¹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè®¸å¤š PCG ç®—æ³•ç¼ºä¹ä»¥å¼€æ”¾å¼æ–¹å¼ç”Ÿæˆå†…å®¹çš„èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨è®¸å¤šä¸åŒçš„é¢†åŸŸéƒ½æ˜¾ç¤ºå‡ºæå…¶æœ‰æ•ˆçš„æ•ˆæœã€‚è¿™äº›ç»è¿‡åŸ¹è®­çš„æ³•å­¦ç¡•å£«å¯ä»¥è¿›è¡Œå¾®è°ƒï¼Œé‡å¤ä½¿ç”¨ä¿¡æ¯å¹¶åŠ é€Ÿæ–°ä»»åŠ¡çš„åŸ¹è®­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç» MarioGPTï¼Œè¿™æ˜¯ä¸€ç§ç»è¿‡å¾®è°ƒçš„ GPT2 æ¨¡å‹ï¼Œç»è¿‡è®­ç»ƒå¯ä»¥ç”ŸæˆåŸºäºå›¾å—çš„æ¸¸æˆå…³å¡ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯ã€Šè¶…çº§é©¬é‡Œå¥¥å…„å¼Ÿã€‹å…³å¡ã€‚ MarioGPTä¸ä»…å¯ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„å…³å¡ï¼Œè¿˜å¯ä»¥é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œå¯æ§å…³å¡ç”Ÿæˆï¼Œè§£å†³äº†å½“å‰PCGæŠ€æœ¯çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMarioGPT æ˜¯ç¬¬ä¸€ä¸ªæ–‡æœ¬åˆ°å…³å¡æ¨¡å‹ï¼Œä¸æ–°é¢–æ€§æœç´¢ç›¸ç»“åˆï¼Œå®ƒèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸åŒæ¸¸æˆé£æ ¼åŠ¨æ€ï¼ˆå³ç©å®¶è·¯å¾„ï¼‰çš„ä¸åŒå…³å¡ï¼Œä»¥åŠæ—¥ç›Šå¤šæ ·åŒ–çš„èŒƒå›´çš„å¼€æ”¾å¼å‘ç°çš„å†…å®¹ã€‚ä»£ç å¯åœ¨ https://github.com/shyamsn97/mario-gpt è·å–ã€‚

## GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction<sup>poster<sup>

Authors: Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan

Link: [https://neurips.cc/virtual/2023/poster/71060](https://neurips.cc/virtual/2023/poster/71060)

Abstract:

 This paper aims to efficiently enable Large Language Models (LLMs) to use multi-modal tools.The advanced proprietary LLMs, such as ChatGPT and GPT-4, have shown great potential for tool usage through sophisticated prompt engineering.Nevertheless, these models typically rely on prohibitive computational costs and publicly inaccessible data.To address these challenges, we propose the GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools.It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts.By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation.Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways.Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools.

æ‘˜è¦:

æœ¬æ–‡æ—¨åœ¨æœ‰æ•ˆåœ°ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿä½¿ç”¨å¤šæ¨¡æ€å·¥å…·ã€‚ChatGPT å’Œ GPT-4 ç­‰å…ˆè¿›çš„ä¸“æœ‰ LLM é€šè¿‡å¤æ‚çš„æç¤ºå·¥ç¨‹æ˜¾ç¤ºå‡ºå·¥å…·ä½¿ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºé«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œå…¬å¼€ä¸å¯è®¿é—®çš„æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè‡ªæŒ‡ä»¤çš„ GPT4Toolsï¼Œä»¥ä½¿å¼€æº LLMï¼ˆä¾‹å¦‚ LLaMA å’Œ OPTï¼‰èƒ½å¤Ÿä½¿ç”¨å·¥å…·ã€‚å®ƒé€šè¿‡ä»¥ä¸‹æ–¹å¼ç”Ÿæˆéµå¾ªæŒ‡ä»¤çš„æ•°æ®é›†ï¼šé€šè¿‡ä½¿ç”¨ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰ä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰åŠ©äºå¼€æºæ³•å­¦ç¡•å£«è§£å†³ä¸€ç³»åˆ—è§†è§‰é—®é¢˜ï¼ŒåŒ…æ‹¬è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›è¯„ä¼°LLMä½¿ç”¨å·¥å…·èƒ½åŠ›çš„åŸºå‡†ï¼Œä»¥é›¶æ ·æœ¬å’Œå¾®è°ƒçš„æ–¹å¼è¿›è¡Œã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è¯­è¨€æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œè¿™ä¸ä»…æ˜¾ç€æé«˜äº†è°ƒç”¨æ‰€è§å·¥å…·çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”è¿˜èƒ½å®ç°çœ‹ä¸è§çš„å·¥å…·çš„é›¶å°„å‡»èƒ½åŠ›ã€‚

## Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning<sup>poster<sup>

Authors: Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati

Link: [https://neurips.cc/virtual/2023/poster/69907](https://neurips.cc/virtual/2023/poster/69907)

Abstract:

 There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.

æ‘˜è¦:

äººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£å°†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºè§„åˆ’é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºå¤šç§å› ç´ ï¼Œç›´æ¥ä½¿ç”¨æ³•å­¦ç¡•å£«ä½œä¸ºè§„åˆ’è€…çš„æ–¹æ³•ç›®å‰æ˜¯ä¸åˆ‡å®é™…çš„ï¼ŒåŒ…æ‹¬è®¡åˆ’çš„æ­£ç¡®æ€§æœ‰é™ã€å¼ºçƒˆä¾èµ–ä¸æ¨¡æ‹Ÿå™¨ç”šè‡³å®é™…ç¯å¢ƒäº¤äº’çš„åé¦ˆï¼Œä»¥åŠåˆ©ç”¨äººç±»åé¦ˆçš„æ•ˆç‡ä½ä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ›¿ä»£èŒƒå¼ï¼Œè¯¥èŒƒå¼åœ¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ä¸­æ„å»ºæ˜¾å¼ä¸–ç•Œï¼ˆé¢†åŸŸï¼‰æ¨¡å‹ï¼Œç„¶åä½¿ç”¨å®ƒä¸å¥å…¨çš„é¢†åŸŸæ— å…³è§„åˆ’å™¨è¿›è¡Œè§„åˆ’ã€‚ä¸ºäº†è§£å†³ LLM æœ€åˆå¯èƒ½æ— æ³•ç”ŸæˆåŠŸèƒ½é½å…¨çš„ PDDL æ¨¡å‹çš„äº‹å®ï¼Œæˆ‘ä»¬é‡‡ç”¨ LLM ä½œä¸º PDDL å’Œçº æ­£åé¦ˆæºï¼ˆä¾‹å¦‚ PDDL éªŒè¯è€…å’Œäººç±»ï¼‰ä¹‹é—´çš„æ¥å£ã€‚å¯¹äºç¼ºä¹ PDDL èƒŒæ™¯çš„ç”¨æˆ·ï¼Œæˆ‘ä»¬è¡¨æ˜æ³•å­¦ç¡•å£«å¯ä»¥å°† PDDL ç¿»è¯‘æˆè‡ªç„¶è¯­è¨€ï¼Œå¹¶æœ‰æ•ˆåœ°å°†çº æ­£åé¦ˆç¼–ç å›åº•å±‚é¢†åŸŸæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ä»…äº«æœ‰å¤–éƒ¨è§„åˆ’è€…æä¾›çš„æ­£ç¡®æ€§ä¿è¯ï¼Œè€Œä¸”è¿˜å…è®¸ç”¨æˆ·åœ¨å¼€å§‹æ—¶çº æ­£åŸŸæ¨¡å‹ï¼Œè€Œä¸æ˜¯åƒä»¥å‰çš„å·¥ä½œé‚£æ ·æ£€æŸ¥å’Œçº æ­£ï¼ˆé€šè¿‡äº¤äº’å¼æç¤ºï¼‰æ¯ä¸ªç”Ÿæˆçš„è®¡åˆ’ï¼Œä»è€Œå‡å°‘äº†äººä¸ºå‚ä¸ã€‚åœ¨ä¸¤ä¸ª IPC åŸŸå’Œä¸€ä¸ªæ¯” ALFWorld ç­‰å¸¸ç”¨åŸºå‡†æ›´å¤æ‚çš„ Household åŸŸä¸Šï¼Œæˆ‘ä»¬è¯æ˜å¯ä»¥åˆ©ç”¨ GPT-4 ä¸º 40 å¤šä¸ªåŠ¨ä½œç”Ÿæˆé«˜è´¨é‡çš„ PDDL æ¨¡å‹ï¼Œç„¶åä½¿ç”¨æ ¡æ­£åçš„ PDDL æ¨¡å‹æˆåŠŸè§£å†³ 48 é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„è§„åˆ’ä»»åŠ¡ã€‚åŒ…æ‹¬æºä»£ç åœ¨å†…çš„èµ„æºå‘å¸ƒäºï¼šhttps://guansuns.github.io/pages/llm-dmã€‚

## From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader<sup>poster<sup>

Authors: Weiwen Xu, Xin Li, Wenxuan Zhang, Meng Zhou, Wai Lam, Luo Si, Lidong Bing

Link: [https://neurips.cc/virtual/2023/poster/72478](https://neurips.cc/virtual/2023/poster/72478)

Abstract:

 We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data.PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs.To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training.Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios.When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR also has the potential to serve as a unified model for tackling various extraction and classification tasks in the MRC formulation.

æ‘˜è¦:

æˆ‘ä»¬æå‡ºäº†é¢„è®­ç»ƒæœºå™¨é˜…è¯»å™¨ï¼ˆPMRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è·å–æ ‡è®°æ•°æ®å³å¯å°†é¢„è®­ç»ƒæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰æ”¹é€ ä¸ºé¢„è®­ç»ƒæœºå™¨é˜…è¯»ç†è§£ï¼ˆMRCï¼‰æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚PMR å¯ä»¥è§£å†³æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ç°æœ‰ MLM çš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸å¾®è°ƒã€‚ä¸ºäº†æ„å»ºæ‰€æå‡ºçš„ PMRï¼Œæˆ‘ä»¬ä½¿ç”¨ç»´åŸºç™¾ç§‘è¶…é“¾æ¥æ„å»ºäº†å¤§é‡é€šç”¨ä¸”é«˜è´¨é‡çš„ MRC å¼è®­ç»ƒæ•°æ®ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ª Wiki Anchor Extraction ä»»åŠ¡æ¥æŒ‡å¯¼MRCå¼çš„é¢„è®­ç»ƒã€‚é™¤äº†ç®€å•æ€§ä¹‹å¤–ï¼ŒPMRè¿˜æœ‰æ•ˆåœ°è§£å†³äº†æå–ä»»åŠ¡ï¼Œä¾‹å¦‚æå–é—®ç­”å’Œå‘½åå®ä½“è¯†åˆ«ã€‚ PMR æ¯”ç°æœ‰æ–¹æ³•æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„åœºæ™¯ä¸­ã€‚å½“åº”ç”¨äº MRC å…¬å¼ä¸­çš„åºåˆ—åˆ†ç±»ä»»åŠ¡æ—¶ï¼ŒPMR èƒ½å¤Ÿæå–é«˜è´¨é‡çš„åŸºæœ¬åŸç†æ¥è§£é‡Šåˆ†ç±»è¿‡ç¨‹ï¼Œä»è€Œæä¾›æ›´å¥½çš„é¢„æµ‹å¯è§£é‡Šæ€§ã€‚ PMR è¿˜æœ‰æ½œåŠ›ä½œä¸ºç»Ÿä¸€æ¨¡å‹æ¥å¤„ç† MRC å…¬å¼ä¸­çš„å„ç§æå–å’Œåˆ†ç±»ä»»åŠ¡ã€‚

## No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models<sup>poster<sup>

Authors: Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, Matt Kusner

Link: [https://neurips.cc/virtual/2023/poster/70190](https://neurips.cc/virtual/2023/poster/70190)

Abstract:

 The computation necessary for training Transformer-based language models has skyrocketed in recent years.This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop., RHO-loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.

æ‘˜è¦:

è¿‘å¹´æ¥ï¼Œè®­ç»ƒåŸºäº Transformer çš„è¯­è¨€æ¨¡å‹æ‰€éœ€çš„è®¡ç®—é‡çŒ›å¢ã€‚è¿™ä¸€è¶‹åŠ¿æ¨åŠ¨äº†å¯¹é«˜æ•ˆè®­ç»ƒç®—æ³•çš„ç ”ç©¶ï¼Œè¿™äº›ç®—æ³•æ—¨åœ¨æ¯”æ ‡å‡†è®­ç»ƒæ›´å¿«åœ°æé«˜è®­ç»ƒã€éªŒè¯å’Œä¸‹æ¸¸æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ä¸‰ç±»æ­¤ç±»ç®—æ³•ï¼šåŠ¨æ€æ¶æ„ï¼ˆå±‚å †å ã€å±‚ä¸¢å¼ƒï¼‰ã€æ‰¹é‡é€‰æ‹©ï¼ˆé€‰æ‹©æ€§åå‘ä¼ æ’­ã€RHO æŸå¤±ï¼‰å’Œé«˜æ•ˆä¼˜åŒ–å™¨ï¼ˆLionã€Sophiaï¼‰ã€‚å½“ä½¿ç”¨æ­¤ç±»æ–¹æ³•ä»¥å›ºå®šè®¡ç®—é¢„ç®—é¢„è®­ç»ƒ BERT å’Œ T5 æ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¸å­¦ä¹ ç‡å®Œå…¨è¡°å‡çš„åŸºçº¿ç›¸æ¯”ï¼Œå®ƒä»¬çš„è®­ç»ƒã€éªŒè¯å’Œä¸‹æ¸¸å¢ç›Šæ¶ˆå¤±äº†ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªè¯„ä¼°åè®®ï¼Œé€šè¿‡å°†æ‰€æœ‰è®¡ç®—æ—¶é—´æ˜ å°„åˆ°æˆ‘ä»¬ç§°ä¸ºå‚è€ƒç³»ç»Ÿæ—¶é—´çš„å‚è€ƒæœºå™¨ï¼Œå¯ä»¥åœ¨ä»»æ„æœºå™¨ä¸Šè¿›è¡Œè®¡ç®—ã€‚æˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬æå‡ºçš„åè®®çš„å±€é™æ€§ï¼Œå¹¶å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ï¼Œä»¥é¼“åŠ±å¯¹é«˜æ•ˆè®­ç»ƒç¨‹åºè¿›è¡Œä¸¥æ ¼ç ”ç©¶ï¼šhttps://github.com/JeanKaddour/NoTrainNoGainã€‚

## AVIS: Autonomous Visual Information Seeking with Large Language Model Agent<sup>poster<sup>

Authors: Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David Ross, Cordelia Schmid, Alireza Fathi

Link: [https://neurips.cc/virtual/2023/poster/72718](https://neurips.cc/virtual/2023/poster/72718)

Abstract:

 In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs via tree search, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-based visual question answering benchmarks such as Infoseek and OK-VQA.

æ‘˜è¦:

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªä¸»ä¿¡æ¯å¯»æ±‚è§†è§‰é—®ç­”æ¡†æ¶ AVISã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥åŠ¨æ€åˆ¶å®šå¤–éƒ¨å·¥å…·çš„ä½¿ç”¨ç­–ç•¥ï¼Œå¹¶é€šè¿‡æ ‘æœç´¢è°ƒæŸ¥å…¶è¾“å‡ºï¼Œä»è€Œè·å–ä¸ºæ‰€æå‡ºçš„é—®é¢˜æä¾›ç­”æ¡ˆæ‰€éœ€çš„ä¸å¯æˆ–ç¼ºçš„çŸ¥è¯†ã€‚å›ç­”éœ€è¦å¤–éƒ¨çŸ¥è¯†çš„è§†è§‰é—®é¢˜ï¼Œä¾‹å¦‚â€œè¯¥å›¾åƒä¸­æç»˜çš„å»ºç­‘ç‰©æ˜¯ä¸ºäº†çºªå¿µä»€ä¹ˆäº‹ä»¶ï¼Ÿâ€ï¼Œæ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ã€‚æ­¤ä»»åŠ¡æä¾›äº†ä¸€ä¸ªç»„åˆæœç´¢ç©ºé—´ï¼Œéœ€è¦ä¸€ç³»åˆ—æ“ä½œï¼ŒåŒ…æ‹¬è°ƒç”¨ APIã€åˆ†æå…¶å“åº”ä»¥åŠåšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œä»¥æ”¶é›†äººç±»åœ¨é¢ä¸´æ­¤ä»»åŠ¡æ—¶åšå‡ºå†³ç­–çš„å„ç§å®ä¾‹ã€‚ç„¶åï¼Œè¿™äº›æ•°æ®è¢«ç”¨æ¥è®¾è®¡ä¸€ä¸ªç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆçš„ç³»ç»Ÿï¼šä¸€ä¸ªç”± LLM é©±åŠ¨çš„è§„åˆ’å™¨ï¼ŒåŠ¨æ€ç¡®å®šä¸‹ä¸€æ­¥è¦ä½¿ç”¨å“ªä¸ªå·¥å…·ï¼›ä¸€ä¸ªç”± LLM é©±åŠ¨çš„æ¨ç†å™¨ï¼Œç”¨äºåˆ†æå¹¶ä»å·¥å…·è¾“å‡ºä¸­æå–å…³é”®ä¿¡æ¯ï¼›ä»¥åŠä¸€ä¸ªå·¥ä½œè®°å¿†ç»„ä»¶ï¼Œåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¿ç•™æ‰€è·å–çš„ä¿¡æ¯ã€‚æ”¶é›†åˆ°çš„ç”¨æˆ·è¡Œä¸ºé€šè¿‡ä¸¤ä¸ªå…³é”®æ–¹å¼ä¸ºæˆ‘ä»¬çš„ç³»ç»Ÿæä¾›æŒ‡å¯¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æç”¨æˆ·åšå‡ºçš„å†³ç­–é¡ºåºæ¥åˆ›å»ºä¸€ä¸ªè½¬æ¢å›¾ã€‚è¯¥å›¾æç»˜äº†ä¸åŒçš„çŠ¶æ€å¹¶é™åˆ¶äº†æ¯ä¸ªçŠ¶æ€ä¸‹å¯ç”¨çš„æ“ä½œé›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç”¨æˆ·å†³ç­–çš„ç¤ºä¾‹ä¸ºæˆ‘ä»¬ç”± LLM æ”¯æŒçš„è§„åˆ’å™¨å’Œæ¨ç†å™¨æä¾›ç›¸å…³çš„ä¸Šä¸‹æ–‡å®ä¾‹ï¼Œä»è€Œå¢å¼ºä»–ä»¬åšå‡ºæ˜æ™ºå†³ç­–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒAVIS åœ¨åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”åŸºå‡†ï¼ˆä¾‹å¦‚ Infoseek å’Œ OK-VQAï¼‰ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

## Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models<sup>poster<sup>

Authors: Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun

Link: [https://neurips.cc/virtual/2023/poster/72296](https://neurips.cc/virtual/2023/poster/72296)

Abstract:

 Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.

æ‘˜è¦:

è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å¤§é‡äº‹å®ä¿¡æ¯ï¼Œæœ€è¿‘çš„å·¥ä½œå°†è¿™äº›ä¿¡æ¯å®šä½åˆ°ç‰¹å®šçš„æ¨¡å‹æƒé‡ï¼Œä¾‹å¦‚ä¸­é—´å±‚ MLP æƒé‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬å¯ä»¥é€šè¿‡ç¼–è¾‘ä¸ç°æœ‰æ–¹æ³•å»ºè®®äº‹å®å­˜å‚¨ä½ç½®ä¸åŒçš„ä½ç½®çš„æƒé‡æ¥æ›´æ”¹äº‹å®åœ¨æ¨¡å‹ä¸­çš„å­˜å‚¨æ–¹å¼ã€‚è¿™æ˜¯ä»¤äººæƒŠè®¶çš„ï¼Œå› ä¸ºæˆ‘ä»¬æœŸæœ›å°†äº‹å®æœ¬åœ°åŒ–åˆ°ç‰¹å®šçš„æ¨¡å‹å‚æ•°ä¼šå‘Šè¯‰æˆ‘ä»¬åœ¨å“ªé‡Œæ“çºµæ¨¡å‹ä¸­çš„çŸ¥è¯†ï¼Œå¹¶ä¸”è¿™ç§å‡è®¾æ¿€å‘äº†è¿‡å»å¯¹æ¨¡å‹ç¼–è¾‘æ–¹æ³•çš„ç ”ç©¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œè¡¨ç¤ºå»å™ªï¼ˆä¹Ÿç§°ä¸ºå› æœè¿½è¸ªï¼‰çš„å®šä½ç»“è®ºå¹¶æ²¡æœ‰æä¾›ä»»ä½•å…³äºå“ªä¸ªæ¨¡å‹ MLP å±‚æœ€é€‚åˆç¼–è¾‘ä»¥ä¾¿ç”¨æ–°äº‹å®è¦†ç›–ç°æœ‰å­˜å‚¨äº‹å®çš„è§è§£ã€‚è¿™ä¸€å‘ç°æå‡ºäº†å…³äºè¿‡å»çš„å·¥ä½œå¦‚ä½•ä¾èµ–å› æœè¿½è¸ªæ¥é€‰æ‹©è¦ç¼–è¾‘çš„æ¨¡å‹å±‚çš„é—®é¢˜ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è€ƒè™‘ç¼–è¾‘é—®é¢˜çš„å‡ ç§å˜ä½“ï¼ŒåŒ…æ‹¬åˆ é™¤å’Œæ”¾å¤§äº‹å®ã€‚å¯¹äºæˆ‘ä»¬çš„ä¸€ä¸ªç¼–è¾‘é—®é¢˜ï¼Œç¼–è¾‘æ€§èƒ½ç¡®å®ä¸è¡¨ç¤ºå»å™ªçš„æœ¬åœ°åŒ–ç»“æœæœ‰å…³ï¼Œä½†æˆ‘ä»¬å‘ç°æˆ‘ä»¬ç¼–è¾‘çš„å›¾å±‚å¯ä»¥æ›´å¥½åœ°é¢„æµ‹æ€§èƒ½ã€‚ä¸ç›´è§‰ç›¸åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¦‚ä½•å·¥ä½œçš„æ›´å¥½çš„æœºæ¢°ç†è§£å¯èƒ½å¹¶ä¸æ€»æ˜¯è½¬åŒ–ä¸ºå…³äºå¦‚ä½•æœ€å¥½åœ°æ”¹å˜å…¶è¡Œä¸ºçš„è§è§£ã€‚

## LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models<sup>poster<sup>

Authors: Neel Guha, Julian Nyarko, Daniel Ho, Christopher RÃ©, Adam Chilton, Aditya K, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, Zehua Li

Link: [https://neurips.cc/virtual/2023/poster/73565](https://neurips.cc/virtual/2023/poster/73565)

Abstract:

 The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoningâ€”which distinguish between its many formsâ€”correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.

æ‘˜è¦:

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°åŠå…¶è¢«æ³•å¾‹ç•Œçš„é‡‡ç”¨å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šLLM å¯ä»¥æ‰§è¡Œå“ªäº›ç±»å‹çš„æ³•å¾‹æ¨ç†ï¼Ÿä¸ºäº†æ›´å¥½åœ°ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† LegalBenchï¼šä¸€ä¸ªåä½œæ„å»ºçš„æ³•å¾‹æ¨ç†åŸºå‡†ï¼Œç”±æ¶µç›–å…­ç§ä¸åŒç±»å‹çš„æ³•å¾‹æ¨ç†çš„ 162 é¡¹ä»»åŠ¡ç»„æˆã€‚ LegalBench æ˜¯é€šè¿‡è·¨å­¦ç§‘è¿‡ç¨‹æ„å»ºçš„ï¼Œå…¶ä¸­æˆ‘ä»¬æ”¶é›†äº†ç”±æ³•å¾‹ä¸“ä¸šäººå£«è®¾è®¡å’Œæ‰‹å·¥åˆ¶ä½œçš„ä»»åŠ¡ã€‚ç”±äºè¿™äº›ä¸»é¢˜ä¸“å®¶åœ¨æ„å»ºä¸­å‘æŒ¥ä¸»å¯¼ä½œç”¨ï¼Œå› æ­¤ä»»åŠ¡è¦ä¹ˆè¡¡é‡å®é™…æœ‰ç”¨çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ï¼Œè¦ä¹ˆè¡¡é‡å¾‹å¸ˆæ„Ÿå…´è¶£çš„æ¨ç†æŠ€èƒ½ã€‚ä¸ºäº†èƒ½å¤Ÿå°±æ³•å¾‹ä¸­çš„æ³•å­¦ç¡•å£«è¿›è¡Œè·¨å­¦ç§‘å¯¹è¯ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æè¿°æ³•å¾‹æ¨ç†çš„æµè¡Œæ³•å¾‹æ¡†æ¶ï¼ˆåŒºåˆ†å…¶å¤šç§å½¢å¼ï¼‰å¦‚ä½•ä¸ LegalBench ä»»åŠ¡ç›¸å¯¹åº”ï¼Œä»è€Œä¸ºå¾‹å¸ˆå’Œæ³•å­¦ç¡•å£«å¼€å‘äººå‘˜æä¾›äº†é€šç”¨è¯æ±‡ã€‚æœ¬æ–‡ä»‹ç»äº† LegalBenchï¼Œå¯¹ 20 ä¸ªå¼€æºå’Œå•†ä¸šæ³•å­¦ç¡•å£«è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œå¹¶è¯´æ˜äº† LegalBench æ‰€æ”¯æŒçš„ç ”ç©¶æ¢ç´¢ç±»å‹ã€‚

## Large Language Models as Commonsense Knowledge for Large-Scale Task Planning<sup>poster<sup>

Authors: Zirui Zhao, Wee Sun Lee, David Hsu

Link: [https://neurips.cc/virtual/2023/poster/71394](https://neurips.cc/virtual/2023/poster/71394)

Abstract:

 Large-scale task planning is a major challenge.  Recent work exploits large  language models (LLMs) directly as a policy and shows surprisingly  interesting results.  This paper shows that LLMs provide a  commonsense model of the world in addition to a policy that acts on it.  The world model and the policy can be combined in a search  algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task  planning.  In our new LLM-MCTS algorithm, the LLM-induced world model  provides a commonsense prior belief for MCTS to achieve effective reasoning;  the LLM-induced policy acts as a heuristic to guide the search, vastly  improving search efficiency. Experiments show that LLM-MCTS outperforms  both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide  margin, for complex, novel tasks.   Further experiments and analyses on multiple tasks --  multiplication, travel planning, object rearrangement --  suggest minimum description length (MDL)  as a general guiding principle: if the  description length of the world model is substantially smaller than that of  the  policy, using LLM as a world model for model-based planning is likely better  than using LLM solely as a policy.

æ‘˜è¦:

å¤§è§„æ¨¡ä»»åŠ¡è§„åˆ’æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘çš„å·¥ä½œç›´æ¥åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸€ç§ç­–ç•¥ï¼Œå¹¶æ˜¾ç¤ºå‡ºä»¤äººæƒŠè®¶çš„æœ‰è¶£ç»“æœã€‚æœ¬æ–‡è¡¨æ˜ï¼Œæ³•å­¦ç¡•å£«æä¾›äº†ä¸€ä¸ªå…³äºä¸–ç•Œçš„å¸¸è¯†æ¨¡å‹ä»¥åŠå¯¹å…¶é‡‡å–è¡ŒåŠ¨çš„æ”¿ç­–ã€‚ä¸–ç•Œæ¨¡å‹å’Œç­–ç•¥å¯ä»¥ç»“åˆåœ¨æœç´¢ç®—æ³•ä¸­ï¼Œä¾‹å¦‚è’™ç‰¹å¡ç½—æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œä»¥æ‰©å¤§ä»»åŠ¡è§„åˆ’è§„æ¨¡ã€‚åœ¨æˆ‘ä»¬æ–°çš„LLM-MCTSç®—æ³•ä¸­ï¼ŒLLMå¼•å‘çš„ä¸–ç•Œæ¨¡å‹ä¸ºMCTSæä¾›äº†å¸¸è¯†æ€§å…ˆéªŒä¿¡å¿µï¼Œä»¥å®ç°æœ‰æ•ˆæ¨ç†ï¼› LLM å¼•å‘çš„ç­–ç•¥ä½œä¸ºå¯å‘å¼æŒ‡å¯¼æœç´¢ï¼Œæå¤§åœ°æé«˜äº†æœç´¢æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹äºå¤æ‚ã€æ–°é¢–çš„ä»»åŠ¡ï¼ŒLLM-MCTS è¿œè¿œä¼˜äºå•ç‹¬çš„ MCTS å’Œ LLMï¼ˆGPT2 å’Œ GPT3.5ï¼‰è¯±å¯¼çš„ç­–ç•¥ã€‚å¯¹å¤šé¡¹ä»»åŠ¡ï¼ˆä¹˜æ³•ã€æ—…è¡Œè§„åˆ’ã€å¯¹è±¡é‡æ–°æ’åˆ—ï¼‰çš„è¿›ä¸€æ­¥å®éªŒå’Œåˆ†æå»ºè®®å°†æœ€å°æè¿°é•¿åº¦ï¼ˆMDLï¼‰ä½œä¸ºä¸€èˆ¬æŒ‡å¯¼åŸåˆ™ï¼šå¦‚æœä¸–ç•Œæ¨¡å‹çš„æè¿°é•¿åº¦è¿œå°äºç­–ç•¥çš„æè¿°é•¿åº¦ï¼Œåˆ™ä½¿ç”¨ LLMä½œä¸ºåŸºäºæ¨¡å‹çš„è§„åˆ’çš„ä¸–ç•Œæ¨¡å‹å¯èƒ½æ¯”ä»…å°†æ³•å­¦ç¡•å£«ç”¨ä½œæ”¿ç­–æ›´å¥½ã€‚

## Benchmarking Large Language Models on CMExam - A comprehensive Chinese Medical Exam Dataset<sup>poster<sup>

Authors: Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, LEI ZHU, Michael Li

Link: [https://neurips.cc/virtual/2023/poster/73638](https://neurips.cc/virtual/2023/poster/73638)

Abstract:

 Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å·²ç»æ”¹å˜äº†é—®ç­”ï¼ˆQAï¼‰é¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŒ–å’Œå…¨é¢çš„æ•°æ®é›†ï¼Œè¯„ä¼°åŒ»å­¦é¢†åŸŸçš„æ³•å­¦ç¡•å£«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†æºè‡ªä¸­å›½å›½å®¶åŒ»å¸ˆèµ„æ ¼è€ƒè¯•çš„ CMExamã€‚ CMExamåŒ…å«60K+å¤šé¡¹é€‰æ‹©é¢˜ï¼Œç”¨äºæ ‡å‡†åŒ–å’Œå®¢è§‚çš„è¯„ä¼°ï¼Œä»¥åŠå¼€æ”¾å¼æ¨¡å‹æ¨ç†è¯„ä¼°çš„è§£å†³æ–¹æ¡ˆè§£é‡Šã€‚ä¸ºäº†å¯¹æ³•å­¦ç¡•å£«è¿›è¡Œæ·±å…¥åˆ†æï¼Œæˆ‘ä»¬é‚€è¯·åŒ»å­¦ä¸“ä¸šäººå£«é¢å¤–æ ‡æ³¨äº†äº”ä¸ªé—®é¢˜æ³¨é‡Šï¼ŒåŒ…æ‹¬ç–¾ç—…ç»„ã€ä¸´åºŠç§‘å®¤ã€åŒ»å­¦å­¦ç§‘ã€èƒ½åŠ›é¢†åŸŸå’Œé—®é¢˜éš¾åº¦çº§åˆ«ã€‚é™¤äº†æ•°æ®é›†ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨ CMExam ä¸Šä½¿ç”¨ä»£è¡¨æ€§çš„ LLM å’Œ QA ç®—æ³•è¿›è¡Œäº†å½»åº•çš„å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4 çš„å‡†ç¡®ç‡æœ€å¥½ï¼Œä¸º 61.6%ï¼ŒåŠ æƒ F1 å¾—åˆ†ä¸º 0.617ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä¸äººç±»å‡†ç¡®ç‡ï¼ˆ71.6%ï¼‰ç›¸æ¯”çš„å·¨å¤§å·®è·ã€‚å¯¹äºè§£é‡Šä»»åŠ¡ï¼Œè™½ç„¶æ³•å­¦ç¡•å£«å¯ä»¥ç”Ÿæˆç›¸å…³æ¨ç†å¹¶åœ¨å¾®è°ƒåè¡¨ç°å‡ºæ”¹è¿›çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬æ²¡æœ‰è¾¾åˆ°é¢„æœŸçš„æ ‡å‡†ï¼Œè¿™è¡¨æ˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCMExam æ˜¯ç¬¬ä¸€ä¸ªæä¾›å…¨é¢åŒ»å­¦æ³¨é‡Šçš„ä¸­å›½åŒ»å­¦æ£€æŸ¥æ•°æ®é›†ã€‚æ³•å­¦ç¡•å£«è¯„ä¼°çš„å®éªŒå’Œç»“æœä¹Ÿä¸ºå¼€å‘ä¸­å›½åŒ»å­¦è´¨é‡ä¿è¯ç³»ç»Ÿå’Œæ³•å­¦ç¡•å£«è¯„ä¼°ç®¡é“çš„æŒ‘æˆ˜å’Œæ½œåœ¨è§£å†³æ–¹æ¡ˆæä¾›äº†å®è´µçš„è§è§£ã€‚

## Large Language Models of Code Fail at Completing Code with Potential Bugs<sup>poster<sup>

Authors: Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, George Karypis

Link: [https://neurips.cc/virtual/2023/poster/70988](https://neurips.cc/virtual/2023/poster/70988)

Abstract:

 Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs â€“ anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a large gap in post-mitigation performance.

æ‘˜è¦:

å¤§å‹ä»£ç è¯­è¨€æ¨¡å‹ï¼ˆCode-LLMï¼‰æœ€è¿‘åœ¨ä»£ç è¡¥å…¨æ–¹é¢å¸¦æ¥äº†å·¨å¤§è¿›æ­¥ï¼Œè¿™æ˜¯ç¼–ç¨‹è¾…åŠ©å’Œä»£ç æ™ºèƒ½çš„åŸºæœ¬ç‰¹å¾ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å·¥ä½œéƒ½å¿½ç•¥äº†ç”Ÿæˆä»£ç ä¸Šä¸‹æ–‡ä¸­å¯èƒ½å­˜åœ¨çš„é”™è¯¯ï¼Œè¿™åœ¨è½¯ä»¶å¼€å‘ä¸­æ˜¯ä¸å¯é¿å…çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥å¹¶ç ”ç©¶äº†é”™è¯¯ä»£ç å®Œæˆé—®é¢˜ï¼Œå—åˆ°å®æ—¶ä»£ç å»ºè®®çš„ç°å®åœºæ™¯çš„å¯å‘ï¼Œå…¶ä¸­ä»£ç ä¸Šä¸‹æ–‡åŒ…å«æ½œåœ¨çš„é”™è¯¯ - å¯èƒ½æˆä¸ºå·²å®Œæˆç¨‹åºä¸­çš„é”™è¯¯çš„åæ¨¡å¼ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¯¥ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ•°æ®é›†ï¼šä¸€ä¸ªæ•°æ®é›†åŒ…å«æºè‡ªè¯­ä¹‰æ”¹å˜æ“ä½œç¬¦æ›´æ”¹çš„åˆæˆé”™è¯¯ (buggy-HumanEval)ï¼Œå¦ä¸€ä¸ªæ•°æ®é›†åŒ…å«æºè‡ªç”¨æˆ·æäº¤çš„ç¼–ç é—®é¢˜çš„å®é™…é”™è¯¯ (buggy-FixEval)ã€‚æˆ‘ä»¬å‘ç°æ½œåœ¨é”™è¯¯çš„å­˜åœ¨ä¼šæ˜¾ç€é™ä½é«˜æ€§èƒ½ Code-LLM çš„ç”Ÿæˆæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘åˆ°ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨å•ä¸ªæ½œåœ¨é”™è¯¯ï¼ŒCODEGEN-2B-MONO åœ¨ buggy-HumanEval æµ‹è¯•ç”¨ä¾‹ä¸Šçš„é€šè¿‡ç‡ä¸‹é™äº† 50% ä»¥ä¸Šã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å‡ ç§å‡è½»æ½œåœ¨é”™è¯¯ä¸åˆ©å½±å“çš„äº‹åæ–¹æ³•ï¼Œå‘ç°ç¼“è§£åçš„æ€§èƒ½ä»ç„¶å­˜åœ¨å¾ˆå¤§å·®è·ã€‚

## Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model<sup>poster<sup>

Authors: Zirui Liu, Guanchu Wang, Shaochen (Henry) Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia Hu

Link: [https://neurips.cc/virtual/2023/poster/71585](https://neurips.cc/virtual/2023/poster/71585)

Abstract:

 As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, machine learning models are typically trained using stochastic gradient descent.We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.

æ‘˜è¦:

éšç€æ¨¡å‹å¤§å°çš„å¿«é€Ÿå¢é•¿ï¼Œç”±äºå…¶å¤§é‡çš„å†…å­˜ä½¿ç”¨ï¼Œå¯¹å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒå˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚ä»¥å‰çš„å·¥ä½œé€šå¸¸ä¾§é‡äºå‡å°‘ç½‘ç»œä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚è™½ç„¶æ¨¡å‹å‚æ•°ç¡®å®ä¼šå½±å“å†…å­˜ä½¿ç”¨ï¼Œä½†è®­ç»ƒæœŸé—´çš„ä¸»è¦å†…å­˜ç“¶é¢ˆæ¥è‡ªäºå­˜å‚¨ç‰¹å¾å›¾ï¼ˆä¹Ÿç§°ä¸ºæ¿€æ´»ï¼‰ï¼Œå› ä¸ºå®ƒä»¬å¯¹äºæ¢¯åº¦è®¡ç®—è‡³å…³é‡è¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨éšæœºä¼˜åŒ–ä¸­ï¼Œåªè¦æ¢¯åº¦ä¼°è®¡å™¨æ— åä¸”å…·æœ‰åˆç†çš„æ–¹å·®ï¼Œæ¨¡å‹å°±å¯ä»¥å¤„ç†å™ªå£°æ¢¯åº¦ã€‚éµå¾ªè¿™ä¸€åŠ¨æœºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ— åä¼°è®¡å™¨ç³»åˆ—ï¼Œç§°ä¸º\sasï¼Œå¯¹äºæ–¹å·®å‡å°‘çš„çŸ©é˜µç”Ÿæˆï¼Œåªéœ€è¦å­˜å‚¨å­é‡‡æ ·æ¿€æ´»æ¥è®¡ç®—æ¢¯åº¦ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ç†è®ºå’Œå®éªŒè¯æ®ï¼Œè¡¨æ˜åœ¨è°ƒæ•´å˜å‹å™¨çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„ä¼°è®¡å™¨ä¸ç°æœ‰çš„ç›¸æ¯”è¡¨ç°å‡ºæ›´ä½çš„æ–¹å·®é€šè¿‡ç”¨ Transformer ä¸­çš„è¿‘ä¼¼çº¿æ€§è¿ç®—æ›¿æ¢çº¿æ€§è¿ç®—ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å‡ ä¹æ²¡æœ‰ç²¾åº¦ä¸‹é™çš„æƒ…å†µä¸‹å®ç°é«˜è¾¾ 2.7 å€çš„å³°å€¼å†…å­˜å‡å°‘ï¼Œå¹¶å¯å®ç°é«˜è¾¾ $6.4\times$ çš„æ›´å¤§æ‰¹é‡å¤§å°ã€‚åœ¨ç›¸åŒçš„ç¡¬ä»¶ä¸‹ï¼Œ\sas å¯ä»¥å®ç°æ›´å¥½çš„æ€§èƒ½é€šè¿‡åº”ç”¨æ›´å¤§çš„æ¨¡å‹å’Œ/æˆ–æ›´å¤§æ‰¹é‡å¤§å°çš„æ›´å¿«è®­ç»ƒé€Ÿåº¦æ¥é™ä½ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ä»£ç å¯åœ¨ https://anonymous.4open.science/r/WTACRS-A5C5/ ä¸Šè·å–ã€‚

## ProPILE: Probing Privacy Leakage in Large Language Models<sup>poster<sup>

Authors: Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh

Link: [https://neurips.cc/virtual/2023/poster/71697](https://neurips.cc/virtual/2023/poster/71697)

Abstract:

 The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.

æ‘˜è¦:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å’Œå¹¿æ³›ä½¿ç”¨å¼•èµ·äº†äººä»¬å¯¹ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰æ½œåœ¨æ³„éœ²çš„ä¸¥é‡æ‹…å¿§ã€‚è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯æ ¹æ®å¤§é‡ç½‘ç»œæ”¶é›†çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„ï¼Œå…¶ä¸­å¯èƒ½ä¼šæ— æ„ä¸­åŒ…å«æ•æ„Ÿçš„ä¸ªäººæ•°æ®ã€‚æœ¬æ–‡ä»‹ç»äº† ProPILEï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¢æµ‹å·¥å…·ï¼Œæ—¨åœ¨ä½¿æ•°æ®ä¸»ä½“æˆ– PII æ‰€æœ‰è€…èƒ½å¤Ÿæ„è¯†åˆ°åŸºäº LLM çš„æœåŠ¡ä¸­æ½œåœ¨çš„ PII æ³„æ¼ã€‚ ProPILEè®©æ•°æ®ä¸»ä½“æ ¹æ®è‡ªå·±çš„PIIåˆ¶å®šæç¤ºï¼Œä»¥è¯„ä¼°æ³•å­¦ç¡•å£«çš„éšç§ä¾µçŠ¯ç¨‹åº¦ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„ Pile æ•°æ®é›†ä¸Šè®­ç»ƒçš„ OPT-1.3B æ¨¡å‹ä¸Šæ¼”ç¤ºäº†å…¶åº”ç”¨ã€‚æˆ‘ä»¬å±•ç¤ºäº†å‡è®¾çš„æ•°æ®ä¸»ä½“å¦‚ä½•è¯„ä¼°å…¶ PII è¢«åŒ…å«åœ¨æ‰€æ­ç¤ºçš„ Pile æ•°æ®é›†ä¸­çš„å¯èƒ½æ€§ã€‚ LLM æœåŠ¡æä¾›å•†è¿˜å¯ä»¥åˆ©ç”¨ ProPILE æ¥æœ‰æ•ˆè¯„ä¼°è‡ªå·±çš„ PII æ³„æ¼æ°´å¹³ï¼Œå¹¶æä¾›ä¸“é—¨é’ˆå¯¹å…¶å†…éƒ¨æ¨¡å‹è°ƒæ•´çš„æ›´å¼ºå¤§çš„æç¤ºã€‚è¯¥å·¥å…·ä»£è¡¨ç€å‘æ•°æ®ä¸»ä½“å¢å¼ºå…¶å¯¹ç½‘ç»œä¸Šè‡ªå·±æ•°æ®çš„è®¤è¯†å’Œæ§åˆ¶æƒè¿ˆå‡ºçš„å¼€åˆ›æ€§ä¸€æ­¥ã€‚

## M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models<sup>poster<sup>

Authors: Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing

Link: [https://neurips.cc/virtual/2023/poster/73506](https://neurips.cc/virtual/2023/poster/73506)

Abstract:

 Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.

æ‘˜è¦:

å°½ç®¡å­˜åœ¨å„ç§è¯„ä¼°è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹çš„åŸºå‡†ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºï¼Œäººç±»è€ƒè¯•æ˜¯è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸€èˆ¬æ™ºåŠ›çš„æ›´åˆé€‚çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬æœ¬è´¨ä¸Šéœ€è¦æ›´å¹¿æ³›çš„èƒ½åŠ›ï¼Œä¾‹å¦‚è¯­è¨€ç†è§£ã€é¢†åŸŸçŸ¥è¯†å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº† M3Examï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºå‡†ï¼Œæºè‡ªçœŸå®å’Œå®˜æ–¹çš„çœŸäººè€ƒè¯•é—®é¢˜ï¼Œç”¨äºåœ¨å¤šè¯­è¨€ã€å¤šæ¨¡å¼å’Œå¤šå±‚æ¬¡çš„èƒŒæ™¯ä¸‹è¯„ä¼°æ³•å­¦ç¡•å£«ã€‚ M3Examå‘ˆç°å‡ºä¸‰ä¸ªç‹¬ç‰¹çš„ç‰¹ç‚¹ï¼šï¼ˆ1ï¼‰å¤šè¯­è¨€æ€§ï¼Œæ¶µç›–å¤šä¸ªå›½å®¶çš„é—®é¢˜ï¼Œéœ€è¦å¾ˆå¼ºçš„å¤šè¯­è¨€èƒ½åŠ›å’Œæ–‡åŒ–çŸ¥è¯†ï¼› ï¼ˆ2ï¼‰å¤šæ¨¡æ€ï¼Œè€ƒè™‘å¾ˆå¤šè¯•é¢˜çš„å¤šæ¨¡æ€æ€§è´¨ï¼Œæµ‹è¯•æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼› ï¼ˆ3ï¼‰å¤šå±‚æ¬¡ç»“æ„ï¼Œä»¥ä¸‰ä¸ªå…³é”®æ•™è‚²æ—¶æœŸçš„è€ƒè¯•ä¸ºç‰¹è‰²ï¼Œç»¼åˆè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒå±‚æ¬¡çš„ç†Ÿç»ƒç¨‹åº¦ã€‚ M3Exam æ€»å…±åŒ…å« 9 ç§ä¸åŒè¯­è¨€ã€ä¸‰ä¸ªæ•™è‚²çº§åˆ«çš„ 12,317 ä¸ªé—®é¢˜ï¼Œå…¶ä¸­çº¦ 23% çš„é—®é¢˜éœ€è¦å¤„ç†å›¾åƒæ‰èƒ½æˆåŠŸè§£å†³ã€‚æˆ‘ä»¬è¯„ä¼°äº† M3Exam ä¸Šè¡¨ç°æœ€å¥½çš„æ³•å­¦ç¡•å£«çš„è¡¨ç°ï¼Œå‘ç°å½“å‰æ¨¡å‹ï¼ˆåŒ…æ‹¬ GPT-4ï¼‰ä»ç„¶éš¾ä»¥å¤„ç†å¤šè¯­è¨€æ–‡æœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹å’Œéæ‹‰ä¸æ–‡å­—è¯­è¨€ä¸­ã€‚å¤šæ¨¡å¼æ³•å­¦ç¡•å£«åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡å¼é—®é¢˜æ—¶ä¹Ÿè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒM3Exam å¯ä»¥é€šè¿‡æ£€æŸ¥æ³•å­¦ç¡•å£«çš„å¤šè¯­è¨€å’Œå¤šæ¨¡å¼èƒ½åŠ›å¹¶è·Ÿè¸ªä»–ä»¬çš„å‘å±•ï¼Œæˆä¸ºå…¨é¢è¯„ä¼°æ³•å­¦ç¡•å£«çš„å®è´µèµ„æºã€‚æ•°æ®å’Œè¯„ä¼°ä»£ç å¯åœ¨ \url{https://github.com/DAMO-NLP-SG/M3Exam} è·å–ã€‚

## BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing<sup>poster<sup>

Authors: Subhro Roy, Samuel Thomson, Tongfei Chen, Richard Shin, Adam Pauls, Jason Eisner, Benjamin Van Durme

Link: [https://neurips.cc/virtual/2023/poster/73488](https://neurips.cc/virtual/2023/poster/73488)

Abstract:

 Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output meaning representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark seven language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or even surpass state-of-the-art methods for both syntactic and semantic parsing when the model output is constrained to be valid.

æ‘˜è¦:

æœ€è¿‘çš„å·¥ä½œè¡¨æ˜ï¼Œå½“è¾“å‡ºè¢«é™åˆ¶ä¸ºæœ‰æ•ˆçš„è¯­ä¹‰è¡¨ç¤ºæ—¶ï¼Œä»æç¤ºæˆ–å¾®è°ƒçš„è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯ä»¥åœ¨è¯­ä¹‰è§£æä¸­è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬å¼•å…¥äº† BenchCLAMPï¼Œä¸€ä¸ªè¯„ä¼°çº¦æŸè¯­è¨€æ¨¡å‹è§£æçš„åŸºå‡†ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸ƒä¸ªè¯­ä¹‰è§£ææ•°æ®é›†çš„ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•å’Œä¸¤ä¸ªå…·æœ‰ä¸åŒè¾“å‡ºå«ä¹‰è¡¨ç¤ºçš„å¥æ³•è§£ææ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªçº¦æŸè§£ç æ¥å£ï¼Œç”¨äºä»…ç”Ÿæˆè¿™äº›æ•°æ®è¦†ç›–çš„æœ‰æ•ˆè¾“å‡ºè¯­æ³•ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªæ•°æ®é›†æä¾›ä½ã€ä¸­ã€é«˜èµ„æºåˆ’åˆ†ï¼Œå…è®¸åœ¨ä¸åŒæ•°æ®ä½“ç³»ä¸‹å‡†ç¡®æ¯”è¾ƒå„ç§è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ”¯æŒä½¿ç”¨åŸºäºæç¤ºçš„å­¦ä¹ å’Œå¾®è°ƒæ¥è¯„ä¼°è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹ä¸ƒç§è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ç§ä»…é€šè¿‡ API æä¾›çš„ GPT-3 å˜ä½“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“æ¨¡å‹è¾“å‡ºè¢«é™åˆ¶ä¸ºæœ‰æ•ˆæ—¶ï¼Œç¼–ç å™¨-è§£ç å™¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯ä»¥å®ç°ç±»ä¼¼çš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šå¥æ³•å’Œè¯­ä¹‰è§£æçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

## UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models<sup>poster<sup>

Authors: Xin Li, Sima Behpour, Thang Long Doan, Wenbin He, Liang Gou, Liu Ren

Link: [https://neurips.cc/virtual/2023/poster/71462](https://neurips.cc/virtual/2023/poster/71462)

Abstract:

 In this study, we investigate the task of data pre-selection, which aims to select instances for labeling from an unlabeled dataset through a single pass, thereby optimizing performance for undefined downstream tasks with a limited annotation budget. Previous approaches to data pre-selection relied solely on visual features extracted from foundation models, such as CLIP and BLIP-2, but largely ignored the powerfulness of text features. In this work, we argue that, with proper design, the joint feature space of both vision and text can yield a better representation for data pre-selection. To this end, we introduce UP-DP, a simple yet effective unsupervised prompt learning approach that adapts vision-language models, like BLIP-2, for data pre-selection. Specifically, with the BLIP-2 parameters frozen, we train text prompts to extract the joint features with improved representation, ensuring a diverse cluster structure that covers the entire dataset. We extensively compare our method with the state-of-the-art using seven benchmark datasets in different settings, achieving up to a performance gain of 20\%. Interestingly, the prompts learned from one dataset demonstrate significant generalizability and can be applied directly to enhance the feature extraction of BLIP-2 from other datasets. To the best of our knowledge, UP-DP is the first work to incorporate unsupervised prompt learning in a vision-language model for data pre-selection.

æ‘˜è¦:

åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ•°æ®é¢„é€‰æ‹©çš„ä»»åŠ¡ï¼Œå…¶ç›®çš„æ˜¯é€šè¿‡å•æ¬¡ä»æœªæ ‡è®°çš„æ•°æ®é›†ä¸­é€‰æ‹©ç”¨äºæ ‡è®°çš„å®ä¾‹ï¼Œä»è€Œåœ¨æœ‰é™çš„æ³¨é‡Šé¢„ç®—ä¸‹ä¼˜åŒ–æœªå®šä¹‰çš„ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚ä»¥å‰çš„æ•°æ®é¢„é€‰æ–¹æ³•ä»…ä¾èµ–äºä»åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚ CLIP å’Œ BLIP-2ï¼‰ä¸­æå–çš„è§†è§‰ç‰¹å¾ï¼Œè€Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†æ–‡æœ¬ç‰¹å¾çš„å¼ºå¤§åŠŸèƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºï¼Œé€šè¿‡é€‚å½“çš„è®¾è®¡ï¼Œè§†è§‰å’Œæ–‡æœ¬çš„è”åˆç‰¹å¾ç©ºé—´å¯ä»¥ä¸ºæ•°æ®é¢„é€‰æ‹©æä¾›æ›´å¥½çš„è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº† UP-DPï¼Œä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ— ç›‘ç£å³æ—¶å­¦ä¹ æ–¹æ³•ï¼Œå®ƒé‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ BLIP-2ï¼‰è¿›è¡Œæ•°æ®é¢„é€‰æ‹©ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å†»ç»“ BLIP-2 å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®­ç»ƒæ–‡æœ¬æç¤ºæ¥æå–å…·æœ‰æ”¹è¿›è¡¨ç¤ºçš„è”åˆç‰¹å¾ï¼Œç¡®ä¿è¦†ç›–æ•´ä¸ªæ•°æ®é›†çš„å¤šæ ·åŒ–é›†ç¾¤ç»“æ„ã€‚æˆ‘ä»¬åœ¨ä¸åŒè®¾ç½®ä¸‹ä½¿ç”¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›æ¯”è¾ƒï¼Œå®ç°äº†é«˜è¾¾ 20% çš„æ€§èƒ½å¢ç›Šã€‚æœ‰è¶£çš„æ˜¯ï¼Œä»ä¸€ä¸ªæ•°æ®é›†å­¦åˆ°çš„æç¤ºè¡¨ç°å‡ºæ˜¾ç€çš„é€šç”¨æ€§ï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºå¢å¼º BLIP-2 ä»å…¶ä»–æ•°æ®é›†çš„ç‰¹å¾æå–ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒUP-DP æ˜¯ç¬¬ä¸€ä¸ªå°†æ— ç›‘ç£å³æ—¶å­¦ä¹ çº³å…¥è§†è§‰è¯­è¨€æ¨¡å‹ä»¥è¿›è¡Œæ•°æ®é¢„é€‰çš„å·¥ä½œã€‚

